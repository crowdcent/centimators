{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Overview","text":""},{"location":"#centimators-essential-data-transformers-and-model-estimators-for-ml-and-data-science-competitions","title":"Centimators: essential data transformers and model estimators for ML and data science competitions","text":"<p><code>centimators</code> is an open-source python library built on scikit-learn, keras, and narwhals: designed for building and sharing dataframe-agnostic (pandas/polars), multi-framework (jax/tf/pytorch), sklearn-style (fit/transform/predict) transformers, meta-estimators, and machine learning models for data science competitions like Numerai, Kaggle, and the CrowdCent Challenge.</p>"},{"location":"#built-for","title":"Built for \u2026","text":"\ud83c\udfc6 Competition Data Scientists \ud83d\udcca Financial-ML Engineers \u26a1 Performance Hunters \ud83e\udde0 Neural Network Architects Iterate fast on Numerai, Kaggle, &amp; CrowdCent submissions without reinventing feature engineering each time. Production-ready transformers for ranking, lagging, returns, and rolling stats that respect time &amp; group boundaries. Swap Pandas \u2194 Polars with a one-liner, leverage JAX/TF/PyTorch back-ends, and squeeze every millisecond out of your pipeline. Build and improve upon architectures like bottleneck autoencoders and self-improving neural networks with the same scikit-learn-style API."},{"location":"#basic-usage","title":"Basic Usage","text":"<p>For common transformations like ranking (which groups by date) and lagging (which groups by ticker), use out-of-the-box transformers to create features out of your data, while easily handling group boundaries:</p> <pre><code>from centimators.feature_transformers import RankTransformer, LagTransformer\n\n# Cross-sectional ranking: rank features within each date\nrank_transformer = RankTransformer(feature_names=['close', 'volume'])\nranked_features = rank_transformer.fit_transform(\n    df[['close', 'volume']], \n    date_series=df['date']\n)\n\n# Time-series lagging: create lagged features within each ticker\nlag_transformer = LagTransformer(windows=[1, 5, 10])\nlagged_features = lag_transformer.fit_transform(\n    df[['close', 'volume']], \n    ticker_series=df['ticker']\n)\n</code></pre> <p>For modeling your features, use centimators's model estimators. A family of Keras-backed estimators are available, including MLPRegressor, BottleneckEncoder, LSTMRegressor for sequences, and always more to come.</p> <pre><code>from centimators.model_estimators import MLPRegressor, LSTMRegressor\n\n# For tabular data\nmodel = MLPRegressor()\nmodel.fit(df[feature_names], df['target'])\n\n# For sequential/time-series data\nlstm = LSTMRegressor(\n    lag_windows=[1, 2, 3, 4, 5],\n    n_features_per_timestep=len(feature_names),\n    lstm_units=[(64, 0.2, 0.1)],\n    bidirectional=True\n)\nlstm.fit(lagged_features, df['target'])\n</code></pre>"},{"location":"#getting-started","title":"Getting Started","text":"<p>Ready to dive in? Check out our Installation &amp; Quick Start guide to get up and running in minutes.</p> <p>For comprehensive examples and advanced usage patterns, explore our User Guide and Tutorials.</p>"},{"location":"#philosophy","title":"Philosophy","text":"<p><code>centimators</code> makes heavy use of advanced scikit-learn concepts such as metadata routing. This enables powerful workflows where auxiliary data (like dates or tickers) flow seamlessly through complex pipelines to the specific components that need them.</p> <p>The library is designed for practitioners who need production-ready, composable components for financial machine learning workflows while maintaining compatibility with the broader Python ML ecosystem.</p>"},{"location":"installation-quick-start/","title":"Installation &amp; Quick Start","text":""},{"location":"installation-quick-start/#installation","title":"Installation","text":"uv (Recommended)pip <pre><code>uv add centimators\n</code></pre> <pre><code>pip install centimators\n</code></pre>"},{"location":"installation-quick-start/#quick-start","title":"Quick Start","text":"<p><code>centimators</code> transformers are dataframe-agnostic, powered by narwhals. You can use the same transformer (like <code>RankTransformer</code>) seamlessly with both Pandas and Polars DataFrames. This transformer calculates the normalized rank of features within each date group.</p> <p>First, let's define some common data: <pre><code>import pandas as pd\nimport polars as pl\n# Create sample OHLCV data for two stocks over four trading days\ndata = {\n    'date': ['2021-01-01', '2021-01-01', '2021-01-02', '2021-01-02', \n             '2021-01-03', '2021-01-03', '2021-01-04', '2021-01-04'],\n    'ticker': ['AAPL', 'MSFT', 'AAPL', 'MSFT', 'AAPL', 'MSFT', 'AAPL', 'MSFT'],\n    'open': [150.0, 280.0, 151.0, 282.0, 152.0, 283.0, 153.0, 284.0],    # Opening prices\n    'high': [152.0, 282.0, 153.0, 284.0, 154.0, 285.0, 155.0, 286.0],    # Daily highs\n    'low': [149.0, 278.0, 150.0, 280.0, 151.0, 281.0, 152.0, 282.0],     # Daily lows\n    'close': [151.0, 281.0, 152.0, 283.0, 153.0, 284.0, 154.0, 285.0],   # Closing prices\n    'volume': [1000000, 800000, 1200000, 900000, 1100000, 850000, 1050000, 820000]  # Trading volume\n}\n\n# Create both Pandas and Polars DataFrames\ndf_pd = pd.DataFrame(data)\ndf_pl = pl.DataFrame(data)\n\n# Define the OHLCV features we want to transform\nfeature_cols = ['volume', 'close']\n</code></pre></p> <p>Now, let's use the transformer: <pre><code>from centimators.feature_transformers import RankTransformer\n\ntransformer = RankTransformer(feature_names=feature_cols)\nresult_pd = transformer.fit_transform(df_pd[feature_cols], date_series=df_pd['date'])\nresult_pl = transformer.fit_transform(df_pl[feature_cols], date_series=df_pl['date'])\n</code></pre></p> <p>Both <code>result_pd</code> (from Pandas) and <code>result_pl</code> (from Polars) will contain the same transformed data in their native DataFrame formats. You may find significant performance gains using Polars for certain operations. </p>"},{"location":"api-reference/feature_transformers/","title":"Feature Transformers API","text":""},{"location":"api-reference/feature_transformers/#centimators.feature_transformers","title":"<code>centimators.feature_transformers</code>","text":"<p>Feature transformers (in the scikit-learn sense) that integrate seamlessly with pipelines. Using metadata routing, centimators' transformers specialize in grouping features by a date or ticker series, and applying transformations to each group independently.</p> <p>This module provides a family of stateless feature/target transformers built on top of narwhals. Each class follows the <code>sklearn.base. TransformerMixin</code> interface which allows them to participate in <code>sklearn.pipeline.Pipeline</code> or <code>ColumnTransformer</code> objects without extra boiler-plate.</p> <p>All transformers are fully vectorised, backend-agnostic (pandas, polars, \u2026) and suitable for cross-validation, grid-search and other classic machine-learning workflows.</p> Highlights <ul> <li>RankTransformer \u2013 converts numeric features into their (0,\u20061]-normalised rank within a user-supplied grouping column (e.g. a date).</li> <li>LagTransformer \u2013 creates shifted/lagged copies of features to expose temporal context for time-series models.</li> <li>MovingAverageTransformer \u2013 rolling mean across arbitrary window sizes.</li> <li>LogReturnTransformer \u2013 first-difference of the natural logarithm of a signal, a common way to compute returns.</li> <li>GroupStatsTransformer \u2013 horizontally aggregates arbitrary sets of columns and exposes statistics such as mean, standard deviation, skew, kurtosis, range and coefficient of variation.</li> </ul>"},{"location":"api-reference/feature_transformers/#centimators.feature_transformers.RankTransformer","title":"<code>RankTransformer</code>","text":"<p>               Bases: <code>_BaseFeatureTransformer</code></p> <p>RankTransformer transforms features into their normalized rank within groups defined by a date series.</p> <p>Parameters:</p> Name Type Description Default <code>feature_names</code> <code>list of str</code> <p>Names of columns to transform. If None, all columns of X are used.</p> <code>None</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; from centimators.feature_transformers import RankTransformer\n&gt;&gt;&gt; df = pd.DataFrame({\n...     'date': ['2021-01-01', '2021-01-01', '2021-01-02'],\n...     'feature1': [3, 1, 2],\n...     'feature2': [30, 20, 10]\n... })\n&gt;&gt;&gt; transformer = RankTransformer(feature_names=['feature1', 'feature2'])\n&gt;&gt;&gt; result = transformer.fit_transform(df[['feature1', 'feature2']], date_series=df['date'])\n&gt;&gt;&gt; print(result)\n   feature1_rank  feature2_rank\n0            0.5            0.5\n1            1.0            1.0\n2            1.0            1.0\n</code></pre> Source code in <code>src/centimators/feature_transformers.py</code> <pre><code>class RankTransformer(_BaseFeatureTransformer):\n    \"\"\"\n    RankTransformer transforms features into their normalized rank within groups defined by a date series.\n\n    Args:\n        feature_names (list of str, optional): Names of columns to transform.\n            If None, all columns of X are used.\n\n    Examples:\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; from centimators.feature_transformers import RankTransformer\n        &gt;&gt;&gt; df = pd.DataFrame({\n        ...     'date': ['2021-01-01', '2021-01-01', '2021-01-02'],\n        ...     'feature1': [3, 1, 2],\n        ...     'feature2': [30, 20, 10]\n        ... })\n        &gt;&gt;&gt; transformer = RankTransformer(feature_names=['feature1', 'feature2'])\n        &gt;&gt;&gt; result = transformer.fit_transform(df[['feature1', 'feature2']], date_series=df['date'])\n        &gt;&gt;&gt; print(result)\n           feature1_rank  feature2_rank\n        0            0.5            0.5\n        1            1.0            1.0\n        2            1.0            1.0\n    \"\"\"\n\n    def __init__(self, feature_names=None):\n        super().__init__(feature_names)\n\n    @nw.narwhalify(allow_series=True)\n    def transform(self, X: FrameT, y=None, date_series: IntoSeries = None) -&gt; FrameT:\n        \"\"\"Transforms features to their normalized rank.\n\n        Args:\n            X (FrameT): Input data frame.\n            y (Any, optional): Ignored. Kept for compatibility.\n            date_series (IntoSeries, optional): Series defining groups for ranking (e.g., dates).\n\n        Returns:\n            FrameT: Transformed data frame with ranked features.\n        \"\"\"\n        X, date_col_name = _attach_group(X, date_series, \"date\")\n\n        # compute absolute rank for each feature\n        rank_columns: list[nw.Expr] = [\n            nw.col(feature_name)\n            .rank()\n            .over(date_col_name)\n            .alias(f\"{feature_name}_rank_temp\")\n            for feature_name in self.feature_names\n        ]\n\n        # compute count for each feature\n        count_columns: list[nw.Expr] = [\n            nw.col(feature_name)\n            .count()\n            .over(date_col_name)\n            .alias(f\"{feature_name}_count\")\n            for feature_name in self.feature_names\n        ]\n\n        X = X.select([*rank_columns, *count_columns])\n\n        # compute normalized rank for each feature\n        final_columns: list[nw.Expr] = [\n            (\n                nw.col(f\"{feature_name}_rank_temp\") / nw.col(f\"{feature_name}_count\")\n            ).alias(f\"{feature_name}_rank\")\n            for feature_name in self.feature_names\n        ]\n\n        X = X.select(final_columns)\n\n        return X\n\n    def get_feature_names_out(self, input_features=None) -&gt; list[str]:\n        \"\"\"Returns the output feature names.\n\n        Args:\n            input_features (list[str], optional): Ignored. Kept for compatibility.\n\n        Returns:\n            list[str]: List of transformed feature names.\n        \"\"\"\n        return [f\"{feature_name}_rank\" for feature_name in self.feature_names]\n</code></pre>"},{"location":"api-reference/feature_transformers/#centimators.feature_transformers.RankTransformer.transform","title":"<code>transform(X, y=None, date_series=None)</code>","text":"<p>Transforms features to their normalized rank.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>FrameT</code> <p>Input data frame.</p> required <code>y</code> <code>Any</code> <p>Ignored. Kept for compatibility.</p> <code>None</code> <code>date_series</code> <code>IntoSeries</code> <p>Series defining groups for ranking (e.g., dates).</p> <code>None</code> <p>Returns:</p> Name Type Description <code>FrameT</code> <code>FrameT</code> <p>Transformed data frame with ranked features.</p> Source code in <code>src/centimators/feature_transformers.py</code> <pre><code>@nw.narwhalify(allow_series=True)\ndef transform(self, X: FrameT, y=None, date_series: IntoSeries = None) -&gt; FrameT:\n    \"\"\"Transforms features to their normalized rank.\n\n    Args:\n        X (FrameT): Input data frame.\n        y (Any, optional): Ignored. Kept for compatibility.\n        date_series (IntoSeries, optional): Series defining groups for ranking (e.g., dates).\n\n    Returns:\n        FrameT: Transformed data frame with ranked features.\n    \"\"\"\n    X, date_col_name = _attach_group(X, date_series, \"date\")\n\n    # compute absolute rank for each feature\n    rank_columns: list[nw.Expr] = [\n        nw.col(feature_name)\n        .rank()\n        .over(date_col_name)\n        .alias(f\"{feature_name}_rank_temp\")\n        for feature_name in self.feature_names\n    ]\n\n    # compute count for each feature\n    count_columns: list[nw.Expr] = [\n        nw.col(feature_name)\n        .count()\n        .over(date_col_name)\n        .alias(f\"{feature_name}_count\")\n        for feature_name in self.feature_names\n    ]\n\n    X = X.select([*rank_columns, *count_columns])\n\n    # compute normalized rank for each feature\n    final_columns: list[nw.Expr] = [\n        (\n            nw.col(f\"{feature_name}_rank_temp\") / nw.col(f\"{feature_name}_count\")\n        ).alias(f\"{feature_name}_rank\")\n        for feature_name in self.feature_names\n    ]\n\n    X = X.select(final_columns)\n\n    return X\n</code></pre>"},{"location":"api-reference/feature_transformers/#centimators.feature_transformers.RankTransformer.get_feature_names_out","title":"<code>get_feature_names_out(input_features=None)</code>","text":"<p>Returns the output feature names.</p> <p>Parameters:</p> Name Type Description Default <code>input_features</code> <code>list[str]</code> <p>Ignored. Kept for compatibility.</p> <code>None</code> <p>Returns:</p> Type Description <code>list[str]</code> <p>list[str]: List of transformed feature names.</p> Source code in <code>src/centimators/feature_transformers.py</code> <pre><code>def get_feature_names_out(self, input_features=None) -&gt; list[str]:\n    \"\"\"Returns the output feature names.\n\n    Args:\n        input_features (list[str], optional): Ignored. Kept for compatibility.\n\n    Returns:\n        list[str]: List of transformed feature names.\n    \"\"\"\n    return [f\"{feature_name}_rank\" for feature_name in self.feature_names]\n</code></pre>"},{"location":"api-reference/feature_transformers/#centimators.feature_transformers.LagTransformer","title":"<code>LagTransformer</code>","text":"<p>               Bases: <code>_BaseFeatureTransformer</code></p> <p>LagTransformer shifts features by specified lag windows within groups defined by a ticker series.</p> <p>Parameters:</p> Name Type Description Default <code>windows</code> <code>iterable of int</code> <p>Lag periods to compute. Each feature will have shifted versions for each lag.</p> required <code>feature_names</code> <code>list of str</code> <p>Names of columns to transform. If None, all columns of X are used.</p> <code>None</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; from centimators.feature_transformers import LagTransformer\n&gt;&gt;&gt; df = pd.DataFrame({\n...     'ticker': ['A', 'A', 'A', 'B', 'B'],\n...     'price': [10, 11, 12, 20, 21]\n... })\n&gt;&gt;&gt; transformer = LagTransformer(windows=[1, 2], feature_names=['price'])\n&gt;&gt;&gt; result = transformer.fit_transform(df[['price']], ticker_series=df['ticker'])\n&gt;&gt;&gt; print(result)\n   price_lag1  price_lag2\n0         NaN         NaN\n1        10.0         NaN\n2        11.0        10.0\n3         NaN         NaN\n4        20.0         NaN\n</code></pre> Source code in <code>src/centimators/feature_transformers.py</code> <pre><code>class LagTransformer(_BaseFeatureTransformer):\n    \"\"\"\n    LagTransformer shifts features by specified lag windows within groups defined by a ticker series.\n\n    Args:\n        windows (iterable of int): Lag periods to compute. Each feature will have\n            shifted versions for each lag.\n        feature_names (list of str, optional): Names of columns to transform.\n            If None, all columns of X are used.\n\n    Examples:\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; from centimators.feature_transformers import LagTransformer\n        &gt;&gt;&gt; df = pd.DataFrame({\n        ...     'ticker': ['A', 'A', 'A', 'B', 'B'],\n        ...     'price': [10, 11, 12, 20, 21]\n        ... })\n        &gt;&gt;&gt; transformer = LagTransformer(windows=[1, 2], feature_names=['price'])\n        &gt;&gt;&gt; result = transformer.fit_transform(df[['price']], ticker_series=df['ticker'])\n        &gt;&gt;&gt; print(result)\n           price_lag1  price_lag2\n        0         NaN         NaN\n        1        10.0         NaN\n        2        11.0        10.0\n        3         NaN         NaN\n        4        20.0         NaN\n    \"\"\"\n\n    def __init__(self, windows, feature_names=None):\n        self.windows = sorted(windows, reverse=True)\n        super().__init__(feature_names)\n\n    @nw.narwhalify(allow_series=True)\n    def transform(\n        self,\n        X: FrameT,\n        y=None,\n        ticker_series: IntoSeries = None,\n    ) -&gt; FrameT:\n        \"\"\"Applies lag transformation to the features.\n\n        Args:\n            X (FrameT): Input data frame.\n            y (Any, optional): Ignored. Kept for compatibility.\n            ticker_series (IntoSeries, optional): Series defining groups for lagging (e.g., tickers).\n\n        Returns:\n            FrameT: Transformed data frame with lagged features. Columns are ordered\n                by lag (as in `self.windows`), then by feature (as in `self.feature_names`).\n                For example, with `windows=[2,1]` and `feature_names=['A','B']`,\n                the output columns will be `A_lag2, B_lag2, A_lag1, B_lag1`.\n        \"\"\"\n        X, ticker_col_name = _attach_group(X, ticker_series, \"ticker\")\n\n        lag_columns = [\n            nw.col(feature_name)\n            .shift(lag)\n            .alias(f\"{feature_name}_lag{lag}\")\n            .over(ticker_col_name)\n            for lag in self.windows  # Iterate over lags first\n            for feature_name in self.feature_names  # Then over feature names\n        ]\n\n        X = X.select(lag_columns)\n\n        return X\n\n    def get_feature_names_out(self, input_features=None) -&gt; list[str]:\n        \"\"\"Returns the output feature names.\n\n        Args:\n            input_features (list[str], optional): Ignored. Kept for compatibility.\n\n        Returns:\n            list[str]: List of transformed feature names, ordered by lag, then by feature.\n        \"\"\"\n        return [\n            f\"{feature_name}_lag{lag}\"\n            for lag in self.windows  # Iterate over lags first\n            for feature_name in self.feature_names  # Then over feature names\n        ]\n</code></pre>"},{"location":"api-reference/feature_transformers/#centimators.feature_transformers.LagTransformer.transform","title":"<code>transform(X, y=None, ticker_series=None)</code>","text":"<p>Applies lag transformation to the features.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>FrameT</code> <p>Input data frame.</p> required <code>y</code> <code>Any</code> <p>Ignored. Kept for compatibility.</p> <code>None</code> <code>ticker_series</code> <code>IntoSeries</code> <p>Series defining groups for lagging (e.g., tickers).</p> <code>None</code> <p>Returns:</p> Name Type Description <code>FrameT</code> <code>FrameT</code> <p>Transformed data frame with lagged features. Columns are ordered by lag (as in <code>self.windows</code>), then by feature (as in <code>self.feature_names</code>). For example, with <code>windows=[2,1]</code> and <code>feature_names=['A','B']</code>, the output columns will be <code>A_lag2, B_lag2, A_lag1, B_lag1</code>.</p> Source code in <code>src/centimators/feature_transformers.py</code> <pre><code>@nw.narwhalify(allow_series=True)\ndef transform(\n    self,\n    X: FrameT,\n    y=None,\n    ticker_series: IntoSeries = None,\n) -&gt; FrameT:\n    \"\"\"Applies lag transformation to the features.\n\n    Args:\n        X (FrameT): Input data frame.\n        y (Any, optional): Ignored. Kept for compatibility.\n        ticker_series (IntoSeries, optional): Series defining groups for lagging (e.g., tickers).\n\n    Returns:\n        FrameT: Transformed data frame with lagged features. Columns are ordered\n            by lag (as in `self.windows`), then by feature (as in `self.feature_names`).\n            For example, with `windows=[2,1]` and `feature_names=['A','B']`,\n            the output columns will be `A_lag2, B_lag2, A_lag1, B_lag1`.\n    \"\"\"\n    X, ticker_col_name = _attach_group(X, ticker_series, \"ticker\")\n\n    lag_columns = [\n        nw.col(feature_name)\n        .shift(lag)\n        .alias(f\"{feature_name}_lag{lag}\")\n        .over(ticker_col_name)\n        for lag in self.windows  # Iterate over lags first\n        for feature_name in self.feature_names  # Then over feature names\n    ]\n\n    X = X.select(lag_columns)\n\n    return X\n</code></pre>"},{"location":"api-reference/feature_transformers/#centimators.feature_transformers.LagTransformer.get_feature_names_out","title":"<code>get_feature_names_out(input_features=None)</code>","text":"<p>Returns the output feature names.</p> <p>Parameters:</p> Name Type Description Default <code>input_features</code> <code>list[str]</code> <p>Ignored. Kept for compatibility.</p> <code>None</code> <p>Returns:</p> Type Description <code>list[str]</code> <p>list[str]: List of transformed feature names, ordered by lag, then by feature.</p> Source code in <code>src/centimators/feature_transformers.py</code> <pre><code>def get_feature_names_out(self, input_features=None) -&gt; list[str]:\n    \"\"\"Returns the output feature names.\n\n    Args:\n        input_features (list[str], optional): Ignored. Kept for compatibility.\n\n    Returns:\n        list[str]: List of transformed feature names, ordered by lag, then by feature.\n    \"\"\"\n    return [\n        f\"{feature_name}_lag{lag}\"\n        for lag in self.windows  # Iterate over lags first\n        for feature_name in self.feature_names  # Then over feature names\n    ]\n</code></pre>"},{"location":"api-reference/feature_transformers/#centimators.feature_transformers.MovingAverageTransformer","title":"<code>MovingAverageTransformer</code>","text":"<p>               Bases: <code>_BaseFeatureTransformer</code></p> <p>MovingAverageTransformer computes the moving average of a feature over a specified window.</p> <p>Parameters:</p> Name Type Description Default <code>windows</code> <code>list of int</code> <p>The windows over which to compute the moving average.</p> required <code>feature_names</code> <code>list of str</code> <p>The names of the features to compute the moving average for.</p> <code>None</code> Source code in <code>src/centimators/feature_transformers.py</code> <pre><code>class MovingAverageTransformer(_BaseFeatureTransformer):\n    \"\"\"\n    MovingAverageTransformer computes the moving average of a feature over a specified window.\n\n    Args:\n        windows (list of int): The windows over which to compute the moving average.\n        feature_names (list of str, optional): The names of the features to compute\n            the moving average for.\n    \"\"\"\n\n    def __init__(self, windows, feature_names=None):\n        self.windows = windows\n        super().__init__(feature_names)\n\n    @nw.narwhalify(allow_series=True)\n    def transform(self, X: FrameT, y=None, ticker_series: IntoSeries = None) -&gt; FrameT:\n        \"\"\"Applies moving average transformation to the features.\n\n        Args:\n            X (FrameT): Input data frame.\n            y (Any, optional): Ignored. Kept for compatibility.\n            ticker_series (IntoSeries, optional): Series defining groups for moving average (e.g., tickers).\n\n        Returns:\n            FrameT: Transformed data frame with moving average features.\n        \"\"\"\n        X, ticker_col_name = _attach_group(X, ticker_series, \"ticker\")\n\n        ma_columns = [\n            nw.col(feature_name)\n            .rolling_mean(window_size=window)\n            .over(ticker_col_name)\n            .alias(f\"{feature_name}_ma{window}\")\n            for feature_name in self.feature_names\n            for window in self.windows\n        ]\n\n        X = X.select(ma_columns)\n\n        return X\n\n    def get_feature_names_out(self, input_features=None) -&gt; list[str]:\n        \"\"\"Returns the output feature names.\n\n        Args:\n            input_features (list[str], optional): Ignored. Kept for compatibility.\n\n        Returns:\n            list[str]: List of transformed feature names.\n        \"\"\"\n        return [\n            f\"{feature_name}_ma{window}\"\n            for feature_name in self.feature_names\n            for window in self.windows\n        ]\n</code></pre>"},{"location":"api-reference/feature_transformers/#centimators.feature_transformers.MovingAverageTransformer.transform","title":"<code>transform(X, y=None, ticker_series=None)</code>","text":"<p>Applies moving average transformation to the features.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>FrameT</code> <p>Input data frame.</p> required <code>y</code> <code>Any</code> <p>Ignored. Kept for compatibility.</p> <code>None</code> <code>ticker_series</code> <code>IntoSeries</code> <p>Series defining groups for moving average (e.g., tickers).</p> <code>None</code> <p>Returns:</p> Name Type Description <code>FrameT</code> <code>FrameT</code> <p>Transformed data frame with moving average features.</p> Source code in <code>src/centimators/feature_transformers.py</code> <pre><code>@nw.narwhalify(allow_series=True)\ndef transform(self, X: FrameT, y=None, ticker_series: IntoSeries = None) -&gt; FrameT:\n    \"\"\"Applies moving average transformation to the features.\n\n    Args:\n        X (FrameT): Input data frame.\n        y (Any, optional): Ignored. Kept for compatibility.\n        ticker_series (IntoSeries, optional): Series defining groups for moving average (e.g., tickers).\n\n    Returns:\n        FrameT: Transformed data frame with moving average features.\n    \"\"\"\n    X, ticker_col_name = _attach_group(X, ticker_series, \"ticker\")\n\n    ma_columns = [\n        nw.col(feature_name)\n        .rolling_mean(window_size=window)\n        .over(ticker_col_name)\n        .alias(f\"{feature_name}_ma{window}\")\n        for feature_name in self.feature_names\n        for window in self.windows\n    ]\n\n    X = X.select(ma_columns)\n\n    return X\n</code></pre>"},{"location":"api-reference/feature_transformers/#centimators.feature_transformers.MovingAverageTransformer.get_feature_names_out","title":"<code>get_feature_names_out(input_features=None)</code>","text":"<p>Returns the output feature names.</p> <p>Parameters:</p> Name Type Description Default <code>input_features</code> <code>list[str]</code> <p>Ignored. Kept for compatibility.</p> <code>None</code> <p>Returns:</p> Type Description <code>list[str]</code> <p>list[str]: List of transformed feature names.</p> Source code in <code>src/centimators/feature_transformers.py</code> <pre><code>def get_feature_names_out(self, input_features=None) -&gt; list[str]:\n    \"\"\"Returns the output feature names.\n\n    Args:\n        input_features (list[str], optional): Ignored. Kept for compatibility.\n\n    Returns:\n        list[str]: List of transformed feature names.\n    \"\"\"\n    return [\n        f\"{feature_name}_ma{window}\"\n        for feature_name in self.feature_names\n        for window in self.windows\n    ]\n</code></pre>"},{"location":"api-reference/feature_transformers/#centimators.feature_transformers.LogReturnTransformer","title":"<code>LogReturnTransformer</code>","text":"<p>               Bases: <code>_BaseFeatureTransformer</code></p> <p>LogReturnTransformer computes the log return of a feature.</p> <p>Parameters:</p> Name Type Description Default <code>feature_names</code> <code>list of str</code> <p>Names of columns to transform. If None, all columns of X are used.</p> <code>None</code> Source code in <code>src/centimators/feature_transformers.py</code> <pre><code>class LogReturnTransformer(_BaseFeatureTransformer):\n    \"\"\"\n    LogReturnTransformer computes the log return of a feature.\n\n    Args:\n        feature_names (list of str, optional): Names of columns to transform.\n            If None, all columns of X are used.\n    \"\"\"\n\n    def __init__(self, feature_names=None):\n        super().__init__(feature_names)\n\n    @nw.narwhalify(allow_series=True)\n    def transform(self, X: FrameT, y=None, ticker_series: IntoSeries = None) -&gt; FrameT:\n        \"\"\"Applies log return transformation to the features.\n\n        Args:\n            X (FrameT): Input data frame.\n            y (Any, optional): Ignored. Kept for compatibility.\n            ticker_series (IntoSeries, optional): Series defining groups for log return (e.g., tickers).\n\n        Returns:\n            FrameT: Transformed data frame with log return features.\n        \"\"\"\n        X, ticker_col_name = _attach_group(X, ticker_series, \"ticker\")\n\n        log_return_columns = [\n            nw.col(feature_name)\n            .log()\n            .diff()\n            .over(ticker_col_name)\n            .alias(f\"{feature_name}_logreturn\")\n            for feature_name in self.feature_names\n        ]\n\n        X = X.select(log_return_columns)\n\n        return X\n\n    def get_feature_names_out(self, input_features=None) -&gt; list[str]:\n        \"\"\"Returns the output feature names.\n\n        Args:\n            input_features (list[str], optional): Ignored. Kept for compatibility.\n\n        Returns:\n            list[str]: List of transformed feature names.\n        \"\"\"\n        return [f\"{feature_name}_logreturn\" for feature_name in self.feature_names]\n</code></pre>"},{"location":"api-reference/feature_transformers/#centimators.feature_transformers.LogReturnTransformer.transform","title":"<code>transform(X, y=None, ticker_series=None)</code>","text":"<p>Applies log return transformation to the features.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>FrameT</code> <p>Input data frame.</p> required <code>y</code> <code>Any</code> <p>Ignored. Kept for compatibility.</p> <code>None</code> <code>ticker_series</code> <code>IntoSeries</code> <p>Series defining groups for log return (e.g., tickers).</p> <code>None</code> <p>Returns:</p> Name Type Description <code>FrameT</code> <code>FrameT</code> <p>Transformed data frame with log return features.</p> Source code in <code>src/centimators/feature_transformers.py</code> <pre><code>@nw.narwhalify(allow_series=True)\ndef transform(self, X: FrameT, y=None, ticker_series: IntoSeries = None) -&gt; FrameT:\n    \"\"\"Applies log return transformation to the features.\n\n    Args:\n        X (FrameT): Input data frame.\n        y (Any, optional): Ignored. Kept for compatibility.\n        ticker_series (IntoSeries, optional): Series defining groups for log return (e.g., tickers).\n\n    Returns:\n        FrameT: Transformed data frame with log return features.\n    \"\"\"\n    X, ticker_col_name = _attach_group(X, ticker_series, \"ticker\")\n\n    log_return_columns = [\n        nw.col(feature_name)\n        .log()\n        .diff()\n        .over(ticker_col_name)\n        .alias(f\"{feature_name}_logreturn\")\n        for feature_name in self.feature_names\n    ]\n\n    X = X.select(log_return_columns)\n\n    return X\n</code></pre>"},{"location":"api-reference/feature_transformers/#centimators.feature_transformers.LogReturnTransformer.get_feature_names_out","title":"<code>get_feature_names_out(input_features=None)</code>","text":"<p>Returns the output feature names.</p> <p>Parameters:</p> Name Type Description Default <code>input_features</code> <code>list[str]</code> <p>Ignored. Kept for compatibility.</p> <code>None</code> <p>Returns:</p> Type Description <code>list[str]</code> <p>list[str]: List of transformed feature names.</p> Source code in <code>src/centimators/feature_transformers.py</code> <pre><code>def get_feature_names_out(self, input_features=None) -&gt; list[str]:\n    \"\"\"Returns the output feature names.\n\n    Args:\n        input_features (list[str], optional): Ignored. Kept for compatibility.\n\n    Returns:\n        list[str]: List of transformed feature names.\n    \"\"\"\n    return [f\"{feature_name}_logreturn\" for feature_name in self.feature_names]\n</code></pre>"},{"location":"api-reference/feature_transformers/#centimators.feature_transformers.GroupStatsTransformer","title":"<code>GroupStatsTransformer</code>","text":"<p>               Bases: <code>_BaseFeatureTransformer</code></p> <p>GroupStatsTransformer calculates statistical measures for defined feature groups.</p> <p>This transformer computes mean, standard deviation, and skewness for each group of features specified in the feature_group_mapping.</p> <p>Parameters:</p> Name Type Description Default <code>feature_group_mapping</code> <code>dict</code> <p>Dictionary mapping group names to lists of feature columns. Example: {'group1': ['feature1', 'feature2'], 'group2': ['feature3', 'feature4']}</p> required <code>stats</code> <code>list of str</code> <p>List of statistics to compute for each group. If None, all statistics are computed. Valid options are 'mean', 'std', 'skew', 'kurt', 'range', and 'cv'.</p> <code>['mean', 'std', 'skew', 'kurt', 'range', 'cv']</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; from centimators.feature_transformers import GroupStatsTransformer\n&gt;&gt;&gt; df = pd.DataFrame({\n...     'feature1': [1, 2, 3],\n...     'feature2': [4, 5, 6],\n...     'feature3': [7, 8, 9],\n...     'feature4': [10, 11, 12]\n... })\n&gt;&gt;&gt; mapping = {'group1': ['feature1', 'feature2'], 'group2': ['feature3', 'feature4']}\n&gt;&gt;&gt; transformer = GroupStatsTransformer(feature_group_mapping=mapping)\n&gt;&gt;&gt; result = transformer.fit_transform(df)\n&gt;&gt;&gt; print(result)\n   group1_groupstats_mean  group1_groupstats_std  group1_groupstats_skew  group2_groupstats_mean  group2_groupstats_std  group2_groupstats_skew\n0                  2.5                 1.5                  0.0                  8.5                 1.5                  0.0\n1                  3.5                 1.5                  0.0                  9.5                 1.5                  0.0\n2                  4.5                 1.5                  0.0                 10.5                 1.5                  0.0\n&gt;&gt;&gt; transformer_mean_only = GroupStatsTransformer(feature_group_mapping=mapping, stats=['mean'])\n&gt;&gt;&gt; result_mean_only = transformer_mean_only.fit_transform(df)\n&gt;&gt;&gt; print(result_mean_only)\n   group1_groupstats_mean  group2_groupstats_mean\n0                  2.5                  8.5\n1                  3.5                  9.5\n2                  4.5                 10.5\n</code></pre> Source code in <code>src/centimators/feature_transformers.py</code> <pre><code>class GroupStatsTransformer(_BaseFeatureTransformer):\n    \"\"\"\n    GroupStatsTransformer calculates statistical measures for defined feature groups.\n\n    This transformer computes mean, standard deviation, and skewness for each\n    group of features specified in the feature_group_mapping.\n\n    Args:\n        feature_group_mapping (dict): Dictionary mapping group names to lists of\n            feature columns. Example: {'group1': ['feature1', 'feature2'],\n            'group2': ['feature3', 'feature4']}\n        stats (list of str, optional): List of statistics to compute for each group.\n            If None, all statistics are computed. Valid options are 'mean', 'std',\n            'skew', 'kurt', 'range', and 'cv'.\n\n    Examples:\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; from centimators.feature_transformers import GroupStatsTransformer\n        &gt;&gt;&gt; df = pd.DataFrame({\n        ...     'feature1': [1, 2, 3],\n        ...     'feature2': [4, 5, 6],\n        ...     'feature3': [7, 8, 9],\n        ...     'feature4': [10, 11, 12]\n        ... })\n        &gt;&gt;&gt; mapping = {'group1': ['feature1', 'feature2'], 'group2': ['feature3', 'feature4']}\n        &gt;&gt;&gt; transformer = GroupStatsTransformer(feature_group_mapping=mapping)\n        &gt;&gt;&gt; result = transformer.fit_transform(df)\n        &gt;&gt;&gt; print(result)\n           group1_groupstats_mean  group1_groupstats_std  group1_groupstats_skew  group2_groupstats_mean  group2_groupstats_std  group2_groupstats_skew\n        0                  2.5                 1.5                  0.0                  8.5                 1.5                  0.0\n        1                  3.5                 1.5                  0.0                  9.5                 1.5                  0.0\n        2                  4.5                 1.5                  0.0                 10.5                 1.5                  0.0\n        &gt;&gt;&gt; transformer_mean_only = GroupStatsTransformer(feature_group_mapping=mapping, stats=['mean'])\n        &gt;&gt;&gt; result_mean_only = transformer_mean_only.fit_transform(df)\n        &gt;&gt;&gt; print(result_mean_only)\n           group1_groupstats_mean  group2_groupstats_mean\n        0                  2.5                  8.5\n        1                  3.5                  9.5\n        2                  4.5                 10.5\n    \"\"\"\n\n    def __init__(\n        self,\n        feature_group_mapping: dict,\n        stats: list[str] = [\"mean\", \"std\", \"skew\", \"kurt\", \"range\", \"cv\"],\n    ):\n        super().__init__(feature_names=None)\n        self.feature_group_mapping = feature_group_mapping\n        self.groups = list(feature_group_mapping.keys())\n        # Supported statistics\n        valid_stats = [\"mean\", \"std\", \"skew\", \"kurt\", \"range\", \"cv\"]\n        if not all(stat in valid_stats for stat in stats):\n            raise ValueError(\n                f\"stats must be a list containing only {valid_stats}. Got {stats}\"\n            )\n        self.stats = stats\n\n    @nw.narwhalify(allow_series=True)\n    def transform(self, X: FrameT, y=None) -&gt; FrameT:\n        \"\"\"Calculates group statistics on the features.\n\n        Args:\n            X (FrameT): Input data frame.\n            y (Any, optional): Ignored. Kept for compatibility.\n\n        Returns:\n            FrameT: Transformed data frame with group statistics features.\n        \"\"\"\n        _expr_factories: dict[str, Callable[[list[str]], nw.Expr]] = {\n            \"mean\": lambda cols: nw.mean_horizontal(*cols),\n            \"std\": lambda cols: std_horizontal(*cols, ddof=1),\n            \"skew\": lambda cols: skew_horizontal(*cols),\n            \"kurt\": lambda cols: kurtosis_horizontal(*cols),\n            \"range\": lambda cols: range_horizontal(*cols),\n            \"cv\": lambda cols: coefficient_of_variation_horizontal(*cols),\n        }\n\n        _min_required_cols: dict[str, int] = {\n            \"mean\": 1,\n            \"range\": 1,\n            \"std\": 2,  # ddof=1 \u21d2 need at least 2 values for a finite result\n            \"cv\": 2,  # depends on std\n            \"skew\": 3,  # bias-corrected formula needs \u22653\n            \"kurt\": 4,  # bias-corrected formula needs \u22654\n        }\n\n        stat_expressions: list[nw.Expr] = []\n\n        for group, cols in self.feature_group_mapping.items():\n            if not cols:\n                raise ValueError(\n                    f\"No valid columns found for group '{group}' in the input frame.\"\n                )\n\n            n_cols = len(cols)\n\n            for stat in self.stats:\n                # Warn early if result is guaranteed to be NaN\n                min_required = _min_required_cols[stat]\n                if n_cols &lt; min_required:\n                    warnings.warn(\n                        (\n                            f\"{self.__class__.__name__}: statistic '{stat}' for group \"\n                            f\"'{group}' requires at least {min_required} feature column(s) \"\n                            f\"but only {n_cols} provided \u2013 the resulting column will be NaN.\"\n                        ),\n                        RuntimeWarning,\n                        stacklevel=2,\n                    )\n\n                expr = _expr_factories[stat](cols).alias(f\"{group}_groupstats_{stat}\")\n                stat_expressions.append(expr)\n\n        return X.select(stat_expressions)\n\n    def get_feature_names_out(self, input_features=None) -&gt; list[str]:\n        \"\"\"Return feature names for all groups.\n\n        Args:\n            input_features (list[str], optional): Ignored. Kept for compatibility.\n\n        Returns:\n            list[str]: List of transformed feature names.\n        \"\"\"\n        return [\n            f\"{group}_groupstats_{stat}\" for group in self.groups for stat in self.stats\n        ]\n</code></pre>"},{"location":"api-reference/feature_transformers/#centimators.feature_transformers.GroupStatsTransformer.transform","title":"<code>transform(X, y=None)</code>","text":"<p>Calculates group statistics on the features.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>FrameT</code> <p>Input data frame.</p> required <code>y</code> <code>Any</code> <p>Ignored. Kept for compatibility.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>FrameT</code> <code>FrameT</code> <p>Transformed data frame with group statistics features.</p> Source code in <code>src/centimators/feature_transformers.py</code> <pre><code>@nw.narwhalify(allow_series=True)\ndef transform(self, X: FrameT, y=None) -&gt; FrameT:\n    \"\"\"Calculates group statistics on the features.\n\n    Args:\n        X (FrameT): Input data frame.\n        y (Any, optional): Ignored. Kept for compatibility.\n\n    Returns:\n        FrameT: Transformed data frame with group statistics features.\n    \"\"\"\n    _expr_factories: dict[str, Callable[[list[str]], nw.Expr]] = {\n        \"mean\": lambda cols: nw.mean_horizontal(*cols),\n        \"std\": lambda cols: std_horizontal(*cols, ddof=1),\n        \"skew\": lambda cols: skew_horizontal(*cols),\n        \"kurt\": lambda cols: kurtosis_horizontal(*cols),\n        \"range\": lambda cols: range_horizontal(*cols),\n        \"cv\": lambda cols: coefficient_of_variation_horizontal(*cols),\n    }\n\n    _min_required_cols: dict[str, int] = {\n        \"mean\": 1,\n        \"range\": 1,\n        \"std\": 2,  # ddof=1 \u21d2 need at least 2 values for a finite result\n        \"cv\": 2,  # depends on std\n        \"skew\": 3,  # bias-corrected formula needs \u22653\n        \"kurt\": 4,  # bias-corrected formula needs \u22654\n    }\n\n    stat_expressions: list[nw.Expr] = []\n\n    for group, cols in self.feature_group_mapping.items():\n        if not cols:\n            raise ValueError(\n                f\"No valid columns found for group '{group}' in the input frame.\"\n            )\n\n        n_cols = len(cols)\n\n        for stat in self.stats:\n            # Warn early if result is guaranteed to be NaN\n            min_required = _min_required_cols[stat]\n            if n_cols &lt; min_required:\n                warnings.warn(\n                    (\n                        f\"{self.__class__.__name__}: statistic '{stat}' for group \"\n                        f\"'{group}' requires at least {min_required} feature column(s) \"\n                        f\"but only {n_cols} provided \u2013 the resulting column will be NaN.\"\n                    ),\n                    RuntimeWarning,\n                    stacklevel=2,\n                )\n\n            expr = _expr_factories[stat](cols).alias(f\"{group}_groupstats_{stat}\")\n            stat_expressions.append(expr)\n\n    return X.select(stat_expressions)\n</code></pre>"},{"location":"api-reference/feature_transformers/#centimators.feature_transformers.GroupStatsTransformer.get_feature_names_out","title":"<code>get_feature_names_out(input_features=None)</code>","text":"<p>Return feature names for all groups.</p> <p>Parameters:</p> Name Type Description Default <code>input_features</code> <code>list[str]</code> <p>Ignored. Kept for compatibility.</p> <code>None</code> <p>Returns:</p> Type Description <code>list[str]</code> <p>list[str]: List of transformed feature names.</p> Source code in <code>src/centimators/feature_transformers.py</code> <pre><code>def get_feature_names_out(self, input_features=None) -&gt; list[str]:\n    \"\"\"Return feature names for all groups.\n\n    Args:\n        input_features (list[str], optional): Ignored. Kept for compatibility.\n\n    Returns:\n        list[str]: List of transformed feature names.\n    \"\"\"\n    return [\n        f\"{group}_groupstats_{stat}\" for group in self.groups for stat in self.stats\n    ]\n</code></pre>"},{"location":"api-reference/keras_cortex/","title":"Keras Cortex","text":""},{"location":"api-reference/keras_cortex/#centimators.keras_cortex","title":"<code>centimators.keras_cortex</code>","text":"<p>Keras Cortex: A self-improving Keras estimator wrapper using DSPy to self-reflect and improve its architecture.</p> <p>This module provides KerasCortex, a scikit-learn compatible meta-estimator. KerasCortex wraps a base Keras estimator (which must have a <code>build_model</code> method) and iteratively refines the <code>build_model</code> method's implementation using a Large Language Model (LLM) through the DSPy library. The goal is to autonomously improve the model's architecture and performance based on validation scores.</p> Highlights <ul> <li>KerasCortex: Meta-estimator that wraps Keras models and uses an LLM   to iteratively suggest improvements to the <code>build_model</code> method.</li> <li>Think: A DSPy module that orchestrates the LLM interaction to generate   Keras code modifications.</li> <li>KerasCodeRefinements: A DSPy signature defining the LLM's task for   suggesting code changes.</li> </ul> <p>Warning</p> <p>This module is a work in progress. It is not yet ready for production use.</p>"},{"location":"api-reference/keras_cortex/#centimators.keras_cortex.KerasCodeRefinements","title":"<code>KerasCodeRefinements</code>","text":"<p>               Bases: <code>Signature</code></p> <p>Suggest modifications to build_model code to improve performance. Consider the history of attempted code. Use Keras 3, there is no tensorflow or tf.keras. Don't use code fences.</p> Source code in <code>src/centimators/keras_cortex.py</code> <pre><code>class KerasCodeRefinements(Signature):\n    \"\"\"Suggest modifications to build_model code to improve performance. Consider the history of attempted code. Use Keras 3, there is no tensorflow or tf.keras. Don't use code fences.\"\"\"\n\n    current_keras_code = InputField(desc=\"Source code of build_model method.\")\n    performance_log = InputField(desc=\"History of (code, metric) pairs.\")\n    optimization_goal = InputField(desc=\"Objective, e.g., 'improve validation scores'.\")\n    suggested_keras_code_modification = OutputField(\n        desc=\"Modified build_model method body as code. No code fences. You must start with only 'def build_model(self):'\"\n    )\n</code></pre>"},{"location":"api-reference/keras_cortex/#centimators.keras_cortex.Think","title":"<code>Think</code>","text":"<p>               Bases: <code>Module</code></p> <p>DSPy Module for suggesting Keras model code modifications.</p> <p>This module uses a <code>ChainOfThought</code> DSPy program with the <code>KerasCodeRefinements</code> signature to prompt an LLM for improvements to a Keras model's <code>build_model</code> method.</p> <p>Parameters:</p> Name Type Description Default <code>verbose</code> <code>bool, default=False</code> <p>If True, prints the LLM's reasoning and suggested code during the <code>forward</code> call.</p> <code>False</code> <p>TODO: Add Keras docs, arXiv access, optimize prompts, pass errors back to LLM, etc.</p> Source code in <code>src/centimators/keras_cortex.py</code> <pre><code>class Think(Module):\n    \"\"\"DSPy Module for suggesting Keras model code modifications.\n\n    This module uses a `ChainOfThought` DSPy program with the\n    `KerasCodeRefinements` signature to prompt an LLM for improvements to a\n    Keras model's `build_model` method.\n\n    Args:\n        verbose (bool, default=False): If True, prints the LLM's reasoning\n            and suggested code during the `forward` call.\n\n    TODO: Add Keras docs, arXiv access, optimize prompts, pass errors back to LLM, etc.\n    \"\"\"\n\n    def __init__(self, verbose=False):\n        super().__init__()\n        self.suggest_code = ChainOfThought(KerasCodeRefinements)\n        self.verbose = verbose\n\n    def forward(\n        self,\n        current_keras_code,\n        performance_log,\n        optimization_goal,\n    ):\n        \"\"\"Generates a Keras code modification suggestion using an LLM.\n\n        Args:\n            current_keras_code (str): The source code of the current\n                `build_model` method.\n            performance_log (list[tuple[str, float]]): A list of (code, metric)\n                tuples representing the history of attempted `build_model` code\n                and their corresponding validation scores.\n            optimization_goal (str): The objective for the LLM, e.g.,\n                'improve validation scores'.\n\n        Returns:\n            str: The LLM's suggested `build_model` method body as a string of code.\n        \"\"\"\n        prediction = self.suggest_code(\n            current_keras_code=current_keras_code,\n            performance_log=performance_log,\n            optimization_goal=optimization_goal,\n        )\n        if self.verbose:\n            print(f\"Reasoning: \\n{prediction.reasoning}\")\n            print(f\"Suggested code: \\n{prediction.suggested_keras_code_modification}\")\n        return prediction.suggested_keras_code_modification\n</code></pre>"},{"location":"api-reference/keras_cortex/#centimators.keras_cortex.Think.forward","title":"<code>forward(current_keras_code, performance_log, optimization_goal)</code>","text":"<p>Generates a Keras code modification suggestion using an LLM.</p> <p>Parameters:</p> Name Type Description Default <code>current_keras_code</code> <code>str</code> <p>The source code of the current <code>build_model</code> method.</p> required <code>performance_log</code> <code>list[tuple[str, float]]</code> <p>A list of (code, metric) tuples representing the history of attempted <code>build_model</code> code and their corresponding validation scores.</p> required <code>optimization_goal</code> <code>str</code> <p>The objective for the LLM, e.g., 'improve validation scores'.</p> required <p>Returns:</p> Name Type Description <code>str</code> <p>The LLM's suggested <code>build_model</code> method body as a string of code.</p> Source code in <code>src/centimators/keras_cortex.py</code> <pre><code>def forward(\n    self,\n    current_keras_code,\n    performance_log,\n    optimization_goal,\n):\n    \"\"\"Generates a Keras code modification suggestion using an LLM.\n\n    Args:\n        current_keras_code (str): The source code of the current\n            `build_model` method.\n        performance_log (list[tuple[str, float]]): A list of (code, metric)\n            tuples representing the history of attempted `build_model` code\n            and their corresponding validation scores.\n        optimization_goal (str): The objective for the LLM, e.g.,\n            'improve validation scores'.\n\n    Returns:\n        str: The LLM's suggested `build_model` method body as a string of code.\n    \"\"\"\n    prediction = self.suggest_code(\n        current_keras_code=current_keras_code,\n        performance_log=performance_log,\n        optimization_goal=optimization_goal,\n    )\n    if self.verbose:\n        print(f\"Reasoning: \\n{prediction.reasoning}\")\n        print(f\"Suggested code: \\n{prediction.suggested_keras_code_modification}\")\n    return prediction.suggested_keras_code_modification\n</code></pre>"},{"location":"api-reference/keras_cortex/#centimators.keras_cortex.KerasCortex","title":"<code>KerasCortex</code>","text":"<p>               Bases: <code>RegressorMixin</code>, <code>BaseEstimator</code></p> <p>A scikit-learn meta-estimator that iteratively refines a Keras model.</p> <p><code>KerasCortex</code> wraps a base Keras estimator (which must expose a <code>build_model</code> method) and uses an LLM via DSPy to suggest modifications to this <code>build_model</code> method. It iteratively attempts these suggestions, evaluates their performance on validation data, and keeps the best-performing model architecture.</p> <p>Parameters:</p> Name Type Description Default <code>base_estimator</code> <code>BaseEstimator</code> <p>An instance of a Keras-based estimator that has a <code>build_model</code> method. Defaults to <code>MLPRegressor()</code>.</p> <code>None</code> <code>n_iterations</code> <code>int, default=5</code> <p>The number of iterations to run the refinement loop.</p> <code>5</code> <code>lm</code> <code>str, default=\"openai/gpt-4o-mini\"</code> <p>The language model to use for code generation, specified as a string recognized by <code>dspy.LM</code>.</p> <code>'openai/gpt-4o-mini'</code> <code>verbose</code> <code>bool, default=False</code> <p>If True, prints detailed information during the refinement process, including LLM reasoning and code suggestions.</p> <code>False</code> <p>Attributes:</p> Name Type Description <code>best_model_</code> <code>BaseEstimator</code> <p>The best Keras estimator found during the refinement process, after fitting.</p> <code>performance_log_</code> <code>list[tuple[str, float]]</code> <p>A log of (code, metric) pairs from the refinement process, after fitting.</p> Source code in <code>src/centimators/keras_cortex.py</code> <pre><code>class KerasCortex(RegressorMixin, BaseEstimator):\n    \"\"\"A scikit-learn meta-estimator that iteratively refines a Keras model.\n\n    `KerasCortex` wraps a base Keras estimator (which must expose a `build_model`\n    method) and uses an LLM via DSPy to suggest modifications to this\n    `build_model` method. It iteratively attempts these suggestions, evaluates\n    their performance on validation data, and keeps the best-performing model\n    architecture.\n\n    Args:\n        base_estimator (BaseEstimator, optional): An instance of a Keras-based\n            estimator that has a `build_model` method. Defaults to `MLPRegressor()`.\n        n_iterations (int, default=5): The number of iterations to run the\n            refinement loop.\n        lm (str, default=\"openai/gpt-4o-mini\"): The language model to use for\n            code generation, specified as a string recognized by `dspy.LM`.\n        verbose (bool, default=False): If True, prints detailed information during\n            the refinement process, including LLM reasoning and code suggestions.\n\n    Attributes:\n        best_model_ (BaseEstimator): The best Keras estimator found during the\n            refinement process, after fitting.\n        performance_log_ (list[tuple[str, float]]): A log of (code, metric) pairs\n            from the refinement process, after fitting.\n    \"\"\"\n\n    def __init__(\n        self,\n        base_estimator=None,\n        n_iterations=5,\n        lm=\"openai/gpt-4o-mini\",\n        verbose=False,\n    ):\n        if base_estimator is None:\n            base_estimator = MLPRegressor()\n        self.base_estimator = base_estimator\n        self.n_iterations = n_iterations\n        self.lm = dspy.LM(lm)\n        dspy.configure(lm=self.lm)\n        self.verbose = verbose\n\n    def think_loop(\n        self, base_estimator, X, y, validation_data, n_iterations=5, **kwargs\n    ) -&gt; tuple[BaseEstimator, list[tuple[str, float]]]:\n        \"\"\"Iteratively refine and retrain a Keras-based estimator.\n\n        This method forms the core of `KerasCortex`. It takes an initial Keras\n        estimator, trains it to get a baseline, and then enters a loop:\n        1.  The current `build_model` code is sent to the `Think` module.\n        2.  The `Think` module (using an LLM) suggests a modification to the code.\n        3.  A new model is created with the modified `build_model` method.\n        4.  The new model is trained and evaluated on validation data.\n        5.  If the new model performs better, its code becomes the current best.\n        This loop repeats for `n_iterations`.\n\n        Args:\n            base_estimator (BaseEstimator): An instance of a Keras-based estimator\n                with `fit`, `predict`, and `build_model` methods.\n            X (array-like): Training data (features).\n            y (array-like): Training data (targets).\n            validation_data (tuple[array-like, array-like]): Data (X_val, y_val) for\n                evaluating model performance during refinement.\n            n_iterations (int, default=5): The number of refinement iterations.\n            **kwargs: Additional keyword arguments passed to the `fit` method of the\n                Keras estimator during each iteration (e.g., `epochs`, `batch_size`).\n\n        Returns:\n            - best_model: The Keras estimator instance with the best-performing\n                `build_model` method found.\n            - performance_log: A list of (code_string, validation_metric)\n                tuples, recording each attempted `build_model` code and its score.\n        \"\"\"\n        # Initial baseline: clone the provided estimator and fit\n        baseline_model = clone(base_estimator)\n        baseline_model.fit(X, y, **kwargs)\n\n        X_val, y_val = validation_data\n        best_metric = baseline_model.score(X_val, y_val)\n        current_code = inspect.getsource(type(baseline_model).build_model)\n        performance_log = [(current_code, best_metric)]\n\n        best_model = baseline_model\n        suggestion = current_code\n\n        think = Think(verbose=self.verbose)\n        for i in range(n_iterations):\n            print(f\"\\n--- Iteration {i + 1} ---\")\n            try:\n                suggestion = think.forward(\n                    current_keras_code=suggestion,\n                    performance_log=performance_log,\n                    optimization_goal=\"improve validation metrics (R2)\",\n                )\n                namespace = {}\n                exec(suggestion, globals(), namespace)\n                build_model_fn = namespace[\"build_model\"]\n\n                # clone from original base_estimator to avoid state pollution\n                new_model = clone(base_estimator)\n                new_model.build_model = types.MethodType(build_model_fn, new_model)\n                new_model.fit(X, y, **kwargs)\n                metric = new_model.score(X_val, y_val)\n\n                performance_log.append((suggestion, metric))\n                if metric &gt; best_metric:\n                    print(\n                        f\"Improvement! New validation score: {metric:.4f} &gt; {best_metric:.4f}\"\n                    )\n                    best_metric = metric\n                    best_model = new_model\n                else:\n                    print(\n                        f\"No improvement ({metric:.4f} &lt;= {best_metric:.4f}), keeping best code.\"\n                    )\n            except Exception as e:\n                print(\"Error during optimization iteration:\", e)\n                break\n\n        return best_model, performance_log\n\n    def fit(self, X, y, validation_data=None, **kwargs):\n        \"\"\"Fit the KerasCortex estimator.\n\n        This method initiates the `think_loop` to find the best model architecture\n        and then fits this best model. The primary purpose of `fit` is to expose\n        a scikit-learn compatible API.\n\n        Args:\n            X (array-like): Training data (features).\n            y (array-like): Training data (targets).\n            validation_data (tuple[array-like, array-like], optional): Data for\n                evaluating model performance during the refinement loop. If None,\n                KerasCortex cannot effectively optimize the model architecture.\n            **kwargs: Additional keyword arguments passed to the `fit` method of the\n                base Keras estimator during the `think_loop` (e.g., `epochs`, `batch_size`).\n\n        Returns:\n            KerasCortex: The fitted estimator instance.\n        \"\"\"\n        self.best_model_, self.performance_log_ = self.think_loop(\n            base_estimator=self.base_estimator,\n            X=X,\n            y=y,\n            validation_data=validation_data,\n            n_iterations=self.n_iterations,\n            **kwargs,\n        )\n        return self\n\n    def predict(self, X):\n        \"\"\"Generate predictions using the best model found by KerasCortex.\n\n        Args:\n            X (array-like): Input data (features) for which to make predictions.\n\n        Returns:\n            array-like: Predictions from the `best_model_`.\n\n        Raises:\n            ValueError: If the estimator has not been fitted (i.e., `fit` has not\n                been called).\n        \"\"\"\n        if not hasattr(self, \"best_model_\"):\n            raise ValueError(\"Estimator not fitted. Call 'fit' first.\")\n        return self.best_model_.predict(X)\n</code></pre>"},{"location":"api-reference/keras_cortex/#centimators.keras_cortex.KerasCortex.think_loop","title":"<code>think_loop(base_estimator, X, y, validation_data, n_iterations=5, **kwargs)</code>","text":"<p>Iteratively refine and retrain a Keras-based estimator.</p> <p>This method forms the core of <code>KerasCortex</code>. It takes an initial Keras estimator, trains it to get a baseline, and then enters a loop: 1.  The current <code>build_model</code> code is sent to the <code>Think</code> module. 2.  The <code>Think</code> module (using an LLM) suggests a modification to the code. 3.  A new model is created with the modified <code>build_model</code> method. 4.  The new model is trained and evaluated on validation data. 5.  If the new model performs better, its code becomes the current best. This loop repeats for <code>n_iterations</code>.</p> <p>Parameters:</p> Name Type Description Default <code>base_estimator</code> <code>BaseEstimator</code> <p>An instance of a Keras-based estimator with <code>fit</code>, <code>predict</code>, and <code>build_model</code> methods.</p> required <code>X</code> <code>array - like</code> <p>Training data (features).</p> required <code>y</code> <code>array - like</code> <p>Training data (targets).</p> required <code>validation_data</code> <code>tuple[array - like, array - like]</code> <p>Data (X_val, y_val) for evaluating model performance during refinement.</p> required <code>n_iterations</code> <code>int, default=5</code> <p>The number of refinement iterations.</p> <code>5</code> <code>**kwargs</code> <p>Additional keyword arguments passed to the <code>fit</code> method of the Keras estimator during each iteration (e.g., <code>epochs</code>, <code>batch_size</code>).</p> <code>{}</code> <p>Returns:</p> Type Description <code>BaseEstimator</code> <ul> <li>best_model: The Keras estimator instance with the best-performing <code>build_model</code> method found.</li> </ul> <code>list[tuple[str, float]]</code> <ul> <li>performance_log: A list of (code_string, validation_metric) tuples, recording each attempted <code>build_model</code> code and its score.</li> </ul> Source code in <code>src/centimators/keras_cortex.py</code> <pre><code>def think_loop(\n    self, base_estimator, X, y, validation_data, n_iterations=5, **kwargs\n) -&gt; tuple[BaseEstimator, list[tuple[str, float]]]:\n    \"\"\"Iteratively refine and retrain a Keras-based estimator.\n\n    This method forms the core of `KerasCortex`. It takes an initial Keras\n    estimator, trains it to get a baseline, and then enters a loop:\n    1.  The current `build_model` code is sent to the `Think` module.\n    2.  The `Think` module (using an LLM) suggests a modification to the code.\n    3.  A new model is created with the modified `build_model` method.\n    4.  The new model is trained and evaluated on validation data.\n    5.  If the new model performs better, its code becomes the current best.\n    This loop repeats for `n_iterations`.\n\n    Args:\n        base_estimator (BaseEstimator): An instance of a Keras-based estimator\n            with `fit`, `predict`, and `build_model` methods.\n        X (array-like): Training data (features).\n        y (array-like): Training data (targets).\n        validation_data (tuple[array-like, array-like]): Data (X_val, y_val) for\n            evaluating model performance during refinement.\n        n_iterations (int, default=5): The number of refinement iterations.\n        **kwargs: Additional keyword arguments passed to the `fit` method of the\n            Keras estimator during each iteration (e.g., `epochs`, `batch_size`).\n\n    Returns:\n        - best_model: The Keras estimator instance with the best-performing\n            `build_model` method found.\n        - performance_log: A list of (code_string, validation_metric)\n            tuples, recording each attempted `build_model` code and its score.\n    \"\"\"\n    # Initial baseline: clone the provided estimator and fit\n    baseline_model = clone(base_estimator)\n    baseline_model.fit(X, y, **kwargs)\n\n    X_val, y_val = validation_data\n    best_metric = baseline_model.score(X_val, y_val)\n    current_code = inspect.getsource(type(baseline_model).build_model)\n    performance_log = [(current_code, best_metric)]\n\n    best_model = baseline_model\n    suggestion = current_code\n\n    think = Think(verbose=self.verbose)\n    for i in range(n_iterations):\n        print(f\"\\n--- Iteration {i + 1} ---\")\n        try:\n            suggestion = think.forward(\n                current_keras_code=suggestion,\n                performance_log=performance_log,\n                optimization_goal=\"improve validation metrics (R2)\",\n            )\n            namespace = {}\n            exec(suggestion, globals(), namespace)\n            build_model_fn = namespace[\"build_model\"]\n\n            # clone from original base_estimator to avoid state pollution\n            new_model = clone(base_estimator)\n            new_model.build_model = types.MethodType(build_model_fn, new_model)\n            new_model.fit(X, y, **kwargs)\n            metric = new_model.score(X_val, y_val)\n\n            performance_log.append((suggestion, metric))\n            if metric &gt; best_metric:\n                print(\n                    f\"Improvement! New validation score: {metric:.4f} &gt; {best_metric:.4f}\"\n                )\n                best_metric = metric\n                best_model = new_model\n            else:\n                print(\n                    f\"No improvement ({metric:.4f} &lt;= {best_metric:.4f}), keeping best code.\"\n                )\n        except Exception as e:\n            print(\"Error during optimization iteration:\", e)\n            break\n\n    return best_model, performance_log\n</code></pre>"},{"location":"api-reference/keras_cortex/#centimators.keras_cortex.KerasCortex.fit","title":"<code>fit(X, y, validation_data=None, **kwargs)</code>","text":"<p>Fit the KerasCortex estimator.</p> <p>This method initiates the <code>think_loop</code> to find the best model architecture and then fits this best model. The primary purpose of <code>fit</code> is to expose a scikit-learn compatible API.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array - like</code> <p>Training data (features).</p> required <code>y</code> <code>array - like</code> <p>Training data (targets).</p> required <code>validation_data</code> <code>tuple[array - like, array - like]</code> <p>Data for evaluating model performance during the refinement loop. If None, KerasCortex cannot effectively optimize the model architecture.</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments passed to the <code>fit</code> method of the base Keras estimator during the <code>think_loop</code> (e.g., <code>epochs</code>, <code>batch_size</code>).</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>KerasCortex</code> <p>The fitted estimator instance.</p> Source code in <code>src/centimators/keras_cortex.py</code> <pre><code>def fit(self, X, y, validation_data=None, **kwargs):\n    \"\"\"Fit the KerasCortex estimator.\n\n    This method initiates the `think_loop` to find the best model architecture\n    and then fits this best model. The primary purpose of `fit` is to expose\n    a scikit-learn compatible API.\n\n    Args:\n        X (array-like): Training data (features).\n        y (array-like): Training data (targets).\n        validation_data (tuple[array-like, array-like], optional): Data for\n            evaluating model performance during the refinement loop. If None,\n            KerasCortex cannot effectively optimize the model architecture.\n        **kwargs: Additional keyword arguments passed to the `fit` method of the\n            base Keras estimator during the `think_loop` (e.g., `epochs`, `batch_size`).\n\n    Returns:\n        KerasCortex: The fitted estimator instance.\n    \"\"\"\n    self.best_model_, self.performance_log_ = self.think_loop(\n        base_estimator=self.base_estimator,\n        X=X,\n        y=y,\n        validation_data=validation_data,\n        n_iterations=self.n_iterations,\n        **kwargs,\n    )\n    return self\n</code></pre>"},{"location":"api-reference/keras_cortex/#centimators.keras_cortex.KerasCortex.predict","title":"<code>predict(X)</code>","text":"<p>Generate predictions using the best model found by KerasCortex.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array - like</code> <p>Input data (features) for which to make predictions.</p> required <p>Returns:</p> Type Description <p>array-like: Predictions from the <code>best_model_</code>.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the estimator has not been fitted (i.e., <code>fit</code> has not been called).</p> Source code in <code>src/centimators/keras_cortex.py</code> <pre><code>def predict(self, X):\n    \"\"\"Generate predictions using the best model found by KerasCortex.\n\n    Args:\n        X (array-like): Input data (features) for which to make predictions.\n\n    Returns:\n        array-like: Predictions from the `best_model_`.\n\n    Raises:\n        ValueError: If the estimator has not been fitted (i.e., `fit` has not\n            been called).\n    \"\"\"\n    if not hasattr(self, \"best_model_\"):\n        raise ValueError(\"Estimator not fitted. Call 'fit' first.\")\n    return self.best_model_.predict(X)\n</code></pre>"},{"location":"api-reference/losses/","title":"Losses","text":""},{"location":"api-reference/losses/#centimators.losses.SpearmanCorrelation","title":"<code>centimators.losses.SpearmanCorrelation</code>","text":"<p>               Bases: <code>Loss</code></p> <p>Differentiable Spearman rank correlation loss.</p> <p>This loss function computes a soft approximation of Spearman's rank correlation coefficient between predictions and targets. Unlike the standard non-differentiable rank correlation, this implementation uses sigmoid-based soft rankings that allow gradient flow during backpropagation.</p> <p>The loss is computed as the negative correlation (to minimize during training) between the soft ranks of predictions and targets.</p> <p>Parameters:</p> Name Type Description Default <code>regularization_strength</code> <code>float, default=1e-3</code> <p>Temperature parameter for the sigmoid function used in soft ranking. Smaller values create sharper (more discrete) rankings, while larger values create smoother approximations. Typically ranges from 1e-4 to 1e-1.</p> <code>0.001</code> <code>name</code> <code>str, default=\"spearman_correlation\"</code> <p>Name of the loss function.</p> <code>'spearman_correlation'</code> <code>**kwargs</code> <p>Additional keyword arguments passed to the base Loss class.</p> <code>{}</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import keras\n&gt;&gt;&gt; loss_fn = SpearmanCorrelation(regularization_strength=0.01)\n&gt;&gt;&gt; model = keras.Sequential([...])\n&gt;&gt;&gt; model.compile(optimizer='adam', loss=loss_fn)\n</code></pre> Source code in <code>src/centimators/losses.py</code> <pre><code>class SpearmanCorrelation(Loss):\n    \"\"\"Differentiable Spearman rank correlation loss.\n\n    This loss function computes a soft approximation of Spearman's rank\n    correlation coefficient between predictions and targets. Unlike the\n    standard non-differentiable rank correlation, this implementation uses\n    sigmoid-based soft rankings that allow gradient flow during backpropagation.\n\n    The loss is computed as the negative correlation (to minimize during training)\n    between the soft ranks of predictions and targets.\n\n    Args:\n        regularization_strength (float, default=1e-3): Temperature parameter for\n            the sigmoid function used in soft ranking. Smaller values create\n            sharper (more discrete) rankings, while larger values create smoother\n            approximations. Typically ranges from 1e-4 to 1e-1.\n        name (str, default=\"spearman_correlation\"): Name of the loss function.\n        **kwargs: Additional keyword arguments passed to the base Loss class.\n\n    Examples:\n        &gt;&gt;&gt; import keras\n        &gt;&gt;&gt; loss_fn = SpearmanCorrelation(regularization_strength=0.01)\n        &gt;&gt;&gt; model = keras.Sequential([...])\n        &gt;&gt;&gt; model.compile(optimizer='adam', loss=loss_fn)\n    \"\"\"\n\n    def __init__(\n        self, regularization_strength=1e-3, name=\"spearman_correlation\", **kwargs\n    ):\n        super().__init__(name=name, **kwargs)\n        self.regularization_strength = regularization_strength\n\n    def call(self, y_true, y_pred):\n        \"\"\"Compute the Spearman correlation loss.\n\n        Args:\n            y_true: Ground truth values of shape (batch_size,) or (batch_size, 1).\n            y_pred: Predicted values of shape (batch_size,) or (batch_size, 1).\n\n        Returns:\n            Scalar loss value (negative correlation).\n        \"\"\"\n        # Reshape inputs to ensure 2D\n        y_true = K.reshape(y_true, (-1, 1))\n        y_pred = K.reshape(y_pred, (-1, 1))\n\n        # Calculate soft ranks for both true and predicted values\n        true_ranks = self._soft_rank(y_true)\n        pred_ranks = self._soft_rank(y_pred)\n\n        # Calculate correlation between ranks\n        return -self._correlation(true_ranks, pred_ranks)\n\n    def _soft_rank(self, x):\n        \"\"\"Compute differentiable soft ranks using sigmoid approximation.\n\n        Args:\n            x: Input tensor of shape (batch_size, 1).\n\n        Returns:\n            Soft ranks tensor of shape (batch_size, 1).\n        \"\"\"\n        # Create pairwise differences matrix\n        x_expanded1 = K.expand_dims(x, 1)\n        x_expanded2 = K.expand_dims(x, 0)\n        diff = x_expanded1 - x_expanded2\n\n        # Apply soft step function\n        soft_step = K.sigmoid(diff / self.regularization_strength)\n\n        # Sum over rows to get ranks\n        ranks = K.sum(soft_step, axis=1)\n        return ranks\n\n    def _correlation(self, x, y):\n        \"\"\"Compute Pearson correlation between two tensors.\n\n        Args:\n            x: First tensor of shape (batch_size, 1).\n            y: Second tensor of shape (batch_size, 1).\n\n        Returns:\n            Scalar correlation value in range [-1, 1].\n        \"\"\"\n        # Mean center\n        x_centered = x - K.mean(x)\n        y_centered = y - K.mean(y)\n\n        # Calculate correlation\n        numerator = K.sum(x_centered * y_centered)\n        denominator = K.sqrt(\n            K.sum(K.square(x_centered)) * K.sum(K.square(y_centered)) + epsilon()\n        )\n\n        return numerator / denominator\n</code></pre>"},{"location":"api-reference/losses/#centimators.losses.SpearmanCorrelation.call","title":"<code>call(y_true, y_pred)</code>","text":"<p>Compute the Spearman correlation loss.</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <p>Ground truth values of shape (batch_size,) or (batch_size, 1).</p> required <code>y_pred</code> <p>Predicted values of shape (batch_size,) or (batch_size, 1).</p> required <p>Returns:</p> Type Description <p>Scalar loss value (negative correlation).</p> Source code in <code>src/centimators/losses.py</code> <pre><code>def call(self, y_true, y_pred):\n    \"\"\"Compute the Spearman correlation loss.\n\n    Args:\n        y_true: Ground truth values of shape (batch_size,) or (batch_size, 1).\n        y_pred: Predicted values of shape (batch_size,) or (batch_size, 1).\n\n    Returns:\n        Scalar loss value (negative correlation).\n    \"\"\"\n    # Reshape inputs to ensure 2D\n    y_true = K.reshape(y_true, (-1, 1))\n    y_pred = K.reshape(y_pred, (-1, 1))\n\n    # Calculate soft ranks for both true and predicted values\n    true_ranks = self._soft_rank(y_true)\n    pred_ranks = self._soft_rank(y_pred)\n\n    # Calculate correlation between ranks\n    return -self._correlation(true_ranks, pred_ranks)\n</code></pre>"},{"location":"api-reference/losses/#centimators.losses.CombinedLoss","title":"<code>centimators.losses.CombinedLoss</code>","text":"<p>               Bases: <code>Loss</code></p> <p>Weighted combination of MSE and Spearman correlation losses.</p> <p>This loss function combines mean squared error (for absolute accuracy) with Spearman correlation loss (for rank preservation). This can be particularly useful when both the exact values and their relative ordering are important.</p> <p>Parameters:</p> Name Type Description Default <code>mse_weight</code> <code>float, default=2.0</code> <p>Weight applied to the MSE component. Higher values prioritize absolute accuracy.</p> <code>2.0</code> <code>spearman_weight</code> <code>float, default=1.0</code> <p>Weight applied to the Spearman correlation component. Higher values prioritize rank preservation.</p> <code>1.0</code> <code>spearman_regularization</code> <code>float, default=1e-3</code> <p>Regularization strength passed to the SpearmanCorrelation loss.</p> <code>0.001</code> <code>name</code> <code>str, default=\"combined_loss\"</code> <p>Name of the loss function.</p> <code>'combined_loss'</code> <code>**kwargs</code> <p>Additional keyword arguments passed to the base Loss class.</p> <code>{}</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # Prioritize ranking accuracy over absolute values\n&gt;&gt;&gt; loss_fn = CombinedLoss(mse_weight=0.5, spearman_weight=2.0)\n&gt;&gt;&gt; model.compile(optimizer='adam', loss=loss_fn)\n</code></pre> Source code in <code>src/centimators/losses.py</code> <pre><code>class CombinedLoss(Loss):\n    \"\"\"Weighted combination of MSE and Spearman correlation losses.\n\n    This loss function combines mean squared error (for absolute accuracy)\n    with Spearman correlation loss (for rank preservation). This can be\n    particularly useful when both the exact values and their relative\n    ordering are important.\n\n    Args:\n        mse_weight (float, default=2.0): Weight applied to the MSE component.\n            Higher values prioritize absolute accuracy.\n        spearman_weight (float, default=1.0): Weight applied to the Spearman\n            correlation component. Higher values prioritize rank preservation.\n        spearman_regularization (float, default=1e-3): Regularization strength\n            passed to the SpearmanCorrelation loss.\n        name (str, default=\"combined_loss\"): Name of the loss function.\n        **kwargs: Additional keyword arguments passed to the base Loss class.\n\n    Examples:\n        &gt;&gt;&gt; # Prioritize ranking accuracy over absolute values\n        &gt;&gt;&gt; loss_fn = CombinedLoss(mse_weight=0.5, spearman_weight=2.0)\n        &gt;&gt;&gt; model.compile(optimizer='adam', loss=loss_fn)\n    \"\"\"\n\n    def __init__(\n        self,\n        mse_weight=2.0,\n        spearman_weight=1.0,\n        spearman_regularization=1e-3,\n        name=\"combined_loss\",\n        **kwargs,\n    ):\n        super().__init__(name=name, **kwargs)\n        self.mse_weight = mse_weight\n        self.spearman_weight = spearman_weight\n        self.spearman_loss = SpearmanCorrelation(\n            regularization_strength=spearman_regularization\n        )\n\n    def call(self, y_true, y_pred):\n        \"\"\"Compute the combined loss.\n\n        Args:\n            y_true: Ground truth values of shape (batch_size,) or (batch_size, 1).\n            y_pred: Predicted values of shape (batch_size,) or (batch_size, 1).\n\n        Returns:\n            Scalar loss value (weighted sum of MSE and negative Spearman correlation).\n        \"\"\"\n        mse = K.mean(K.square(y_pred - y_true))\n        spearman = self.spearman_loss(y_true, y_pred)\n\n        return self.mse_weight * mse + self.spearman_weight * spearman\n</code></pre>"},{"location":"api-reference/losses/#centimators.losses.CombinedLoss.call","title":"<code>call(y_true, y_pred)</code>","text":"<p>Compute the combined loss.</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <p>Ground truth values of shape (batch_size,) or (batch_size, 1).</p> required <code>y_pred</code> <p>Predicted values of shape (batch_size,) or (batch_size, 1).</p> required <p>Returns:</p> Type Description <p>Scalar loss value (weighted sum of MSE and negative Spearman correlation).</p> Source code in <code>src/centimators/losses.py</code> <pre><code>def call(self, y_true, y_pred):\n    \"\"\"Compute the combined loss.\n\n    Args:\n        y_true: Ground truth values of shape (batch_size,) or (batch_size, 1).\n        y_pred: Predicted values of shape (batch_size,) or (batch_size, 1).\n\n    Returns:\n        Scalar loss value (weighted sum of MSE and negative Spearman correlation).\n    \"\"\"\n    mse = K.mean(K.square(y_pred - y_true))\n    spearman = self.spearman_loss(y_true, y_pred)\n\n    return self.mse_weight * mse + self.spearman_weight * spearman\n</code></pre>"},{"location":"api-reference/model_estimators/","title":"Model Estimators","text":""},{"location":"api-reference/model_estimators/#centimators.model_estimators","title":"<code>centimators.model_estimators</code>","text":"<p>Model estimator abstractions that combine Keras with the scikit-learn API.</p> <p>This module exposes two estimator classes that conform to scikit-learn's <code>BaseEstimator</code>/<code>TransformerMixin</code> contracts while delegating all heavy\u2010 lifting to Keras.  The goal is to let neural networks participate in classic ML pipelines without boilerplate.</p> Highlights <ul> <li>Drop-in compatibility \u2013 works with <code>sklearn.pipeline.Pipeline</code>,   <code>GridSearchCV</code>, etc.</li> <li>Distribution strategies \u2013 opt-in data-parallel training across   multiple devices/GPUs.</li> <li>Sequence support \u2013 :class:<code>SequenceEstimator</code> reshapes a flattened   lag matrix into the 3-D tensor expected by recurrent or convolutional   sequence layers.</li> </ul>"},{"location":"api-reference/model_estimators/#centimators.model_estimators.BaseKerasEstimator","title":"<code>BaseKerasEstimator</code>  <code>dataclass</code>","text":"<p>               Bases: <code>TransformerMixin</code>, <code>BaseEstimator</code>, <code>ABC</code></p> <p>Meta-estimator for Keras models following the scikit-learn API.</p> <p>Parameters:</p> Name Type Description Default <code>output_units</code> <code>int, default=1</code> <p>Dimensionality of the model output. It is forwarded to :meth:<code>build_model</code> and can be used there when constructing the final layer.</p> <code>1</code> <code>optimizer</code> <code>Type[optimizers.Optimizer], default=keras.optimizers.Adam</code> <p>Optimiser class not instance. The class is instantiated in :meth:<code>fit</code> with the requested <code>learning_rate</code>.</p> <code>Adam</code> <code>learning_rate</code> <code>float, default=1e-3</code> <p>Learning-rate passed to the optimiser constructor.</p> <code>0.001</code> <code>loss_function</code> <code>str or keras.losses.Loss, default=\"mse\"</code> <p>Loss forwarded to <code>model.compile</code>.</p> <code>'mse'</code> <code>metrics</code> <code>list[str] | None, default=None</code> <p>List of metrics forwarded to <code>model.compile</code>.</p> <code>None</code> <code>model</code> <code>keras.Model | None, default=None</code> <p>Internal Keras model instance. If None it is lazily built on the first call to :meth:<code>fit</code>.</p> <code>None</code> <code>distribution_strategy</code> <code>str | None, default=None</code> <p>Name of a Keras distribution strategy to activate before training. At the moment only <code>\"DataParallel\"</code> is recognised.</p> <code>None</code> <p>Attributes:</p> Name Type Description <code>_n_features_in_</code> <code>int | None</code> <p>Inferred number of features from the data passed to :meth:<code>fit</code>.</p> Notes <p>Sub-classes must implement :meth:<code>build_model</code> which should return a compiled (or at least constructed) <code>keras.Model</code> instance.</p> Source code in <code>src/centimators/model_estimators.py</code> <pre><code>@dataclass(kw_only=True)\nclass BaseKerasEstimator(TransformerMixin, BaseEstimator, ABC):\n    \"\"\"Meta-estimator for Keras models following the scikit-learn API.\n\n    Args:\n        output_units (int, default=1): Dimensionality of the model output.\n            It is forwarded to :meth:`build_model` and can be used there when\n            constructing the final layer.\n        optimizer (Type[optimizers.Optimizer], default=keras.optimizers.Adam):\n            Optimiser class **not instance**. The class is instantiated in\n            :meth:`fit` with the requested ``learning_rate``.\n        learning_rate (float, default=1e-3): Learning-rate passed to the\n            optimiser constructor.\n        loss_function (str or keras.losses.Loss, default=\"mse\"): Loss\n            forwarded to ``model.compile``.\n        metrics (list[str] | None, default=None): List of metrics forwarded\n            to ``model.compile``.\n        model (keras.Model | None, default=None): Internal Keras model instance.\n            If *None* it is lazily built on the first call to :meth:`fit`.\n        distribution_strategy (str | None, default=None): Name of a Keras\n            distribution strategy to activate before training. At the moment\n            only ``\"DataParallel\"`` is recognised.\n\n    Attributes:\n        _n_features_in_ (int | None): Inferred number of features from the data\n            passed to :meth:`fit`.\n\n    Notes:\n        Sub-classes **must** implement :meth:`build_model` which should return\n        a compiled (or at least constructed) ``keras.Model`` instance.\n    \"\"\"\n\n    output_units: int = 1\n    optimizer: Type[optimizers.Optimizer] = optimizers.Adam\n    learning_rate: float = 0.001\n    loss_function: str = \"mse\"\n    metrics: list[str] | None = None\n    model: Any = None\n    distribution_strategy: str | None = None\n\n    @abstractmethod\n    def build_model(self):\n        pass\n\n    def _setup_distribution_strategy(self) -&gt; None:\n        \"\"\"Activate a distribution strategy for multi-device training.\n\n        The current implementation always uses\n        ``keras.distribution.DataParallel`` which mirrors the model on all\n        available devices and splits the batch.  Support for additional\n        strategies can be added later.\n        \"\"\"\n        # TODO: allow for different distribution strategies\n        strategy = distribution.DataParallel()\n        distribution.set_distribution(strategy)\n\n    def fit(\n        self,\n        X,\n        y,\n        epochs: int = 100,\n        batch_size: int = 32,\n        validation_data: tuple[Any, Any] | None = None,\n        callbacks: list[Any] | None = None,\n        **kwargs: Any,\n    ) -&gt; \"BaseKerasEstimator\":\n        \"\"\"Fit the underlying Keras model.\n\n        The model is **lazily** built and compiled on the first call. All\n        extra keyword arguments are forwarded to ``keras.Model.fit``.\n\n        Args:\n            X (array-like): Training data of shape (n_samples, n_features).\n            y (array-like): Training targets of shape (n_samples,) or (n_samples, n_outputs).\n            epochs (int, default=100): Number of training epochs.\n            batch_size (int, default=32): Minibatch size.\n            validation_data (tuple[Any, Any] | None, default=None): Optional\n                validation split forwarded to Keras.\n            callbacks (list[Any] | None, default=None): Optional list of callbacks.\n            **kwargs: Additional keyword arguments forwarded to ``keras.Model.fit``.\n\n        Returns:\n            BaseKerasEstimator: Fitted estimator.\n        \"\"\"\n        self._n_features_in_ = X.shape[1]\n\n        if self.distribution_strategy:\n            self._setup_distribution_strategy()\n\n        if not self.model:\n            self.build_model()\n\n        self.model.fit(\n            _ensure_numpy(X),\n            y=_ensure_numpy(y, allow_series=True),\n            batch_size=batch_size,\n            epochs=epochs,\n            validation_data=validation_data,\n            callbacks=callbacks,\n            **kwargs,\n        )\n        self._is_fitted = True\n        return self\n\n    def predict(self, X, batch_size: int = 512, **kwargs: Any) -&gt; Any:\n        \"\"\"Generate predictions with the trained model.\n\n        Args:\n            X (array-like): Input samples of shape (n_samples, n_features).\n            batch_size (int, default=512): Batch size used for inference.\n            **kwargs: Additional keyword arguments forwarded to ``keras.Model.predict``.\n\n        Returns:\n            Any: Model predictions of shape (n_samples, output_units)\n                in the same order as *X*.\n        \"\"\"\n        if not self.model:\n            raise ValueError(\"Model not built. Call `build_model` first.\")\n\n        return self.model.predict(X, batch_size=batch_size, **kwargs)\n\n    def transform(self, X, **kwargs):\n        \"\"\"Alias for :meth:`predict` to comply with scikit-learn pipelines.\"\"\"\n        return self.predict(X, **kwargs)\n\n    def __sklearn_is_fitted__(self) -&gt; bool:\n        \"\"\"Return ``True`` when the estimator has been fitted.\n\n        scikit-learn relies on :func:`sklearn.utils.validation.check_is_fitted`\n        to decide whether an estimator is ready for inference.\n        \"\"\"\n        return getattr(self, \"_is_fitted\", False)\n</code></pre>"},{"location":"api-reference/model_estimators/#centimators.model_estimators.BaseKerasEstimator.fit","title":"<code>fit(X, y, epochs=100, batch_size=32, validation_data=None, callbacks=None, **kwargs)</code>","text":"<p>Fit the underlying Keras model.</p> <p>The model is lazily built and compiled on the first call. All extra keyword arguments are forwarded to <code>keras.Model.fit</code>.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array - like</code> <p>Training data of shape (n_samples, n_features).</p> required <code>y</code> <code>array - like</code> <p>Training targets of shape (n_samples,) or (n_samples, n_outputs).</p> required <code>epochs</code> <code>int, default=100</code> <p>Number of training epochs.</p> <code>100</code> <code>batch_size</code> <code>int, default=32</code> <p>Minibatch size.</p> <code>32</code> <code>validation_data</code> <code>tuple[Any, Any] | None, default=None</code> <p>Optional validation split forwarded to Keras.</p> <code>None</code> <code>callbacks</code> <code>list[Any] | None, default=None</code> <p>Optional list of callbacks.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments forwarded to <code>keras.Model.fit</code>.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>BaseKerasEstimator</code> <code>BaseKerasEstimator</code> <p>Fitted estimator.</p> Source code in <code>src/centimators/model_estimators.py</code> <pre><code>def fit(\n    self,\n    X,\n    y,\n    epochs: int = 100,\n    batch_size: int = 32,\n    validation_data: tuple[Any, Any] | None = None,\n    callbacks: list[Any] | None = None,\n    **kwargs: Any,\n) -&gt; \"BaseKerasEstimator\":\n    \"\"\"Fit the underlying Keras model.\n\n    The model is **lazily** built and compiled on the first call. All\n    extra keyword arguments are forwarded to ``keras.Model.fit``.\n\n    Args:\n        X (array-like): Training data of shape (n_samples, n_features).\n        y (array-like): Training targets of shape (n_samples,) or (n_samples, n_outputs).\n        epochs (int, default=100): Number of training epochs.\n        batch_size (int, default=32): Minibatch size.\n        validation_data (tuple[Any, Any] | None, default=None): Optional\n            validation split forwarded to Keras.\n        callbacks (list[Any] | None, default=None): Optional list of callbacks.\n        **kwargs: Additional keyword arguments forwarded to ``keras.Model.fit``.\n\n    Returns:\n        BaseKerasEstimator: Fitted estimator.\n    \"\"\"\n    self._n_features_in_ = X.shape[1]\n\n    if self.distribution_strategy:\n        self._setup_distribution_strategy()\n\n    if not self.model:\n        self.build_model()\n\n    self.model.fit(\n        _ensure_numpy(X),\n        y=_ensure_numpy(y, allow_series=True),\n        batch_size=batch_size,\n        epochs=epochs,\n        validation_data=validation_data,\n        callbacks=callbacks,\n        **kwargs,\n    )\n    self._is_fitted = True\n    return self\n</code></pre>"},{"location":"api-reference/model_estimators/#centimators.model_estimators.BaseKerasEstimator.predict","title":"<code>predict(X, batch_size=512, **kwargs)</code>","text":"<p>Generate predictions with the trained model.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array - like</code> <p>Input samples of shape (n_samples, n_features).</p> required <code>batch_size</code> <code>int, default=512</code> <p>Batch size used for inference.</p> <code>512</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments forwarded to <code>keras.Model.predict</code>.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>Any</code> <code>Any</code> <p>Model predictions of shape (n_samples, output_units) in the same order as X.</p> Source code in <code>src/centimators/model_estimators.py</code> <pre><code>def predict(self, X, batch_size: int = 512, **kwargs: Any) -&gt; Any:\n    \"\"\"Generate predictions with the trained model.\n\n    Args:\n        X (array-like): Input samples of shape (n_samples, n_features).\n        batch_size (int, default=512): Batch size used for inference.\n        **kwargs: Additional keyword arguments forwarded to ``keras.Model.predict``.\n\n    Returns:\n        Any: Model predictions of shape (n_samples, output_units)\n            in the same order as *X*.\n    \"\"\"\n    if not self.model:\n        raise ValueError(\"Model not built. Call `build_model` first.\")\n\n    return self.model.predict(X, batch_size=batch_size, **kwargs)\n</code></pre>"},{"location":"api-reference/model_estimators/#centimators.model_estimators.BaseKerasEstimator.transform","title":"<code>transform(X, **kwargs)</code>","text":"<p>Alias for :meth:<code>predict</code> to comply with scikit-learn pipelines.</p> Source code in <code>src/centimators/model_estimators.py</code> <pre><code>def transform(self, X, **kwargs):\n    \"\"\"Alias for :meth:`predict` to comply with scikit-learn pipelines.\"\"\"\n    return self.predict(X, **kwargs)\n</code></pre>"},{"location":"api-reference/model_estimators/#centimators.model_estimators.BaseKerasEstimator.__sklearn_is_fitted__","title":"<code>__sklearn_is_fitted__()</code>","text":"<p>Return <code>True</code> when the estimator has been fitted.</p> <p>scikit-learn relies on :func:<code>sklearn.utils.validation.check_is_fitted</code> to decide whether an estimator is ready for inference.</p> Source code in <code>src/centimators/model_estimators.py</code> <pre><code>def __sklearn_is_fitted__(self) -&gt; bool:\n    \"\"\"Return ``True`` when the estimator has been fitted.\n\n    scikit-learn relies on :func:`sklearn.utils.validation.check_is_fitted`\n    to decide whether an estimator is ready for inference.\n    \"\"\"\n    return getattr(self, \"_is_fitted\", False)\n</code></pre>"},{"location":"api-reference/model_estimators/#centimators.model_estimators.SequenceEstimator","title":"<code>SequenceEstimator</code>  <code>dataclass</code>","text":"<p>               Bases: <code>BaseKerasEstimator</code></p> <p>Estimator for models that consume sequential data.</p> <p>The class assumes that X is a flattened 2-D representation of a sequence built from multiple lagged views of the original signal. The shape transformation performed by :meth:<code>_reshape</code> is visualised below for a concrete example.</p> <p>Parameters:</p> Name Type Description Default <code>lag_windows</code> <code>list[int]</code> <p>Offsets (in number of timesteps) that have been concatenated to form the flattened design matrix.</p> required <code>n_features_per_timestep</code> <code>int</code> <p>Number of original features per timestep before creating the lags.</p> required <p>Attributes:</p> Name Type Description <code>seq_length</code> <code>int</code> <p>Inferred sequence length from lag_windows.</p> Source code in <code>src/centimators/model_estimators.py</code> <pre><code>@dataclass(kw_only=True)\nclass SequenceEstimator(BaseKerasEstimator):\n    \"\"\"Estimator for models that consume sequential data.\n\n    The class assumes that *X* is a **flattened** 2-D representation of a\n    sequence built from multiple lagged views of the original signal.\n    The shape transformation performed by :meth:`_reshape` is visualised\n    below for a concrete example.\n\n    Args:\n        lag_windows (list[int]): Offsets (in number of timesteps) that have been\n            concatenated to form the flattened design matrix.\n        n_features_per_timestep (int): Number of *original* features per timestep\n            **before** creating the lags.\n\n    Attributes:\n        seq_length (int): Inferred sequence length from lag_windows.\n    \"\"\"\n\n    lag_windows: list[int]\n    n_features_per_timestep: int\n\n    def __post_init__(self):\n        self.seq_length = len(self.lag_windows)\n\n    def _reshape(self, X: IntoFrame, validation_data: tuple[Any, Any] | None = None):\n        \"\"\"Reshape a flattened lag matrix into a 3-D tensor.\n\n        Args:\n            X (IntoFrame): Design matrix containing the lagged features.\n            validation_data (tuple[Any, Any] | None, default=None): Optional\n                validation split; its *X* part will be reshaped in the same way.\n\n        Returns:\n            tuple[numpy.ndarray, tuple[Any, Any] | None]:\n                A tuple containing the reshaped training data (numpy.ndarray with shape\n                ``(n_samples, seq_length, n_features_per_timestep)``) and the\n                (potentially reshaped) validation data.\n        \"\"\"\n        X = _ensure_numpy(X)\n        X_reshaped = ops.reshape(\n            X, (X.shape[0], self.seq_length, self.n_features_per_timestep)\n        )\n\n        if validation_data:\n            X_val, y_val = validation_data\n            X_val = _ensure_numpy(X_val)\n            X_val_reshaped = ops.reshape(\n                X_val,\n                (X_val.shape[0], self.seq_length, self.n_features_per_timestep),\n            )\n            validation_data = X_val_reshaped, _ensure_numpy(y_val)\n\n        return X_reshaped, validation_data\n\n    def fit(\n        self, X, y, validation_data: tuple[Any, Any] | None = None, **kwargs: Any\n    ) -&gt; \"SequenceEstimator\":\n        \"\"\"Redefines :meth:`BaseKerasEstimator.fit`\n        to include reshaping for sequence data.\n\n        Args:\n            X (array-like): Training data.\n            y (array-like): Training targets.\n            validation_data (tuple[Any, Any] | None, default=None): Optional\n                validation split.\n            **kwargs: Additional keyword arguments passed to the parent fit method.\n\n        Returns:\n            SequenceEstimator: Fitted estimator.\n        \"\"\"\n        X_reshaped, validation_data_reshaped = self._reshape(X, validation_data)\n        super().fit(\n            X_reshaped,\n            y=_ensure_numpy(y),\n            validation_data=validation_data_reshaped,\n            **kwargs,\n        )\n        return self\n\n    def predict(self, X, **kwargs: Any) -&gt; numpy.ndarray:\n        \"\"\"Redefines :meth:`BaseKerasEstimator.predict`\n        to include reshaping for sequence data.\n\n        Args:\n            X (array-like): Input data.\n            **kwargs: Additional keyword arguments passed to the parent predict method.\n\n        Returns:\n            numpy.ndarray: Predictions of shape (n_samples, output_units).\n        \"\"\"\n        X_reshaped, _ = self._reshape(X)\n        return super().predict(X_reshaped, **kwargs)\n</code></pre>"},{"location":"api-reference/model_estimators/#centimators.model_estimators.SequenceEstimator.fit","title":"<code>fit(X, y, validation_data=None, **kwargs)</code>","text":"<p>Redefines :meth:<code>BaseKerasEstimator.fit</code> to include reshaping for sequence data.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array - like</code> <p>Training data.</p> required <code>y</code> <code>array - like</code> <p>Training targets.</p> required <code>validation_data</code> <code>tuple[Any, Any] | None, default=None</code> <p>Optional validation split.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments passed to the parent fit method.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>SequenceEstimator</code> <code>SequenceEstimator</code> <p>Fitted estimator.</p> Source code in <code>src/centimators/model_estimators.py</code> <pre><code>def fit(\n    self, X, y, validation_data: tuple[Any, Any] | None = None, **kwargs: Any\n) -&gt; \"SequenceEstimator\":\n    \"\"\"Redefines :meth:`BaseKerasEstimator.fit`\n    to include reshaping for sequence data.\n\n    Args:\n        X (array-like): Training data.\n        y (array-like): Training targets.\n        validation_data (tuple[Any, Any] | None, default=None): Optional\n            validation split.\n        **kwargs: Additional keyword arguments passed to the parent fit method.\n\n    Returns:\n        SequenceEstimator: Fitted estimator.\n    \"\"\"\n    X_reshaped, validation_data_reshaped = self._reshape(X, validation_data)\n    super().fit(\n        X_reshaped,\n        y=_ensure_numpy(y),\n        validation_data=validation_data_reshaped,\n        **kwargs,\n    )\n    return self\n</code></pre>"},{"location":"api-reference/model_estimators/#centimators.model_estimators.SequenceEstimator.predict","title":"<code>predict(X, **kwargs)</code>","text":"<p>Redefines :meth:<code>BaseKerasEstimator.predict</code> to include reshaping for sequence data.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array - like</code> <p>Input data.</p> required <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments passed to the parent predict method.</p> <code>{}</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>numpy.ndarray: Predictions of shape (n_samples, output_units).</p> Source code in <code>src/centimators/model_estimators.py</code> <pre><code>def predict(self, X, **kwargs: Any) -&gt; numpy.ndarray:\n    \"\"\"Redefines :meth:`BaseKerasEstimator.predict`\n    to include reshaping for sequence data.\n\n    Args:\n        X (array-like): Input data.\n        **kwargs: Additional keyword arguments passed to the parent predict method.\n\n    Returns:\n        numpy.ndarray: Predictions of shape (n_samples, output_units).\n    \"\"\"\n    X_reshaped, _ = self._reshape(X)\n    return super().predict(X_reshaped, **kwargs)\n</code></pre>"},{"location":"api-reference/model_estimators/#centimators.model_estimators.MLPRegressor","title":"<code>MLPRegressor</code>  <code>dataclass</code>","text":"<p>               Bases: <code>RegressorMixin</code>, <code>BaseKerasEstimator</code></p> <p>A minimal fully-connected multi-layer perceptron for tabular data.</p> <p>The class follows the scikit-learn estimator interface while delegating the heavy lifting to Keras.  It is intended as a sensible baseline model that works out of the box with classic ML workflows such as pipelines or cross-validation.</p> <p>Parameters:</p> Name Type Description Default <code>hidden_units</code> <code>tuple[int, ...], default=(64, 64</code> <p>Width (number of neurons) for each hidden layer.  The length of the tuple defines the depth of the network.</p> <code>(64, 64)</code> <code>activation</code> <code>str, default=\"relu\"</code> <p>Activation function applied after each hidden <code>Dense</code> layer.</p> <code>'relu'</code> <code>dropout_rate</code> <code>float, default=0.0</code> <p>Optional dropout applied after each hidden layer.  Set to 0 to disable dropout entirely.</p> <code>0.0</code> <code>output_units</code> <code>int, default=1</code> <p>Copied from :class:<code>BaseKerasEstimator</code>. Defines the dimensionality of the final layer.</p> <code>1</code> <p>Attributes:</p> Name Type Description <code>_n_features_in_</code> <code>int | None</code> <p>Inferred number of features from the data passed to :meth:<code>fit</code>.</p> Source code in <code>src/centimators/model_estimators.py</code> <pre><code>@dataclass(kw_only=True)\nclass MLPRegressor(RegressorMixin, BaseKerasEstimator):\n    \"\"\"A minimal fully-connected multi-layer perceptron for tabular data.\n\n    The class follows the scikit-learn *estimator* interface while delegating\n    the heavy lifting to Keras.  It is intended as a sensible baseline model\n    that works *out of the box* with classic ML workflows such as pipelines or\n    cross-validation.\n\n    Args:\n        hidden_units (tuple[int, ...], default=(64, 64)): Width (number of\n            neurons) for each hidden layer.  The length of the tuple defines\n            the depth of the network.\n        activation (str, default=\"relu\"): Activation function applied after\n            each hidden ``Dense`` layer.\n        dropout_rate (float, default=0.0): Optional dropout applied **after**\n            each hidden layer.  Set to *0* to disable dropout entirely.\n        output_units (int, default=1): Copied from :class:`BaseKerasEstimator`.\n            Defines the dimensionality of the final layer.\n\n    Attributes:\n        _n_features_in_ (int | None): Inferred number of features from the data\n            passed to :meth:`fit`.\n    \"\"\"\n\n    hidden_units: tuple[int, ...] = (64, 64)\n    activation: str = \"relu\"\n    dropout_rate: float = 0.0\n    metrics: list[str] | None = field(default_factory=lambda: [\"mse\"])\n\n    def build_model(self):\n        \"\"\"Construct a simple MLP with the configured hyper-parameters.\"\"\"\n        inputs = layers.Input(shape=(self._n_features_in_,), name=\"features\")\n        x = inputs\n        for units in self.hidden_units:\n            x = layers.Dense(units, activation=self.activation)(x)\n            if self.dropout_rate &gt; 0:\n                x = layers.Dropout(self.dropout_rate)(x)\n        outputs = layers.Dense(self.output_units, activation=\"linear\")(x)\n        self.model = models.Model(inputs=inputs, outputs=outputs, name=\"mlp_regressor\")\n\n        self.model.compile(\n            optimizer=self.optimizer(learning_rate=self.learning_rate),\n            loss=self.loss_function,\n            metrics=self.metrics,\n        )\n\n        return self\n</code></pre>"},{"location":"api-reference/model_estimators/#centimators.model_estimators.MLPRegressor.build_model","title":"<code>build_model()</code>","text":"<p>Construct a simple MLP with the configured hyper-parameters.</p> Source code in <code>src/centimators/model_estimators.py</code> <pre><code>def build_model(self):\n    \"\"\"Construct a simple MLP with the configured hyper-parameters.\"\"\"\n    inputs = layers.Input(shape=(self._n_features_in_,), name=\"features\")\n    x = inputs\n    for units in self.hidden_units:\n        x = layers.Dense(units, activation=self.activation)(x)\n        if self.dropout_rate &gt; 0:\n            x = layers.Dropout(self.dropout_rate)(x)\n    outputs = layers.Dense(self.output_units, activation=\"linear\")(x)\n    self.model = models.Model(inputs=inputs, outputs=outputs, name=\"mlp_regressor\")\n\n    self.model.compile(\n        optimizer=self.optimizer(learning_rate=self.learning_rate),\n        loss=self.loss_function,\n        metrics=self.metrics,\n    )\n\n    return self\n</code></pre>"},{"location":"api-reference/model_estimators/#centimators.model_estimators.BottleneckEncoder","title":"<code>BottleneckEncoder</code>  <code>dataclass</code>","text":"<p>               Bases: <code>BaseKerasEstimator</code></p> <p>A bottleneck autoencoder that can learn latent representations and predict targets.</p> <p>This estimator implements a bottleneck autoencoder architecture that: 1. Encodes input features to a lower-dimensional latent space 2. Decodes the latent representation back to reconstruct the input 3. Uses an additional MLP branch to predict targets from the decoded features</p> <p>The model can be used both as a regressor (via predict) and as a transformer (via transform) to get latent space representations for dimensionality reduction.</p> <p>Parameters:</p> Name Type Description Default <code>gaussian_noise</code> <code>float, default=0.035</code> <p>Standard deviation of Gaussian noise applied to inputs for regularization.</p> <code>0.035</code> <code>encoder_units</code> <code>list[tuple[int, float]], default=[(1024, 0.1)]</code> <p>List of (units, dropout_rate) tuples defining the encoder architecture.</p> <code>lambda: [(1024, 0.1)]()</code> <code>latent_units</code> <code>tuple[int, float], default=(256, 0.1</code> <p>Tuple of (units, dropout_rate) for the latent bottleneck layer.</p> <code>(256, 0.1)</code> <code>ae_units</code> <code>list[tuple[int, float]], default=[(96, 0.4)]</code> <p>List of (units, dropout_rate) tuples for the autoencoder prediction branch.</p> <code>lambda: [(96, 0.4)]()</code> <code>activation</code> <code>str, default=\"swish\"</code> <p>Activation function used throughout the network.</p> <code>'swish'</code> <code>reconstruction_loss_weight</code> <code>float, default=1.0</code> <p>Weight for the reconstruction loss.</p> <code>1.0</code> <code>target_loss_weight</code> <code>float, default=1.0</code> <p>Weight for the target prediction loss.</p> <code>1.0</code> <p>Attributes:</p> Name Type Description <code>encoder</code> <code>Model</code> <p>The encoder submodel for transforming inputs to latent space.</p> Source code in <code>src/centimators/model_estimators.py</code> <pre><code>@dataclass(kw_only=True)\nclass BottleneckEncoder(BaseKerasEstimator):\n    \"\"\"A bottleneck autoencoder that can learn latent representations and predict targets.\n\n    This estimator implements a bottleneck autoencoder architecture that:\n    1. Encodes input features to a lower-dimensional latent space\n    2. Decodes the latent representation back to reconstruct the input\n    3. Uses an additional MLP branch to predict targets from the decoded features\n\n    The model can be used both as a regressor (via predict) and as a transformer\n    (via transform) to get latent space representations for dimensionality reduction.\n\n    Args:\n        gaussian_noise (float, default=0.035): Standard deviation of Gaussian noise\n            applied to inputs for regularization.\n        encoder_units (list[tuple[int, float]], default=[(1024, 0.1)]): List of\n            (units, dropout_rate) tuples defining the encoder architecture.\n        latent_units (tuple[int, float], default=(256, 0.1)): Tuple of\n            (units, dropout_rate) for the latent bottleneck layer.\n        ae_units (list[tuple[int, float]], default=[(96, 0.4)]): List of\n            (units, dropout_rate) tuples for the autoencoder prediction branch.\n        activation (str, default=\"swish\"): Activation function used throughout the network.\n        reconstruction_loss_weight (float, default=1.0): Weight for the reconstruction loss.\n        target_loss_weight (float, default=1.0): Weight for the target prediction loss.\n\n    Attributes:\n        encoder (keras.Model): The encoder submodel for transforming inputs to latent space.\n    \"\"\"\n\n    gaussian_noise: float = 0.035\n    encoder_units: list[tuple[int, float]] = field(\n        default_factory=lambda: [(1024, 0.1)]\n    )\n    latent_units: tuple[int, float] = (256, 0.1)\n    ae_units: list[tuple[int, float]] = field(default_factory=lambda: [(96, 0.4)])\n    activation: str = \"swish\"\n    reconstruction_loss_weight: float = 1.0\n    target_loss_weight: float = 1.0\n    encoder: Any = None\n\n    def build_model(self):\n        \"\"\"Construct the bottleneck autoencoder architecture.\"\"\"\n        if self._n_features_in_ is None:\n            raise ValueError(\"Must call fit() before building the model\")\n\n        # Input layer\n        inputs = layers.Input(shape=(self._n_features_in_,), name=\"features\")\n        x0 = layers.BatchNormalization()(inputs)\n\n        # Encoder path\n        encoder = layers.GaussianNoise(self.gaussian_noise)(x0)\n        for units, dropout in self.encoder_units:\n            encoder = layers.Dense(units)(encoder)\n            encoder = layers.BatchNormalization()(encoder)\n            encoder = layers.Activation(self.activation)(encoder)\n            encoder = layers.Dropout(dropout)(encoder)\n\n        # Latent bottleneck layer\n        latent_units, latent_dropout = self.latent_units\n        latent = layers.Dense(latent_units)(encoder)\n        latent = layers.BatchNormalization()(latent)\n        latent = layers.Activation(self.activation)(latent)\n        latent_output = layers.Dropout(latent_dropout)(latent)\n\n        # Create separate encoder model for transform method\n        self.encoder = models.Model(\n            inputs=inputs, outputs=latent_output, name=\"encoder\"\n        )\n\n        # Decoder path (reverse of encoder)\n        decoder = latent_output\n        for units, dropout in reversed(self.encoder_units):\n            decoder = layers.Dense(units)(decoder)\n            decoder = layers.BatchNormalization()(decoder)\n            decoder = layers.Activation(self.activation)(decoder)\n            decoder = layers.Dropout(dropout)(decoder)\n\n        # Reconstruction output\n        reconstruction = layers.Dense(self._n_features_in_, name=\"reconstruction\")(\n            decoder\n        )\n\n        # Target prediction branch from decoded features\n        target_pred = reconstruction\n        for units, dropout in self.ae_units:\n            target_pred = layers.Dense(units)(target_pred)\n            target_pred = layers.BatchNormalization()(target_pred)\n            target_pred = layers.Activation(self.activation)(target_pred)\n            target_pred = layers.Dropout(dropout)(target_pred)\n\n        target_output = layers.Dense(\n            self.output_units, activation=\"linear\", name=\"target_prediction\"\n        )(target_pred)\n\n        # Create the full model with multiple outputs\n        self.model = models.Model(\n            inputs=inputs,\n            outputs=[reconstruction, target_output],\n            name=\"bottleneck_encoder\",\n        )\n\n        # Compile with multiple losses\n        self.model.compile(\n            optimizer=self.optimizer(learning_rate=self.learning_rate),\n            loss={\"reconstruction\": \"mse\", \"target_prediction\": self.loss_function},\n            loss_weights={\n                \"reconstruction\": self.reconstruction_loss_weight,\n                \"target_prediction\": self.target_loss_weight,\n            },\n            metrics={\"target_prediction\": self.metrics or [\"mse\"]},\n        )\n\n        return self\n\n    def fit(\n        self,\n        X,\n        y,\n        epochs: int = 100,\n        batch_size: int = 32,\n        validation_data: tuple[Any, Any] | None = None,\n        callbacks: list[Any] | None = None,\n        **kwargs: Any,\n    ) -&gt; \"BottleneckEncoder\":\n        \"\"\"Fit the bottleneck autoencoder.\n\n        Args:\n            X (array-like): Training data (features).\n            y (array-like): Training targets.\n            epochs (int, default=100): Number of training epochs.\n            batch_size (int, default=32): Minibatch size.\n            validation_data (tuple[Any, Any] | None, default=None): Optional\n                validation split.\n            callbacks (list[Any] | None, default=None): Optional callbacks.\n            **kwargs: Additional arguments passed to keras.Model.fit.\n\n        Returns:\n            BottleneckEncoder: Fitted estimator.\n        \"\"\"\n        # Store input dimension and build model\n        self._n_features_in_ = X.shape[1]\n\n        if self.distribution_strategy:\n            self._setup_distribution_strategy()\n\n        if not self.model:\n            self.build_model()\n\n        # Convert inputs to numpy arrays\n        X_np = _ensure_numpy(X)\n        y_np = _ensure_numpy(y, allow_series=True)\n\n        # Create target dictionary for multiple outputs\n        y_dict = {\"reconstruction\": X_np, \"target_prediction\": y_np}\n\n        # Handle validation data\n        if validation_data is not None:\n            X_val, y_val = validation_data\n            X_val_np = _ensure_numpy(X_val)\n            y_val_np = _ensure_numpy(y_val, allow_series=True)\n            validation_data = (\n                X_val_np,\n                {\"reconstruction\": X_val_np, \"target_prediction\": y_val_np},\n            )\n\n        # Train the model\n        self.model.fit(\n            X_np,\n            y_dict,\n            batch_size=batch_size,\n            epochs=epochs,\n            validation_data=validation_data,\n            callbacks=callbacks,\n            **kwargs,\n        )\n\n        self._is_fitted = True\n        return self\n\n    def predict(self, X, batch_size: int = 512, **kwargs: Any) -&gt; Any:\n        \"\"\"Generate target predictions using the fitted model.\n\n        Args:\n            X (array-like): Input samples.\n            batch_size (int, default=512): Batch size for prediction.\n            **kwargs: Additional arguments passed to keras.Model.predict.\n\n        Returns:\n            array-like: Target predictions.\n        \"\"\"\n        if not self.model:\n            raise ValueError(\"Model not built. Call 'fit' first.\")\n\n        X_np = _ensure_numpy(X)\n        predictions = self.model.predict(X_np, batch_size=batch_size, **kwargs)\n\n        # Return only the target predictions (second output)\n        return predictions[1] if isinstance(predictions, list) else predictions\n\n    def transform(self, X, batch_size: int = 512, **kwargs: Any) -&gt; Any:\n        \"\"\"Transform input data to latent space representation.\n\n        Args:\n            X (array-like): Input samples.\n            batch_size (int, default=512): Batch size for transformation.\n            **kwargs: Additional arguments passed to keras.Model.predict.\n\n        Returns:\n            array-like: Latent space representations.\n        \"\"\"\n        if not self.encoder:\n            raise ValueError(\"Encoder not built. Call 'fit' first.\")\n\n        X_np = _ensure_numpy(X)\n        return self.encoder.predict(X_np, batch_size=batch_size, **kwargs)\n\n    def fit_transform(self, X, y, **kwargs) -&gt; Any:\n        \"\"\"Fit the model and return latent space representations.\n\n        Args:\n            X (array-like): Training data.\n            y (array-like): Training targets.\n            **kwargs: Additional arguments passed to fit.\n\n        Returns:\n            array-like: Latent space representations of X.\n        \"\"\"\n        return self.fit(X, y, **kwargs).transform(X)\n\n    def get_feature_names_out(self, input_features=None) -&gt; list[str]:\n        \"\"\"Generate feature names for the latent space output.\n\n        Args:\n            input_features (array-like, optional): Ignored. Present for API compatibility.\n\n        Returns:\n            list[str]: Feature names for latent dimensions.\n        \"\"\"\n        latent_dim = self.latent_units[0]\n        return [f\"latent_{i}\" for i in range(latent_dim)]\n</code></pre>"},{"location":"api-reference/model_estimators/#centimators.model_estimators.BottleneckEncoder.build_model","title":"<code>build_model()</code>","text":"<p>Construct the bottleneck autoencoder architecture.</p> Source code in <code>src/centimators/model_estimators.py</code> <pre><code>def build_model(self):\n    \"\"\"Construct the bottleneck autoencoder architecture.\"\"\"\n    if self._n_features_in_ is None:\n        raise ValueError(\"Must call fit() before building the model\")\n\n    # Input layer\n    inputs = layers.Input(shape=(self._n_features_in_,), name=\"features\")\n    x0 = layers.BatchNormalization()(inputs)\n\n    # Encoder path\n    encoder = layers.GaussianNoise(self.gaussian_noise)(x0)\n    for units, dropout in self.encoder_units:\n        encoder = layers.Dense(units)(encoder)\n        encoder = layers.BatchNormalization()(encoder)\n        encoder = layers.Activation(self.activation)(encoder)\n        encoder = layers.Dropout(dropout)(encoder)\n\n    # Latent bottleneck layer\n    latent_units, latent_dropout = self.latent_units\n    latent = layers.Dense(latent_units)(encoder)\n    latent = layers.BatchNormalization()(latent)\n    latent = layers.Activation(self.activation)(latent)\n    latent_output = layers.Dropout(latent_dropout)(latent)\n\n    # Create separate encoder model for transform method\n    self.encoder = models.Model(\n        inputs=inputs, outputs=latent_output, name=\"encoder\"\n    )\n\n    # Decoder path (reverse of encoder)\n    decoder = latent_output\n    for units, dropout in reversed(self.encoder_units):\n        decoder = layers.Dense(units)(decoder)\n        decoder = layers.BatchNormalization()(decoder)\n        decoder = layers.Activation(self.activation)(decoder)\n        decoder = layers.Dropout(dropout)(decoder)\n\n    # Reconstruction output\n    reconstruction = layers.Dense(self._n_features_in_, name=\"reconstruction\")(\n        decoder\n    )\n\n    # Target prediction branch from decoded features\n    target_pred = reconstruction\n    for units, dropout in self.ae_units:\n        target_pred = layers.Dense(units)(target_pred)\n        target_pred = layers.BatchNormalization()(target_pred)\n        target_pred = layers.Activation(self.activation)(target_pred)\n        target_pred = layers.Dropout(dropout)(target_pred)\n\n    target_output = layers.Dense(\n        self.output_units, activation=\"linear\", name=\"target_prediction\"\n    )(target_pred)\n\n    # Create the full model with multiple outputs\n    self.model = models.Model(\n        inputs=inputs,\n        outputs=[reconstruction, target_output],\n        name=\"bottleneck_encoder\",\n    )\n\n    # Compile with multiple losses\n    self.model.compile(\n        optimizer=self.optimizer(learning_rate=self.learning_rate),\n        loss={\"reconstruction\": \"mse\", \"target_prediction\": self.loss_function},\n        loss_weights={\n            \"reconstruction\": self.reconstruction_loss_weight,\n            \"target_prediction\": self.target_loss_weight,\n        },\n        metrics={\"target_prediction\": self.metrics or [\"mse\"]},\n    )\n\n    return self\n</code></pre>"},{"location":"api-reference/model_estimators/#centimators.model_estimators.BottleneckEncoder.fit","title":"<code>fit(X, y, epochs=100, batch_size=32, validation_data=None, callbacks=None, **kwargs)</code>","text":"<p>Fit the bottleneck autoencoder.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array - like</code> <p>Training data (features).</p> required <code>y</code> <code>array - like</code> <p>Training targets.</p> required <code>epochs</code> <code>int, default=100</code> <p>Number of training epochs.</p> <code>100</code> <code>batch_size</code> <code>int, default=32</code> <p>Minibatch size.</p> <code>32</code> <code>validation_data</code> <code>tuple[Any, Any] | None, default=None</code> <p>Optional validation split.</p> <code>None</code> <code>callbacks</code> <code>list[Any] | None, default=None</code> <p>Optional callbacks.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional arguments passed to keras.Model.fit.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>BottleneckEncoder</code> <code>BottleneckEncoder</code> <p>Fitted estimator.</p> Source code in <code>src/centimators/model_estimators.py</code> <pre><code>def fit(\n    self,\n    X,\n    y,\n    epochs: int = 100,\n    batch_size: int = 32,\n    validation_data: tuple[Any, Any] | None = None,\n    callbacks: list[Any] | None = None,\n    **kwargs: Any,\n) -&gt; \"BottleneckEncoder\":\n    \"\"\"Fit the bottleneck autoencoder.\n\n    Args:\n        X (array-like): Training data (features).\n        y (array-like): Training targets.\n        epochs (int, default=100): Number of training epochs.\n        batch_size (int, default=32): Minibatch size.\n        validation_data (tuple[Any, Any] | None, default=None): Optional\n            validation split.\n        callbacks (list[Any] | None, default=None): Optional callbacks.\n        **kwargs: Additional arguments passed to keras.Model.fit.\n\n    Returns:\n        BottleneckEncoder: Fitted estimator.\n    \"\"\"\n    # Store input dimension and build model\n    self._n_features_in_ = X.shape[1]\n\n    if self.distribution_strategy:\n        self._setup_distribution_strategy()\n\n    if not self.model:\n        self.build_model()\n\n    # Convert inputs to numpy arrays\n    X_np = _ensure_numpy(X)\n    y_np = _ensure_numpy(y, allow_series=True)\n\n    # Create target dictionary for multiple outputs\n    y_dict = {\"reconstruction\": X_np, \"target_prediction\": y_np}\n\n    # Handle validation data\n    if validation_data is not None:\n        X_val, y_val = validation_data\n        X_val_np = _ensure_numpy(X_val)\n        y_val_np = _ensure_numpy(y_val, allow_series=True)\n        validation_data = (\n            X_val_np,\n            {\"reconstruction\": X_val_np, \"target_prediction\": y_val_np},\n        )\n\n    # Train the model\n    self.model.fit(\n        X_np,\n        y_dict,\n        batch_size=batch_size,\n        epochs=epochs,\n        validation_data=validation_data,\n        callbacks=callbacks,\n        **kwargs,\n    )\n\n    self._is_fitted = True\n    return self\n</code></pre>"},{"location":"api-reference/model_estimators/#centimators.model_estimators.BottleneckEncoder.predict","title":"<code>predict(X, batch_size=512, **kwargs)</code>","text":"<p>Generate target predictions using the fitted model.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array - like</code> <p>Input samples.</p> required <code>batch_size</code> <code>int, default=512</code> <p>Batch size for prediction.</p> <code>512</code> <code>**kwargs</code> <code>Any</code> <p>Additional arguments passed to keras.Model.predict.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Any</code> <p>array-like: Target predictions.</p> Source code in <code>src/centimators/model_estimators.py</code> <pre><code>def predict(self, X, batch_size: int = 512, **kwargs: Any) -&gt; Any:\n    \"\"\"Generate target predictions using the fitted model.\n\n    Args:\n        X (array-like): Input samples.\n        batch_size (int, default=512): Batch size for prediction.\n        **kwargs: Additional arguments passed to keras.Model.predict.\n\n    Returns:\n        array-like: Target predictions.\n    \"\"\"\n    if not self.model:\n        raise ValueError(\"Model not built. Call 'fit' first.\")\n\n    X_np = _ensure_numpy(X)\n    predictions = self.model.predict(X_np, batch_size=batch_size, **kwargs)\n\n    # Return only the target predictions (second output)\n    return predictions[1] if isinstance(predictions, list) else predictions\n</code></pre>"},{"location":"api-reference/model_estimators/#centimators.model_estimators.BottleneckEncoder.transform","title":"<code>transform(X, batch_size=512, **kwargs)</code>","text":"<p>Transform input data to latent space representation.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array - like</code> <p>Input samples.</p> required <code>batch_size</code> <code>int, default=512</code> <p>Batch size for transformation.</p> <code>512</code> <code>**kwargs</code> <code>Any</code> <p>Additional arguments passed to keras.Model.predict.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Any</code> <p>array-like: Latent space representations.</p> Source code in <code>src/centimators/model_estimators.py</code> <pre><code>def transform(self, X, batch_size: int = 512, **kwargs: Any) -&gt; Any:\n    \"\"\"Transform input data to latent space representation.\n\n    Args:\n        X (array-like): Input samples.\n        batch_size (int, default=512): Batch size for transformation.\n        **kwargs: Additional arguments passed to keras.Model.predict.\n\n    Returns:\n        array-like: Latent space representations.\n    \"\"\"\n    if not self.encoder:\n        raise ValueError(\"Encoder not built. Call 'fit' first.\")\n\n    X_np = _ensure_numpy(X)\n    return self.encoder.predict(X_np, batch_size=batch_size, **kwargs)\n</code></pre>"},{"location":"api-reference/model_estimators/#centimators.model_estimators.BottleneckEncoder.fit_transform","title":"<code>fit_transform(X, y, **kwargs)</code>","text":"<p>Fit the model and return latent space representations.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array - like</code> <p>Training data.</p> required <code>y</code> <code>array - like</code> <p>Training targets.</p> required <code>**kwargs</code> <p>Additional arguments passed to fit.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Any</code> <p>array-like: Latent space representations of X.</p> Source code in <code>src/centimators/model_estimators.py</code> <pre><code>def fit_transform(self, X, y, **kwargs) -&gt; Any:\n    \"\"\"Fit the model and return latent space representations.\n\n    Args:\n        X (array-like): Training data.\n        y (array-like): Training targets.\n        **kwargs: Additional arguments passed to fit.\n\n    Returns:\n        array-like: Latent space representations of X.\n    \"\"\"\n    return self.fit(X, y, **kwargs).transform(X)\n</code></pre>"},{"location":"api-reference/model_estimators/#centimators.model_estimators.BottleneckEncoder.get_feature_names_out","title":"<code>get_feature_names_out(input_features=None)</code>","text":"<p>Generate feature names for the latent space output.</p> <p>Parameters:</p> Name Type Description Default <code>input_features</code> <code>array - like</code> <p>Ignored. Present for API compatibility.</p> <code>None</code> <p>Returns:</p> Type Description <code>list[str]</code> <p>list[str]: Feature names for latent dimensions.</p> Source code in <code>src/centimators/model_estimators.py</code> <pre><code>def get_feature_names_out(self, input_features=None) -&gt; list[str]:\n    \"\"\"Generate feature names for the latent space output.\n\n    Args:\n        input_features (array-like, optional): Ignored. Present for API compatibility.\n\n    Returns:\n        list[str]: Feature names for latent dimensions.\n    \"\"\"\n    latent_dim = self.latent_units[0]\n    return [f\"latent_{i}\" for i in range(latent_dim)]\n</code></pre>"},{"location":"api-reference/model_estimators/#centimators.model_estimators.LSTMRegressor","title":"<code>LSTMRegressor</code>  <code>dataclass</code>","text":"<p>               Bases: <code>RegressorMixin</code>, <code>SequenceEstimator</code></p> <p>LSTM-based regressor for time series prediction.</p> <p>This estimator uses stacked LSTM layers to model sequential dependencies in time series data. It supports bidirectional processing and various normalization strategies.</p> <p>Parameters:</p> Name Type Description Default <code>lstm_units</code> <code>list[tuple[int, float, float]], default=[(64, 0.01, 0.01)]</code> <p>List of tuples defining LSTM layers. Each tuple contains: - units: Number of LSTM units - dropout_rate: Dropout rate applied to inputs - recurrent_dropout_rate: Dropout rate applied to recurrent connections</p> <code>lambda: [(64, 0.01, 0.01)]()</code> <code>use_batch_norm</code> <code>bool, default=False</code> <p>Whether to apply batch normalization after each LSTM layer.</p> <code>False</code> <code>use_layer_norm</code> <code>bool, default=False</code> <p>Whether to apply layer normalization after each LSTM layer.</p> <code>False</code> <code>bidirectional</code> <code>bool, default=False</code> <p>Whether to use bidirectional LSTM layers.</p> <code>False</code> <code>lag_windows</code> <code>list[int]</code> <p>Inherited from SequenceEstimator.</p> required <code>n_features_per_timestep</code> <code>int</code> <p>Inherited from SequenceEstimator.</p> required <p>Attributes:</p> Name Type Description <code>_n_features_in_</code> <code>int | None</code> <p>Inferred number of features from training data.</p> Source code in <code>src/centimators/model_estimators.py</code> <pre><code>@dataclass(kw_only=True)\nclass LSTMRegressor(RegressorMixin, SequenceEstimator):\n    \"\"\"LSTM-based regressor for time series prediction.\n\n    This estimator uses stacked LSTM layers to model sequential dependencies\n    in time series data. It supports bidirectional processing and various\n    normalization strategies.\n\n    Args:\n        lstm_units (list[tuple[int, float, float]], default=[(64, 0.01, 0.01)]):\n            List of tuples defining LSTM layers. Each tuple contains:\n            - units: Number of LSTM units\n            - dropout_rate: Dropout rate applied to inputs\n            - recurrent_dropout_rate: Dropout rate applied to recurrent connections\n        use_batch_norm (bool, default=False): Whether to apply batch normalization\n            after each LSTM layer.\n        use_layer_norm (bool, default=False): Whether to apply layer normalization\n            after each LSTM layer.\n        bidirectional (bool, default=False): Whether to use bidirectional LSTM layers.\n        lag_windows (list[int]): Inherited from SequenceEstimator.\n        n_features_per_timestep (int): Inherited from SequenceEstimator.\n\n    Attributes:\n        _n_features_in_ (int | None): Inferred number of features from training data.\n    \"\"\"\n\n    lstm_units: list[tuple[int, float, float]] = field(\n        default_factory=lambda: [(64, 0.01, 0.01)]\n    )\n    use_batch_norm: bool = False\n    use_layer_norm: bool = False\n    bidirectional: bool = False\n    metrics: list[str] | None = field(default_factory=lambda: [\"mse\"])\n\n    def build_model(self):\n        \"\"\"Construct the LSTM architecture.\"\"\"\n        if self._n_features_in_ is None:\n            raise ValueError(\"Must call fit() before building the model\")\n\n        # Input layer expecting 3D tensor (batch, timesteps, features)\n        inputs = layers.Input(\n            shape=(self.seq_length, self.n_features_per_timestep), name=\"sequence_input\"\n        )\n\n        x = inputs\n\n        # Stack LSTM layers\n        for layer_num, (units, dropout, recurrent_dropout) in enumerate(\n            self.lstm_units\n        ):\n            return_sequences = layer_num &lt; len(self.lstm_units) - 1\n\n            lstm_layer = layers.LSTM(\n                units=units,\n                activation=\"tanh\",\n                return_sequences=return_sequences,\n                dropout=dropout,\n                recurrent_dropout=recurrent_dropout,\n                name=f\"lstm_{layer_num}\",\n            )\n\n            # Apply bidirectional wrapper if requested\n            if self.bidirectional:\n                x = layers.Bidirectional(lstm_layer, name=f\"bidirectional_{layer_num}\")(\n                    x\n                )\n            else:\n                x = lstm_layer(x)\n\n            # Apply normalization layers if requested\n            if self.use_layer_norm:\n                x = layers.LayerNormalization(name=f\"layer_norm_{layer_num}\")(x)\n            if self.use_batch_norm:\n                x = layers.BatchNormalization(name=f\"batch_norm_{layer_num}\")(x)\n\n        # Output layer\n        outputs = layers.Dense(self.output_units, activation=\"linear\", name=\"output\")(x)\n\n        # Create and compile model\n        self.model = models.Model(inputs=inputs, outputs=outputs, name=\"lstm_regressor\")\n\n        self.model.compile(\n            optimizer=self.optimizer(learning_rate=self.learning_rate),\n            loss=self.loss_function,\n            metrics=self.metrics,\n        )\n\n        return self\n</code></pre>"},{"location":"api-reference/model_estimators/#centimators.model_estimators.LSTMRegressor.build_model","title":"<code>build_model()</code>","text":"<p>Construct the LSTM architecture.</p> Source code in <code>src/centimators/model_estimators.py</code> <pre><code>def build_model(self):\n    \"\"\"Construct the LSTM architecture.\"\"\"\n    if self._n_features_in_ is None:\n        raise ValueError(\"Must call fit() before building the model\")\n\n    # Input layer expecting 3D tensor (batch, timesteps, features)\n    inputs = layers.Input(\n        shape=(self.seq_length, self.n_features_per_timestep), name=\"sequence_input\"\n    )\n\n    x = inputs\n\n    # Stack LSTM layers\n    for layer_num, (units, dropout, recurrent_dropout) in enumerate(\n        self.lstm_units\n    ):\n        return_sequences = layer_num &lt; len(self.lstm_units) - 1\n\n        lstm_layer = layers.LSTM(\n            units=units,\n            activation=\"tanh\",\n            return_sequences=return_sequences,\n            dropout=dropout,\n            recurrent_dropout=recurrent_dropout,\n            name=f\"lstm_{layer_num}\",\n        )\n\n        # Apply bidirectional wrapper if requested\n        if self.bidirectional:\n            x = layers.Bidirectional(lstm_layer, name=f\"bidirectional_{layer_num}\")(\n                x\n            )\n        else:\n            x = lstm_layer(x)\n\n        # Apply normalization layers if requested\n        if self.use_layer_norm:\n            x = layers.LayerNormalization(name=f\"layer_norm_{layer_num}\")(x)\n        if self.use_batch_norm:\n            x = layers.BatchNormalization(name=f\"batch_norm_{layer_num}\")(x)\n\n    # Output layer\n    outputs = layers.Dense(self.output_units, activation=\"linear\", name=\"output\")(x)\n\n    # Create and compile model\n    self.model = models.Model(inputs=inputs, outputs=outputs, name=\"lstm_regressor\")\n\n    self.model.compile(\n        optimizer=self.optimizer(learning_rate=self.learning_rate),\n        loss=self.loss_function,\n        metrics=self.metrics,\n    )\n\n    return self\n</code></pre>"},{"location":"tutorials/keras-cortex/","title":"Keras Cortex Tutorial","text":"In\u00a0[1]: Copied! <pre>%load_ext dotenv\n%dotenv # ensure OPENAI_API_KEY is set in .env\n</pre> %load_ext dotenv %dotenv # ensure OPENAI_API_KEY is set in .env In\u00a0[2]: Copied! <pre>import os\n\nos.environ[\"KERAS_BACKEND\"] = \"jax\"\nos.environ[\"OPENAI_API_KEY\"] = \"sk-proj-...\"\n\nimport polars as pl\nfrom sklearn.datasets import make_regression\nfrom sklearn.model_selection import train_test_split\n\nfrom centimators.model_estimators import MLPRegressor\nfrom centimators.keras_cortex import KerasCortex\n</pre> import os  os.environ[\"KERAS_BACKEND\"] = \"jax\" os.environ[\"OPENAI_API_KEY\"] = \"sk-proj-...\"  import polars as pl from sklearn.datasets import make_regression from sklearn.model_selection import train_test_split  from centimators.model_estimators import MLPRegressor from centimators.keras_cortex import KerasCortex In\u00a0[3]: Copied! <pre>X, y = make_regression(\n    n_samples=10000,\n    n_features=100,\n    noise=0.1,\n    random_state=42,\n)\n\nX = pl.DataFrame(X)\ny = pl.Series(y)\n\n# train / val / test split  (60 / 20 / 20)\nX_train, X_tmp, y_train, y_tmp = train_test_split(X, y, test_size=0.4, random_state=42)\nX_val, X_test, y_val, y_test = train_test_split(\n    X_tmp, y_tmp, test_size=0.5, random_state=42\n)\n\nprint(X_train.shape, X_val.shape, X_test.shape)\n</pre> X, y = make_regression(     n_samples=10000,     n_features=100,     noise=0.1,     random_state=42, )  X = pl.DataFrame(X) y = pl.Series(y)  # train / val / test split  (60 / 20 / 20) X_train, X_tmp, y_train, y_tmp = train_test_split(X, y, test_size=0.4, random_state=42) X_val, X_test, y_val, y_test = train_test_split(     X_tmp, y_tmp, test_size=0.5, random_state=42 )  print(X_train.shape, X_val.shape, X_test.shape) <pre>(6000, 100) (2000, 100) (2000, 100)\n</pre> In\u00a0[4]: Copied! <pre>base_mlp = MLPRegressor(\n    hidden_units=(64, 32),\n    dropout_rate=0.1,\n)\n\ncortex = KerasCortex(\n    base_estimator=base_mlp, n_iterations=6, lm=\"openai/gpt-4o-mini\", verbose=True\n)\n</pre> base_mlp = MLPRegressor(     hidden_units=(64, 32),     dropout_rate=0.1, )  cortex = KerasCortex(     base_estimator=base_mlp, n_iterations=6, lm=\"openai/gpt-4o-mini\", verbose=True ) In\u00a0[5]: Copied! <pre>cortex.fit(\n    X_train,\n    y_train,\n    validation_data=(X_val, y_val),\n    epochs=10,\n    batch_size=516,\n)\n</pre> cortex.fit(     X_train,     y_train,     validation_data=(X_val, y_val),     epochs=10,     batch_size=516, ) <pre>WARNING:2025-05-23 02:23:28,521:jax._src.xla_bridge:909: An NVIDIA GPU may be present on this machine, but a CUDA-enabled jaxlib is not installed. Falling back to cpu.\n</pre> <pre>Epoch 1/10\n12/12 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 1s 64ms/step - loss: 38910.3633 - mse: 38910.3633\nEpoch 2/10\n12/12 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - loss: 37584.1914 - mse: 37584.1914  \nEpoch 3/10\n12/12 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - loss: 38222.5898 - mse: 38222.5898\nEpoch 4/10\n12/12 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 4ms/step - loss: 38282.4961 - mse: 38282.4961\nEpoch 5/10\n12/12 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 4ms/step - loss: 37073.2383 - mse: 37073.2383\nEpoch 6/10\n12/12 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 4ms/step - loss: 36388.0781 - mse: 36388.0781\nEpoch 7/10\n12/12 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 4ms/step - loss: 36612.1328 - mse: 36612.1328\nEpoch 8/10\n12/12 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 4ms/step - loss: 34292.5352 - mse: 34292.5352\nEpoch 9/10\n12/12 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 4ms/step - loss: 33463.0625 - mse: 33463.0625\nEpoch 10/10\n12/12 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 4ms/step - loss: 31842.3281 - mse: 31842.3281\n4/4 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 26ms/step\n\n--- Iteration 1 ---\nReasoning: \nTo improve the validation metrics, particularly the R2 score, we can consider several modifications to the model architecture and training process. First, we can experiment with adding more layers or increasing the number of units in the existing layers to allow the model to learn more complex patterns. Additionally, we can adjust the activation function to a non-linear one like 'relu' or 'swish' which often performs better in practice. Furthermore, we can implement batch normalization after each dense layer to stabilize and accelerate training. Lastly, we can consider using a different optimizer like 'Adam' with a learning rate schedule to improve convergence.\nSuggested code: \ndef build_model(self):\n    \"\"\"Construct a simple MLP with the configured hyper-parameters.\"\"\"\n    inputs = layers.Input(shape=(self._n_features_in_,), name=\"features\")\n    x = inputs\n    for units in self.hidden_units:\n        x = layers.Dense(units, activation='relu')(x)\n        if self.dropout_rate &gt; 0:\n            x = layers.Dropout(self.dropout_rate)(x)\n        x = layers.BatchNormalization()(x)  # Adding batch normalization\n    outputs = layers.Dense(self.output_units, activation=\"linear\")(x)\n    self.model = models.Model(inputs=inputs, outputs=outputs, name=\"mlp_regressor\")\n\n    self.model.compile(\n        optimizer='adam',  # Using Adam optimizer\n        loss=self.loss_function,\n        metrics=self.metrics,\n    )\n\n    return self\nEpoch 1/10\n12/12 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 1s 78ms/step - loss: 37700.0586 - mse: 37700.0586\nEpoch 2/10\n12/12 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - loss: 37608.2930 - mse: 37608.2930  \nEpoch 3/10\n12/12 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 4ms/step - loss: 37138.8477 - mse: 37138.8477\nEpoch 4/10\n12/12 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 4ms/step - loss: 37910.6211 - mse: 37910.6211\nEpoch 5/10\n12/12 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 5ms/step - loss: 37415.6562 - mse: 37415.6562\nEpoch 6/10\n12/12 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 5ms/step - loss: 36536.8359 - mse: 36536.8359\nEpoch 7/10\n12/12 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 5ms/step - loss: 35623.8125 - mse: 35623.8125\nEpoch 8/10\n12/12 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 5ms/step - loss: 35256.6914 - mse: 35256.6914\nEpoch 9/10\n12/12 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 5ms/step - loss: 34580.7734 - mse: 34580.7734\nEpoch 10/10\n12/12 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 5ms/step - loss: 34898.6445 - mse: 34898.6445\n4/4 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 31ms/step\nNo improvement (0.1016 &lt;= 0.1919), keeping best code.\n\n--- Iteration 2 ---\nReasoning: \nTo improve the validation metrics, particularly the R2 score, we can consider several modifications to the model architecture and training process. The current model uses a simple MLP structure with ReLU activations and dropout, which may not be sufficient for capturing complex patterns in the data. \n\n1. **Activation Function**: Experimenting with different activation functions, such as Leaky ReLU or ELU, can help mitigate issues with dying neurons and improve learning.\n2. **Layer Configuration**: Adding more layers or increasing the number of units in existing layers can enhance the model's capacity to learn complex relationships.\n3. **Regularization**: Adjusting dropout rates or adding L2 regularization can help prevent overfitting, which is crucial for improving validation metrics.\n4. **Learning Rate**: Fine-tuning the learning rate of the optimizer can lead to better convergence during training.\n\nGiven these considerations, I suggest modifying the activation function to Leaky ReLU, increasing the number of units in the hidden layers, and adjusting the dropout rate if necessary.\nSuggested code: \ndef build_model(self):\n    \"\"\"Construct a simple MLP with the configured hyper-parameters.\"\"\"\n    inputs = layers.Input(shape=(self._n_features_in_,), name=\"features\")\n    x = inputs\n    for units in self.hidden_units:\n        x = layers.Dense(units, activation='leaky_relu')(x)  # Changed to Leaky ReLU\n        if self.dropout_rate &gt; 0:\n            x = layers.Dropout(self.dropout_rate)(x)\n        x = layers.BatchNormalization()(x)  # Adding batch normalization\n    outputs = layers.Dense(self.output_units, activation=\"linear\")(x)\n    self.model = models.Model(inputs=inputs, outputs=outputs, name=\"mlp_regressor\")\n\n    self.model.compile(\n        optimizer='adam',  # Using Adam optimizer\n        loss=self.loss_function,\n        metrics=self.metrics,\n    )\n\n    return self\nEpoch 1/10\n12/12 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 1s 80ms/step - loss: 38861.8711 - mse: 38861.8711\nEpoch 2/10\n12/12 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 4ms/step - loss: 37264.9961 - mse: 37264.9961  \nEpoch 3/10\n12/12 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 4ms/step - loss: 37546.3281 - mse: 37546.3281\nEpoch 4/10\n12/12 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 4ms/step - loss: 37600.1172 - mse: 37600.1172\nEpoch 5/10\n12/12 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 5ms/step - loss: 36725.0469 - mse: 36725.0469\nEpoch 6/10\n12/12 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 5ms/step - loss: 36916.5859 - mse: 36916.5859\nEpoch 7/10\n12/12 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 5ms/step - loss: 35638.0977 - mse: 35638.0977\nEpoch 8/10\n12/12 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 5ms/step - loss: 35082.1836 - mse: 35082.1836\nEpoch 9/10\n12/12 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 5ms/step - loss: 34401.2461 - mse: 34401.2461\nEpoch 10/10\n12/12 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 5ms/step - loss: 33381.0312 - mse: 33381.0312\n4/4 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 25ms/step\nNo improvement (0.1285 &lt;= 0.1919), keeping best code.\n\n--- Iteration 3 ---\nReasoning: \nTo improve the validation metrics (R2), we can consider several modifications to the model architecture and training process. The current model uses Leaky ReLU activation, which is a good choice, but we can experiment with adding more complexity to the model by increasing the number of layers or units. Additionally, we can adjust the dropout rate to prevent overfitting, especially if the model is too complex for the dataset. We can also consider using a different optimizer or adjusting the learning rate for better convergence. Finally, we can add more batch normalization layers to stabilize the learning process.\nSuggested code: \ndef build_model(self):\n    \"\"\"Construct a simple MLP with the configured hyper-parameters.\"\"\"\n    inputs = layers.Input(shape=(self._n_features_in_,), name=\"features\")\n    x = inputs\n    for units in self.hidden_units:\n        x = layers.Dense(units, activation='leaky_relu')(x)  # Keeping Leaky ReLU\n        if self.dropout_rate &gt; 0:\n            x = layers.Dropout(self.dropout_rate)(x)\n        x = layers.BatchNormalization()(x)  # Adding batch normalization\n    # Adding an additional Dense layer for more complexity\n    x = layers.Dense(64, activation='leaky_relu')(x)  # Additional layer\n    if self.dropout_rate &gt; 0:\n        x = layers.Dropout(self.dropout_rate)(x)\n    outputs = layers.Dense(self.output_units, activation=\"linear\")(x)\n    self.model = models.Model(inputs=inputs, outputs=outputs, name=\"mlp_regressor\")\n\n    self.model.compile(\n        optimizer='adam',  # Using Adam optimizer\n        loss=self.loss_function,\n        metrics=self.metrics,\n    )\n\n    return self\nEpoch 1/10\n12/12 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 1s 88ms/step - loss: 38137.1836 - mse: 38137.1836\nEpoch 2/10\n12/12 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 5ms/step - loss: 38227.8203 - mse: 38227.8203  \nEpoch 3/10\n12/12 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 5ms/step - loss: 38262.1445 - mse: 38262.1445\nEpoch 4/10\n12/12 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 6ms/step - loss: 37551.5586 - mse: 37551.5586\nEpoch 5/10\n12/12 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 6ms/step - loss: 35559.9570 - mse: 35559.9570\nEpoch 6/10\n12/12 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 7ms/step - loss: 33512.2930 - mse: 33512.2930\nEpoch 7/10\n12/12 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 7ms/step - loss: 30176.9766 - mse: 30176.9766\nEpoch 8/10\n12/12 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 7ms/step - loss: 26692.1738 - mse: 26692.1738\nEpoch 9/10\n12/12 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 6ms/step - loss: 23776.6172 - mse: 23776.6172\nEpoch 10/10\n12/12 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 7ms/step - loss: 20573.6172 - mse: 20573.6172\n4/4 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 30ms/step\nImprovement! New validation score: 0.6164 &gt; 0.1919\n\n--- Iteration 4 ---\nReasoning: \nTo improve the validation metrics (R2), we can consider several modifications to the model architecture and training process. The current model uses Leaky ReLU activations, which is good, but we can experiment with adding more complexity and regularization. The addition of more layers or units can help the model learn better representations. Additionally, we can adjust the learning rate of the Adam optimizer to ensure that the model converges more effectively. Implementing early stopping during training can also help prevent overfitting, which is crucial for improving validation metrics.\nSuggested code: \ndef build_model(self):\n    \"\"\"Construct a simple MLP with the configured hyper-parameters.\"\"\"\n    inputs = layers.Input(shape=(self._n_features_in_,), name=\"features\")\n    x = inputs\n    for units in self.hidden_units:\n        x = layers.Dense(units, activation='leaky_relu')(x)  # Keeping Leaky ReLU\n        if self.dropout_rate &gt; 0:\n            x = layers.Dropout(self.dropout_rate)(x)\n        x = layers.BatchNormalization()(x)  # Adding batch normalization\n    # Adding an additional Dense layer for more complexity\n    x = layers.Dense(128, activation='leaky_relu')(x)  # Increased units for additional layer\n    if self.dropout_rate &gt; 0:\n        x = layers.Dropout(self.dropout_rate)(x)\n    outputs = layers.Dense(self.output_units, activation=\"linear\")(x)\n    self.model = models.Model(inputs=inputs, outputs=outputs, name=\"mlp_regressor\")\n\n    self.model.compile(\n        optimizer='adam',  # Using Adam optimizer\n        loss=self.loss_function,\n        metrics=self.metrics,\n    )\n\n    return self\nEpoch 1/10\n12/12 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 1s 87ms/step - loss: 40391.3164 - mse: 40391.3164\nEpoch 2/10\n12/12 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 5ms/step - loss: 38161.0781 - mse: 38161.0781 \nEpoch 3/10\n12/12 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 6ms/step - loss: 37445.9609 - mse: 37445.9609\nEpoch 4/10\n12/12 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 6ms/step - loss: 36245.1328 - mse: 36245.1328\nEpoch 5/10\n12/12 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 7ms/step - loss: 34253.0195 - mse: 34253.0195\nEpoch 6/10\n12/12 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 7ms/step - loss: 30194.2812 - mse: 30194.2812\nEpoch 7/10\n12/12 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 7ms/step - loss: 26387.5801 - mse: 26387.5801\nEpoch 8/10\n12/12 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 7ms/step - loss: 22368.8633 - mse: 22368.8633\nEpoch 9/10\n12/12 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 7ms/step - loss: 16535.9961 - mse: 16535.9961\nEpoch 10/10\n12/12 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 7ms/step - loss: 12131.1436 - mse: 12131.1436\n4/4 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 29ms/step\nImprovement! New validation score: 0.8433 &gt; 0.6164\n\n--- Iteration 5 ---\nReasoning: \nTo improve validation metrics, particularly R2, we can consider several modifications to the model architecture and training process. The current model has a relatively simple structure with a few dense layers and dropout for regularization. However, the performance log indicates that the model's complexity may not be sufficient to capture the underlying patterns in the data. \n\n1. **Increase Model Complexity**: Adding more layers or increasing the number of units in existing layers can help the model learn more complex representations.\n2. **Adjust Activation Functions**: While Leaky ReLU is a good choice, experimenting with other activation functions like 'relu' or 'swish' might yield better results.\n3. **Learning Rate Adjustment**: Fine-tuning the learning rate of the Adam optimizer can significantly impact convergence and performance.\n4. **Regularization Techniques**: Besides dropout, we could also consider L2 regularization on the dense layers to prevent overfitting.\n\nGiven these considerations, I suggest increasing the number of units in the additional dense layer and experimenting with a different activation function.\nSuggested code: \ndef build_model(self):\n    \"\"\"Construct a simple MLP with the configured hyper-parameters.\"\"\"\n    inputs = layers.Input(shape=(self._n_features_in_,), name=\"features\")\n    x = inputs\n    for units in self.hidden_units:\n        x = layers.Dense(units, activation='relu')(x)  # Changed to ReLU for potential better performance\n        if self.dropout_rate &gt; 0:\n            x = layers.Dropout(self.dropout_rate)(x)\n        x = layers.BatchNormalization()(x)  # Adding batch normalization\n    # Adding an additional Dense layer with increased units for more complexity\n    x = layers.Dense(256, activation='relu')(x)  # Increased units for additional layer\n    if self.dropout_rate &gt; 0:\n        x = layers.Dropout(self.dropout_rate)(x)\n    outputs = layers.Dense(self.output_units, activation=\"linear\")(x)\n    self.model = models.Model(inputs=inputs, outputs=outputs, name=\"mlp_regressor\")\n\n    self.model.compile(\n        optimizer='adam',  # Using Adam optimizer\n        loss=self.loss_function,\n        metrics=self.metrics,\n    )\n\n    return self\nEpoch 1/10\n12/12 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 2s 89ms/step - loss: 38065.1797 - mse: 38065.1797\nEpoch 2/10\n12/12 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 6ms/step - loss: 37779.0117 - mse: 37779.0117 \nEpoch 3/10\n12/12 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 7ms/step - loss: 37103.6758 - mse: 37103.6758\nEpoch 4/10\n12/12 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 7ms/step - loss: 36028.3477 - mse: 36028.3477\nEpoch 5/10\n12/12 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 7ms/step - loss: 33991.7109 - mse: 33991.7109\nEpoch 6/10\n12/12 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 6ms/step - loss: 30745.7578 - mse: 30745.7578\nEpoch 7/10\n12/12 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 7ms/step - loss: 24856.4180 - mse: 24856.4180\nEpoch 8/10\n12/12 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 7ms/step - loss: 18043.0879 - mse: 18043.0879\nEpoch 9/10\n12/12 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 7ms/step - loss: 12032.8086 - mse: 12032.8086\nEpoch 10/10\n12/12 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 7ms/step - loss: 7566.1025 - mse: 7566.1025\n4/4 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 32ms/step\nImprovement! New validation score: 0.8788 &gt; 0.8433\n\n--- Iteration 6 ---\nReasoning: \nTo improve the validation metrics (R2), we can consider several modifications to the model architecture. The current model has a relatively high number of units in the last dense layer (256), which may lead to overfitting, especially if the dataset is not large enough. Reducing the complexity of the model by decreasing the number of units in the last layer can help generalize better. Additionally, we can introduce a more advanced activation function like 'swish' or 'gelu' which may provide better performance than 'leaky_relu' or 'relu'. We can also consider adding more dropout layers or increasing the dropout rate to further prevent overfitting.\nSuggested code: \ndef build_model(self):\n    \"\"\"Construct a simple MLP with the configured hyper-parameters.\"\"\"\n    inputs = layers.Input(shape=(self._n_features_in_,), name=\"features\")\n    x = inputs\n    for units in self.hidden_units:\n        x = layers.Dense(units, activation='swish')(x)  # Changed to Swish activation\n        if self.dropout_rate &gt; 0:\n            x = layers.Dropout(self.dropout_rate)(x)\n        x = layers.BatchNormalization()(x)  # Adding batch normalization\n    # Reducing the number of units in the additional Dense layer\n    x = layers.Dense(128, activation='swish')(x)  # Decreased units for additional layer\n    if self.dropout_rate &gt; 0:\n        x = layers.Dropout(self.dropout_rate)(x)\n    outputs = layers.Dense(self.output_units, activation=\"linear\")(x)\n    self.model = models.Model(inputs=inputs, outputs=outputs, name=\"mlp_regressor\")\n\n    self.model.compile(\n        optimizer='adam',  # Using Adam optimizer\n        loss=self.loss_function,\n        metrics=self.metrics,\n    )\n\n    return self\nEpoch 1/10\n12/12 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 1s 89ms/step - loss: 38398.5625 - mse: 38398.5625\nEpoch 2/10\n12/12 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 6ms/step - loss: 38273.7734 - mse: 38273.7734 \nEpoch 3/10\n12/12 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 6ms/step - loss: 37329.9453 - mse: 37329.9453\nEpoch 4/10\n12/12 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 7ms/step - loss: 37087.1641 - mse: 37087.1641\nEpoch 5/10\n12/12 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 7ms/step - loss: 34895.7227 - mse: 34895.7227\nEpoch 6/10\n12/12 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 7ms/step - loss: 31744.3477 - mse: 31744.3477\nEpoch 7/10\n12/12 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 7ms/step - loss: 28108.6406 - mse: 28108.6406\nEpoch 8/10\n12/12 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 7ms/step - loss: 24307.8398 - mse: 24307.8398\nEpoch 9/10\n12/12 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 7ms/step - loss: 20269.2207 - mse: 20269.2207\nEpoch 10/10\n12/12 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 7ms/step - loss: 16637.2168 - mse: 16637.2168\n4/4 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 36ms/step\nNo improvement (0.6662 &lt;= 0.8788), keeping best code.\n</pre> Out[5]: <pre>KerasCortex(base_estimator=MLPRegressor(output_units=1,\n                                        optimizer=&lt;class 'keras.src.optimizers.adam.Adam'&gt;,\n                                        learning_rate=0.001,\n                                        loss_function='mse',\n                                        metrics=['mse'],\n                                        model=None,\n                                        distribution_strategy=None,\n                                        hidden_units=(64, 32),\n                                        activation='relu',\n                                        dropout_rate=0.1),\n            lm=&lt;dspy.clients.lm.LM object at 0x7f9c4414daf0&gt;, n_iterations=6,\n            verbose=True)</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.KerasCortexiFitted<pre>KerasCortex(base_estimator=MLPRegressor(output_units=1,\n                                        optimizer=&lt;class 'keras.src.optimizers.adam.Adam'&gt;,\n                                        learning_rate=0.001,\n                                        loss_function='mse',\n                                        metrics=['mse'],\n                                        model=None,\n                                        distribution_strategy=None,\n                                        hidden_units=(64, 32),\n                                        activation='relu',\n                                        dropout_rate=0.1),\n            lm=&lt;dspy.clients.lm.LM object at 0x7f9c4414daf0&gt;, n_iterations=6,\n            verbose=True)</pre> base_estimator: MLPRegressor<pre>MLPRegressor(output_units=1, optimizer=&lt;class 'keras.src.optimizers.adam.Adam'&gt;, learning_rate=0.001, loss_function='mse', metrics=['mse'], model=None, distribution_strategy=None, hidden_units=(64, 32), activation='relu', dropout_rate=0.1)</pre> MLPRegressor<pre>MLPRegressor(output_units=1, optimizer=&lt;class 'keras.src.optimizers.adam.Adam'&gt;, learning_rate=0.001, loss_function='mse', metrics=['mse'], model=None, distribution_strategy=None, hidden_units=(64, 32), activation='relu', dropout_rate=0.1)</pre>"},{"location":"tutorials/keras-cortex/#keras-cortex-tutorial","title":"Keras Cortex Tutorial\u00b6","text":"<p>This tutorial demonstrates how to use <code>KerasCortex</code>, a meta-estimator that uses Large Language Models (LLMs) to iteratively improve neural network architectures through self-reflection and iterative improvement.</p>"},{"location":"tutorials/keras-cortex/#overview","title":"Overview\u00b6","text":"<p><code>KerasCortex</code> wraps a base Keras estimator and uses an LLM via DSPy to suggest modifications to the <code>build_model</code> method. It iteratively attempts these suggestions, evaluates their performance on validation data, and keeps the best-performing model architecture. Each step, it reasons about changes it should make to the model before re-fitting it.</p>"},{"location":"tutorials/keras-cortex/#prerequisites","title":"Prerequisites\u00b6","text":"<p>To run the full tutorial, you'll need:</p> <ul> <li>An OpenAI API key (or access to another LLM via DSPy)</li> <li>JAX, TensorFlow, or PyTorch backend for Keras</li> <li>The <code>dspy</code> library for LLM orchestration</li> </ul> <p>!!! Warning</p> <pre><code>This module is a work in progress. It is not yet ready for production use.\nThis is highly experimental and likely to overfit.</code></pre>"},{"location":"tutorials/playground/","title":"Playground","text":"In\u00a0[\u00a0]: Copied! <pre>import marimo\n</pre> import marimo In\u00a0[\u00a0]: Copied! <pre>__generated_with = \"0.13.10\"\napp = marimo.App(width=\"medium\")\n</pre> __generated_with = \"0.13.10\" app = marimo.App(width=\"medium\") In\u00a0[\u00a0]: Copied! <pre>@app.cell(hide_code=True)\ndef _():\n    import marimo as mo\n    import pandas as pd\n    import polars as pl\n    import numpy as np\n    import altair as alt\n    from datetime import datetime, timedelta\n\n    from centimators.feature_transformers import (\n        RankTransformer,\n        LagTransformer,\n        MovingAverageTransformer,\n        LogReturnTransformer,\n        GroupStatsTransformer,\n    )\n\n    return (\n        GroupStatsTransformer,\n        LagTransformer,\n        LogReturnTransformer,\n        MovingAverageTransformer,\n        RankTransformer,\n        alt,\n        datetime,\n        mo,\n        np,\n        pd,\n        pl,\n        timedelta,\n    )\n</pre> @app.cell(hide_code=True) def _():     import marimo as mo     import pandas as pd     import polars as pl     import numpy as np     import altair as alt     from datetime import datetime, timedelta      from centimators.feature_transformers import (         RankTransformer,         LagTransformer,         MovingAverageTransformer,         LogReturnTransformer,         GroupStatsTransformer,     )      return (         GroupStatsTransformer,         LagTransformer,         LogReturnTransformer,         MovingAverageTransformer,         RankTransformer,         alt,         datetime,         mo,         np,         pd,         pl,         timedelta,     ) In\u00a0[\u00a0]: Copied! <pre>@app.cell(hide_code=True)\ndef _(datetime, np, pd, pl, timedelta):\n    dates = [datetime.now() - timedelta(days=i) for i in range(90)]\n    dates.reverse()\n    tickers = [f\"Ticker{i}\" for i in range(1, 21)]\n    data = {\n        \"ticker\": [],\n        \"date\": [],\n        \"open\": [],\n        \"high\": [],\n        \"low\": [],\n        \"close\": [],\n        \"volume\": [],\n    }\n    for _ticker in tickers:\n        base_price = np.random.uniform(10, 1000)\n        for date in dates:\n            daily_return = np.random.normal(0.005, 0.03)\n            close = base_price * (1 + daily_return)\n            high = close * (1 + abs(np.random.normal(0, 0.01)))\n            low = close * (1 - abs(np.random.normal(0, 0.01)))\n            open_price = close * (1 + np.random.normal(-0.005, 0.005))\n            volume = int(np.random.lognormal(10, 1))\n            data[\"ticker\"].append(_ticker)\n            data[\"date\"].append(date)\n            data[\"open\"].append(round(open_price, 2))\n            data[\"high\"].append(round(high, 2))\n            data[\"low\"].append(round(low, 2))\n            data[\"close\"].append(round(close, 2))\n            data[\"volume\"].append(volume)\n            base_price = close\n    df_pandas = pd.DataFrame(data)\n    df_polars = pl.DataFrame(data)\n    return df_pandas, df_polars\n</pre> @app.cell(hide_code=True) def _(datetime, np, pd, pl, timedelta):     dates = [datetime.now() - timedelta(days=i) for i in range(90)]     dates.reverse()     tickers = [f\"Ticker{i}\" for i in range(1, 21)]     data = {         \"ticker\": [],         \"date\": [],         \"open\": [],         \"high\": [],         \"low\": [],         \"close\": [],         \"volume\": [],     }     for _ticker in tickers:         base_price = np.random.uniform(10, 1000)         for date in dates:             daily_return = np.random.normal(0.005, 0.03)             close = base_price * (1 + daily_return)             high = close * (1 + abs(np.random.normal(0, 0.01)))             low = close * (1 - abs(np.random.normal(0, 0.01)))             open_price = close * (1 + np.random.normal(-0.005, 0.005))             volume = int(np.random.lognormal(10, 1))             data[\"ticker\"].append(_ticker)             data[\"date\"].append(date)             data[\"open\"].append(round(open_price, 2))             data[\"high\"].append(round(high, 2))             data[\"low\"].append(round(low, 2))             data[\"close\"].append(round(close, 2))             data[\"volume\"].append(volume)             base_price = close     df_pandas = pd.DataFrame(data)     df_polars = pl.DataFrame(data)     return df_pandas, df_polars In\u00a0[\u00a0]: Copied! <pre>@app.cell(hide_code=True)\ndef _(df_polars):\n    df_polars.plot.line(x=\"date\", y=\"close\", color=\"ticker\").properties(\n        width=600, height=400, title=\"Mock Price Data Over Time Grouped by Tickers\"\n    )\n    return\n</pre> @app.cell(hide_code=True) def _(df_polars):     df_polars.plot.line(x=\"date\", y=\"close\", color=\"ticker\").properties(         width=600, height=400, title=\"Mock Price Data Over Time Grouped by Tickers\"     )     return In\u00a0[\u00a0]: Copied! <pre>@app.cell(hide_code=True)\ndef _(mo):\n    mo.md(r\"\"\"## Create transformers\"\"\")\n    return\n</pre> @app.cell(hide_code=True) def _(mo):     mo.md(r\"\"\"## Create transformers\"\"\")     return In\u00a0[\u00a0]: Copied! <pre>@app.cell\ndef _(\n    LagTransformer,\n    LogReturnTransformer,\n    MovingAverageTransformer,\n    RankTransformer,\n):\n    ranker: RankTransformer = RankTransformer()\n    ranker\n\n    lag_windows = [0, 5, 10, 15]\n    _lagger: LagTransformer = LagTransformer(windows=lag_windows)\n\n    ma_windows = [5, 10, 20, 40]\n    _ma_transformer = MovingAverageTransformer(windows=ma_windows)\n    _log_return_transformer = LogReturnTransformer()\n\n    _log_return_transformer, ranker, _lagger, _ma_transformer\n    return lag_windows, ma_windows, ranker\n</pre> @app.cell def _(     LagTransformer,     LogReturnTransformer,     MovingAverageTransformer,     RankTransformer, ):     ranker: RankTransformer = RankTransformer()     ranker      lag_windows = [0, 5, 10, 15]     _lagger: LagTransformer = LagTransformer(windows=lag_windows)      ma_windows = [5, 10, 20, 40]     _ma_transformer = MovingAverageTransformer(windows=ma_windows)     _log_return_transformer = LogReturnTransformer()      _log_return_transformer, ranker, _lagger, _ma_transformer     return lag_windows, ma_windows, ranker In\u00a0[\u00a0]: Copied! <pre>@app.cell(hide_code=True)\ndef _(mo):\n    mo.md(r\"\"\"Use individually (dataframe agnostic)\"\"\")\n    return\n</pre> @app.cell(hide_code=True) def _(mo):     mo.md(r\"\"\"Use individually (dataframe agnostic)\"\"\")     return In\u00a0[\u00a0]: Copied! <pre>@app.cell\ndef _(df_pandas, df_polars, ranker):\n    # Compare Pandas vs Polars performance\n    import time\n\n    # Test with Pandas\n    start_time = time.time()\n    result_pd = ranker.fit_transform(df_pandas, date_series=df_pandas[\"date\"])\n    pandas_time = time.time() - start_time\n\n    # Test with Polars\n    start_time = time.time()\n    result_pl = ranker.fit_transform(df_polars, date_series=df_polars[\"date\"])\n    polars_time = time.time() - start_time\n\n    print(f\"Pandas execution time: {pandas_time:.4f} seconds\")\n    print(f\"Polars execution time: {polars_time:.4f} seconds\")\n    print(f\"Polars Speedup: {pandas_time / polars_time:.2f}x\")\n\n    # Verify results are equivalent\n    pd_result = result_pd\n    pl_result = result_pl.to_pandas()\n    assert pd_result.equals(pl_result), \"Results should be identical!\"\n\n    # Display sample of results\n    print(result_pl.head())\n    return\n</pre> @app.cell def _(df_pandas, df_polars, ranker):     # Compare Pandas vs Polars performance     import time      # Test with Pandas     start_time = time.time()     result_pd = ranker.fit_transform(df_pandas, date_series=df_pandas[\"date\"])     pandas_time = time.time() - start_time      # Test with Polars     start_time = time.time()     result_pl = ranker.fit_transform(df_polars, date_series=df_polars[\"date\"])     polars_time = time.time() - start_time      print(f\"Pandas execution time: {pandas_time:.4f} seconds\")     print(f\"Polars execution time: {polars_time:.4f} seconds\")     print(f\"Polars Speedup: {pandas_time / polars_time:.2f}x\")      # Verify results are equivalent     pd_result = result_pd     pl_result = result_pl.to_pandas()     assert pd_result.equals(pl_result), \"Results should be identical!\"      # Display sample of results     print(result_pl.head())     return In\u00a0[\u00a0]: Copied! <pre>@app.cell\ndef _(GroupStatsTransformer):\n    feature_mapping = {\n        \"group_1\": [\"close\", \"open\", \"high\", \"open\"],\n        \"group_2\": [\"low\", \"volume\", \"close\"],\n    }\n    group_stats = GroupStatsTransformer(feature_group_mapping=feature_mapping)\n    group_stats\n    return (group_stats,)\n</pre> @app.cell def _(GroupStatsTransformer):     feature_mapping = {         \"group_1\": [\"close\", \"open\", \"high\", \"open\"],         \"group_2\": [\"low\", \"volume\", \"close\"],     }     group_stats = GroupStatsTransformer(feature_group_mapping=feature_mapping)     group_stats     return (group_stats,) In\u00a0[\u00a0]: Copied! <pre>@app.cell\ndef _(df_polars, group_stats):\n    result_df = group_stats.fit_transform(df_polars)\n    print(result_df.head())\n    return\n</pre> @app.cell def _(df_polars, group_stats):     result_df = group_stats.fit_transform(df_polars)     print(result_df.head())     return In\u00a0[\u00a0]: Copied! <pre>@app.cell(hide_code=True)\ndef _(mo):\n    mo.md(r\"\"\"## Or chain them together in a pipeline\"\"\")\n    return\n</pre> @app.cell(hide_code=True) def _(mo):     mo.md(r\"\"\"## Or chain them together in a pipeline\"\"\")     return In\u00a0[\u00a0]: Copied! <pre>@app.cell(hide_code=True)\ndef _():\n    from sklearn import set_config\n    from sklearn.pipeline import make_pipeline\n\n    set_config(enable_metadata_routing=True)\n    return (make_pipeline,)\n</pre> @app.cell(hide_code=True) def _():     from sklearn import set_config     from sklearn.pipeline import make_pipeline      set_config(enable_metadata_routing=True)     return (make_pipeline,) In\u00a0[\u00a0]: Copied! <pre>@app.cell\ndef _(\n    LagTransformer,\n    LogReturnTransformer,\n    MovingAverageTransformer,\n    RankTransformer,\n    lag_windows,\n    ma_windows,\n    make_pipeline,\n):\n    _lagger = LagTransformer(windows=lag_windows).set_transform_request(\n        ticker_series=True\n    )\n    ranker_1 = RankTransformer().set_transform_request(date_series=True)\n    _ma_transformer = MovingAverageTransformer(\n        windows=ma_windows\n    ).set_transform_request(ticker_series=True)\n    _log_return_transformer = LogReturnTransformer().set_transform_request(\n        ticker_series=True\n    )\n    lagged_ranker = make_pipeline(\n        _log_return_transformer, ranker_1, _lagger, _ma_transformer\n    )\n    lagged_ranker\n    return (lagged_ranker,)\n</pre> @app.cell def _(     LagTransformer,     LogReturnTransformer,     MovingAverageTransformer,     RankTransformer,     lag_windows,     ma_windows,     make_pipeline, ):     _lagger = LagTransformer(windows=lag_windows).set_transform_request(         ticker_series=True     )     ranker_1 = RankTransformer().set_transform_request(date_series=True)     _ma_transformer = MovingAverageTransformer(         windows=ma_windows     ).set_transform_request(ticker_series=True)     _log_return_transformer = LogReturnTransformer().set_transform_request(         ticker_series=True     )     lagged_ranker = make_pipeline(         _log_return_transformer, ranker_1, _lagger, _ma_transformer     )     lagged_ranker     return (lagged_ranker,) In\u00a0[\u00a0]: Copied! <pre>@app.cell\ndef _(df_polars, lagged_ranker):\n    feature_names = [\"open\", \"close\", \"volume\"]\n    transformed_df = lagged_ranker.fit_transform(\n        df_polars[feature_names],\n        date_series=df_polars[\"date\"],\n        ticker_series=df_polars[\"ticker\"],\n    )\n    transformed_df.tail()\n    return feature_names, transformed_df\n</pre> @app.cell def _(df_polars, lagged_ranker):     feature_names = [\"open\", \"close\", \"volume\"]     transformed_df = lagged_ranker.fit_transform(         df_polars[feature_names],         date_series=df_polars[\"date\"],         ticker_series=df_polars[\"ticker\"],     )     transformed_df.tail()     return feature_names, transformed_df In\u00a0[\u00a0]: Copied! <pre>@app.cell(hide_code=True)\ndef _(alt, df_polars, pl, transformed_df):\n    # Visualization of the transformation into features\n    chart_df = pl.concat([df_polars, transformed_df], how=\"horizontal\")\n    original_chart = chart_df.plot.line(x=\"date\", y=\"close\", color=\"ticker\").properties(\n        width=300, height=300, title=\"Input: Raw Stock Prices Over Time\"\n    )\n\n    transformed_chart = chart_df.plot.line(\n        x=\"date\",\n        y=\"close_logreturn_rank_lag0_ma20\",\n        color=\"ticker\",\n    ).properties(\n        width=300, height=300, title=\"Pipeline Output: Normalized/Smoothed Features\"\n    )\n    transformed_chart.encoding.y.scale = alt.Scale(domain=[0, 1])\n\n    chart = original_chart | transformed_chart\n    chart.interactive()\n    return (chart_df,)\n</pre> @app.cell(hide_code=True) def _(alt, df_polars, pl, transformed_df):     # Visualization of the transformation into features     chart_df = pl.concat([df_polars, transformed_df], how=\"horizontal\")     original_chart = chart_df.plot.line(x=\"date\", y=\"close\", color=\"ticker\").properties(         width=300, height=300, title=\"Input: Raw Stock Prices Over Time\"     )      transformed_chart = chart_df.plot.line(         x=\"date\",         y=\"close_logreturn_rank_lag0_ma20\",         color=\"ticker\",     ).properties(         width=300, height=300, title=\"Pipeline Output: Normalized/Smoothed Features\"     )     transformed_chart.encoding.y.scale = alt.Scale(domain=[0, 1])      chart = original_chart | transformed_chart     chart.interactive()     return (chart_df,) In\u00a0[\u00a0]: Copied! <pre>@app.cell(hide_code=True)\ndef _(alt, chart_df, lag_windows, ma_windows, pl):\n    def create_feature_visualization(df, columns, title, width=300, height=300):\n        melted_df = df.unpivot(\n            index=[\"date\"], on=columns, variable_name=\"variable\", value_name=\"value\"\n        )\n        chart = melted_df.plot.line(x=\"date\", y=\"value\", color=\"variable\").properties(\n            width=width, height=height, title=title\n        )\n        chart.encoding.y.scale = alt.Scale(domain=[0, 1])\n        return chart\n\n    _ticker = \"Ticker1\"\n    filtered_df = chart_df.filter(pl.col(\"ticker\") == _ticker)\n    ma_columns = [f\"close_logreturn_rank_lag0_ma{w}\" for w in ma_windows]\n    lag_columns = [f\"close_logreturn_rank_lag{i}_ma5\" for i in lag_windows]\n    moving_average_chart = create_feature_visualization(\n        filtered_df, ma_columns, f\"Different Moving Average Windows for {_ticker}\"\n    )\n    lagged_chart = create_feature_visualization(\n        filtered_df, lag_columns, f\"Different Lag Periods for {_ticker}\"\n    )\n    (moving_average_chart | lagged_chart).interactive()\n    return\n</pre> @app.cell(hide_code=True) def _(alt, chart_df, lag_windows, ma_windows, pl):     def create_feature_visualization(df, columns, title, width=300, height=300):         melted_df = df.unpivot(             index=[\"date\"], on=columns, variable_name=\"variable\", value_name=\"value\"         )         chart = melted_df.plot.line(x=\"date\", y=\"value\", color=\"variable\").properties(             width=width, height=height, title=title         )         chart.encoding.y.scale = alt.Scale(domain=[0, 1])         return chart      _ticker = \"Ticker1\"     filtered_df = chart_df.filter(pl.col(\"ticker\") == _ticker)     ma_columns = [f\"close_logreturn_rank_lag0_ma{w}\" for w in ma_windows]     lag_columns = [f\"close_logreturn_rank_lag{i}_ma5\" for i in lag_windows]     moving_average_chart = create_feature_visualization(         filtered_df, ma_columns, f\"Different Moving Average Windows for {_ticker}\"     )     lagged_chart = create_feature_visualization(         filtered_df, lag_columns, f\"Different Lag Periods for {_ticker}\"     )     (moving_average_chart | lagged_chart).interactive()     return In\u00a0[\u00a0]: Copied! <pre>@app.cell(hide_code=True)\ndef _(mo):\n    mo.md(r\"\"\"## Custom centimators keras estimators\"\"\")\n    return\n</pre> @app.cell(hide_code=True) def _(mo):     mo.md(r\"\"\"## Custom centimators keras estimators\"\"\")     return In\u00a0[\u00a0]: Copied! <pre>@app.cell\ndef _():\n    import os\n\n    os.environ[\"KERAS_BACKEND\"] = \"jax\"\n\n    from centimators.model_estimators import MLPRegressor\n\n    return (MLPRegressor,)\n</pre> @app.cell def _():     import os      os.environ[\"KERAS_BACKEND\"] = \"jax\"      from centimators.model_estimators import MLPRegressor      return (MLPRegressor,) In\u00a0[\u00a0]: Copied! <pre>@app.cell\ndef _(MLPRegressor, lagged_ranker, make_pipeline):\n    from sklearn.impute import SimpleImputer\n    imputer = SimpleImputer(strategy=\"constant\", fill_value=0.5).set_output(transform=\"pandas\")\n    mlp_regressor = MLPRegressor().set_fit_request(epochs=True)\n\n    mlp_pipeline = make_pipeline(lagged_ranker, imputer, mlp_regressor)\n    return (mlp_pipeline,)\n</pre> @app.cell def _(MLPRegressor, lagged_ranker, make_pipeline):     from sklearn.impute import SimpleImputer     imputer = SimpleImputer(strategy=\"constant\", fill_value=0.5).set_output(transform=\"pandas\")     mlp_regressor = MLPRegressor().set_fit_request(epochs=True)      mlp_pipeline = make_pipeline(lagged_ranker, imputer, mlp_regressor)     return (mlp_pipeline,) In\u00a0[\u00a0]: Copied! <pre>@app.cell\ndef _(df_polars, feature_names, mlp_pipeline):\n    mlp_pipeline.fit(\n        df_polars[feature_names],\n        df_polars[\"close\"],\n        date_series=df_polars[\"date\"],\n        ticker_series=df_polars[\"ticker\"],\n        epochs=5\n    )\n    return\n</pre> @app.cell def _(df_polars, feature_names, mlp_pipeline):     mlp_pipeline.fit(         df_polars[feature_names],         df_polars[\"close\"],         date_series=df_polars[\"date\"],         ticker_series=df_polars[\"ticker\"],         epochs=5     )     return In\u00a0[\u00a0]: Copied! <pre>@app.cell\ndef _(df_polars, feature_names, mlp_pipeline):\n    mlp_pipeline.transform(\n        df_polars[feature_names],\n        date_series=df_polars[\"date\"],\n        ticker_series=df_polars[\"ticker\"],\n    )\n    return\n</pre> @app.cell def _(df_polars, feature_names, mlp_pipeline):     mlp_pipeline.transform(         df_polars[feature_names],         date_series=df_polars[\"date\"],         ticker_series=df_polars[\"ticker\"],     )     return In\u00a0[\u00a0]: Copied! <pre>if __name__ == \"__main__\":\n    app.run()\n</pre> if __name__ == \"__main__\":     app.run()"},{"location":"user-guide/advanced-pipelines/","title":"Advanced Pipelines","text":"<p><code>centimators</code> transformers are designed to work seamlessly within scikit-learn Pipelines, leveraging its metadata routing capabilities. This allows you to pass data like date or ticker series through the pipeline to the specific transformers that need them, while also chaining together multiple transformers. This is useful for building more complex feature pipelines, but also allows for better cross-validation, hyperparameter tuning, and model selection. For example, if you add a Regressor at the end of the pipeline, you can imagine searching over various combinations of lags, moving average windows, and model hyperparameters during the training process.</p>"},{"location":"user-guide/advanced-pipelines/#building-feature-pipelines","title":"Building Feature Pipelines","text":"<pre><code>from sklearn import set_config\nfrom sklearn.pipeline import make_pipeline\nfrom centimators.feature_transformers import (\n    LogReturnTransformer,\n    RankTransformer,\n    LagTransformer,\n    MovingAverageTransformer,\n)\n\n# Enable metadata routing globally\nset_config(enable_metadata_routing=True)\n\n# Define individual transformers with their parameters\nlog_return_transformer = LogReturnTransformer().set_transform_request(\n    ticker_series=True\n)\nranker = RankTransformer().set_transform_request(date_series=True)\nlag_windows = [0, 5, 10, 15]\nlagger = LagTransformer(windows=lag_windows).set_transform_request(\n    ticker_series=True\n)\nma_windows = [5, 10, 20, 40]\nma_transformer = MovingAverageTransformer(\n    windows=ma_windows\n).set_transform_request(ticker_series=True)\n\n# Create the pipeline\nfeature_pipeline = make_pipeline(\n    log_return_transformer, ranker, lagger, ma_transformer\n)\n</code></pre> <p>Explanation:</p> <ul> <li><code>set_config(enable_metadata_routing=True)</code> turns on scikit-learn's metadata routing.</li> <li><code>set_transform_request(metadata_name=True)</code> on each transformer tells the pipeline that this transformer expects <code>metadata_name</code> (e.g., <code>date_series</code>).</li> <li>When <code>pipeline.fit_transform(X, date_series=dates, ticker_series=tickers)</code> is called:<ul> <li>The <code>date_series</code> is automatically passed to <code>RankTransformer</code>.</li> <li>The <code>ticker_series</code> is automatically passed to <code>LagTransformer</code>, <code>MovingAverageTransformer</code>, and <code>LogReturnTransformer</code>.</li> <li>The output of <code>LogReturnTransformer</code> is passed to <code>RankTransformer</code>.</li> <li>The output of <code>RankTransformer</code> is passed to <code>LagTransformer</code>.</li> <li>The output of <code>LagTransformer</code> is passed to <code>MovingAverageTransformer</code>.</li> </ul> </li> </ul> <p>This allows for complex data transformations where different steps require different auxiliary information, all managed cleanly by the pipeline.</p> <pre><code># Now you can use this pipeline with your data\nfeature_names = [\"open\", \"high\", \"low\", \"close\"]\ntransformed_df = feature_pipeline.fit_transform(\n    df_pl[feature_names],\n    date_series=df_pl[\"date\"],\n    ticker_series=df_pl[\"ticker\"],\n)\n</code></pre> <p>We can take a closer look at a sample output for a single ticker and for a single initial feature. This clearly shows how the close price for a cross-sectional dataset is transformed into a log return, ranked (between 0 and 1) by date, and smoothed (moving average windows) by ticker:</p>"},{"location":"user-guide/advanced-pipelines/#end-to-end-pipeline-with-an-estimator","title":"End-to-End Pipeline with an Estimator","text":"<p>The previous section constructed only the feature engineering part of a workflow. Thanks to Centimators' Keras-backed estimators you can seamlessly append a model as the final step and train everything through a single <code>fit</code> call.</p> <pre><code>from sklearn.impute import SimpleImputer\nfrom centimators.model_estimators import MLPRegressor\n\nlag_windows = [0, 5, 10, 15]\nma_windows = [5, 10, 20, 40]\n\nmlp_pipeline = make_pipeline(\n    feature_pipeline,\n    # Replace NaNs created by lagging with a constant value\n    SimpleImputer(strategy=\"constant\", fill_value=0.5).set_output(transform=\"pandas\"),\n    # Train a neural network in-place\n    MLPRegressor().set_fit_request(epochs=True),\n)\n\nfeature_names = [\"open\", \"high\", \"low\", \"close\"]\n\nmlp_pipeline.fit(\n    df_pl[feature_names],\n    df_pl[\"close\"],\n    date_series=df_pl[\"date\"],\n    ticker_series=df_pl[\"ticker\"],\n    epochs=5,\n)\n</code></pre> <p></p> <p>Just as before, scikit-learn's metadata routing ensures that auxiliary inputs (<code>date_series</code>, <code>ticker_series</code>, <code>epochs</code>) are forwarded only to the steps that explicitly requested them.</p>"},{"location":"user-guide/advanced-pipelines/#cross-validation-and-hyperparameter-tuning","title":"Cross-Validation and Hyperparameter Tuning","text":"<p>Because everything follows the scikit-learn API, you can use standard tools for model validation and optimization:</p> <pre><code>from sklearn.model_selection import GridSearchCV, TimeSeriesSplit\n\n# Define parameter grid for the pipeline\nparam_grid = {\n    'lagTransformer__windows': [[1, 5], [1, 5, 10], [1, 5, 10, 20]],\n    'movingaverageTransformer__windows': [[5, 10], [5, 10, 20], [5, 10, 20, 50]],\n    'mlpregressor__hidden_units': [(64,), (64, 32), (128, 64)],\n    'mlpregressor__dropout_rate': [0.0, 0.1, 0.2],\n}\n\n# Use time series cross-validation\ntscv = TimeSeriesSplit(n_splits=5)\n\n# Grid search with the pipeline\ngrid_search = GridSearchCV(\n    mlp_pipeline,\n    param_grid,\n    cv=tscv,\n    scoring='neg_mean_squared_error',\n    n_jobs=-1\n)\n\n# Fit with metadata routing\ngrid_search.fit(\n    df_pl[feature_names],\n    df_pl[\"target\"],\n    date_series=df_pl[\"date\"],\n    ticker_series=df_pl[\"ticker\"],\n)\n\nprint(f\"Best parameters: {grid_search.best_params_}\")\nprint(f\"Best score: {grid_search.best_score_}\")\n</code></pre> <p>This allows you to search over various combinations of lag windows, moving average windows, model architectures, and other hyperparameters while maintaining proper time series validation and metadata routing. </p>"},{"location":"user-guide/feature-transformers/","title":"Feature Transformers","text":"<p>Feature transformers are the backbone of <code>centimators</code>, providing scikit-learn compatible transformations that work seamlessly with both Pandas and Polars DataFrames through narwhals. These transformers specialize in time-series and cross-sectional financial data transformations.</p> <p>All transformers follow the standard scikit-learn API (<code>fit</code>, <code>transform</code>, <code>fit_transform</code>) and support metadata routing for passing auxiliary data like date or ticker series through pipelines.</p>"},{"location":"user-guide/feature-transformers/#ranktransformer","title":"RankTransformer","text":"<p>Converts numeric features into their normalized rank within groups (typically by date). This is essential for creating market-neutral features that capture relative performance across assets.</p> <pre><code>from centimators.feature_transformers import RankTransformer\n\n# Rank features within each date\nranker = RankTransformer(feature_names=['close', 'volume'])\nranked_features = ranker.fit_transform(\n    X[['close', 'volume']], \n    date_series=df['date']\n)\n# Output: close_rank, volume_rank (values between 0 and 1)\n</code></pre> <p>Key Features: - Normalizes ranks to [0, 1] range - Handles missing values gracefully - Groups by any categorical variable (typically dates)</p>"},{"location":"user-guide/feature-transformers/#lagtransformer","title":"LagTransformer","text":"<p>Creates lagged (shifted) versions of features within groups (typically by ticker). Essential for exposing temporal patterns to machine learning models.</p> <pre><code>from centimators.feature_transformers import LagTransformer\n\n# Create multiple lags for each feature\nlagger = LagTransformer(\n    windows=[1, 5, 10, 20],  # 1-day, 1-week, 2-week, 1-month lags\n    feature_names=['close', 'volume']\n)\nlagged_features = lagger.fit_transform(\n    X[['close', 'volume']], \n    ticker_series=df['ticker']\n)\n# Output: close_lag1, volume_lag1, close_lag5, volume_lag5, etc.\n</code></pre> <p>Key Features: - Preserves temporal ordering within groups - Prevents data leakage across different assets - Configurable lag windows</p>"},{"location":"user-guide/feature-transformers/#movingaveragetransformer","title":"MovingAverageTransformer","text":"<p>Computes rolling averages over specified windows within groups. Useful for smoothing noisy signals and creating trend indicators.</p> <pre><code>from centimators.feature_transformers import MovingAverageTransformer\n\n# Create moving averages with different windows\nma_transformer = MovingAverageTransformer(\n    windows=[5, 10, 20, 50],  # Short to long-term trends\n    feature_names=['close', 'volume']\n)\nma_features = ma_transformer.fit_transform(\n    X[['close', 'volume']], \n    ticker_series=df['ticker']\n)\n# Output: close_ma5, volume_ma5, close_ma10, volume_ma10, etc.\n</code></pre> <p>Key Features: - Rolling window calculations within groups - Multiple window sizes in single transformation - Handles edge cases (insufficient data) naturally</p>"},{"location":"user-guide/feature-transformers/#logreturntransformer","title":"LogReturnTransformer","text":"<p>Computes log returns (first difference of natural logarithm) within groups. The standard way to calculate asset returns while ensuring stationarity.</p> <pre><code>from centimators.feature_transformers import LogReturnTransformer\n\n# Calculate log returns for price data\nlog_return_transformer = LogReturnTransformer(feature_names=['close', 'open'])\nreturns = log_return_transformer.fit_transform(\n    X[['close', 'open']], \n    ticker_series=df['ticker']\n)\n# Output: close_logreturn, open_logreturn\n</code></pre> <p>Key Features: - Mathematically sound return calculation - Handles zero/negative values appropriately - Maintains group boundaries (no cross-ticker contamination)</p>"},{"location":"user-guide/feature-transformers/#groupstatstransformer","title":"GroupStatsTransformer","text":"<p>Calculates statistical measures across groups of related features horizontally (row-wise). Useful for creating aggregate features from multiple related columns.</p> <pre><code>from centimators.feature_transformers import GroupStatsTransformer\n\n# Define feature groups and calculate statistics\nfeature_groups = {\n    'price_features': ['open', 'high', 'low', 'close'],\n    'volume_features': ['volume', 'dollar_volume']\n}\n\nstats_transformer = GroupStatsTransformer(\n    feature_group_mapping=feature_groups,\n    stats=['mean', 'std', 'skew']  # Choose specific statistics\n)\n\ngroup_stats = stats_transformer.fit_transform(X)\n# Output: price_features_groupstats_mean, price_features_groupstats_std, etc.\n</code></pre> <p>Available Statistics: - <code>mean</code>: Average across the group - <code>std</code>: Standard deviation (sample, ddof=1) - <code>skew</code>: Skewness (bias-corrected) - <code>kurt</code>: Excess kurtosis (bias-corrected) - <code>range</code>: Max - Min - <code>cv</code>: Coefficient of variation (std/mean)</p>"},{"location":"user-guide/feature-transformers/#pipeline-integration","title":"Pipeline Integration","text":"<p>All transformers work seamlessly in scikit-learn pipelines with metadata routing:</p> <pre><code>from sklearn import set_config\nfrom sklearn.pipeline import make_pipeline\n\n# Enable metadata routing\nset_config(enable_metadata_routing=True)\n\n# Create pipeline with multiple transformers\npipeline = make_pipeline(\n    LogReturnTransformer().set_transform_request(ticker_series=True),\n    RankTransformer().set_transform_request(date_series=True),\n    LagTransformer(windows=[1, 5, 10]).set_transform_request(ticker_series=True),\n    MovingAverageTransformer(windows=[5, 20]).set_transform_request(ticker_series=True)\n)\n\n# Transform data with metadata routing\ntransformed = pipeline.fit_transform(\n    df[['close', 'volume']],\n    date_series=df['date'],\n    ticker_series=df['ticker']\n)\n</code></pre> <p>Metadata Routing: - <code>date_series</code>: Used by <code>RankTransformer</code> for cross-sectional ranking - <code>ticker_series</code>: Used by temporal transformers (<code>LagTransformer</code>, <code>MovingAverageTransformer</code>, <code>LogReturnTransformer</code>) to maintain asset boundaries </p>"},{"location":"user-guide/keras-cortex/","title":"KerasCortex","text":"<p>Warning</p> <p>This module is a work in progress. It is not yet ready for production use. This is highly experimental and likely to overfit.</p> <p><code>centimators.keras_cortex.KerasCortex</code> introduces a novel approach to model development by automating aspects of architecture search. It wraps a Keras-based estimator and leverages a Large Language Model (LLM) to recursively self-reflect on its own architecture. The LLM suggests improvements to the model's source code, which are then tested. This iterative process allows <code>KerasCortex</code> to refine its internal model over several cycles, potentially discovering more optimal architectures for the given data.</p>"},{"location":"user-guide/keras-cortex/#how-it-works","title":"How It Works","text":"<p>At its core, <code>KerasCortex</code> utilizes DSPy to manage the interaction with the LLM through two key components:</p> <p>The <code>Think</code> Module: A DSPy <code>Module</code> that orchestrates the LLM's code generation process. It uses DSPy's <code>ChainOfThought</code> to enable access to the LLM's reasoning process as it improves its own architecture. The <code>Think</code> module takes the current <code>build_model</code> source code, a history of attempted code modifications and their performance, and an optimization goal (e.g., \"improve validation R2 score\"), then returns the LLM's suggested <code>build_model</code> method modification.</p> <p>The <code>think_loop</code> Method: The heart of <code>KerasCortex</code>'s self-improvement mechanism. This iterative process works as follows:</p> <ol> <li>Establish Baseline: The initial Keras estimator is cloned and trained on the training data to establish baseline performance on the validation set</li> <li>Refine Architecture: For each iteration:<ul> <li>The <code>Think</code> module suggests a new <code>build_model</code> code modification based on the current best code and performance history of all previous iterations</li> <li>The suggested code is executed to create a new <code>build_model</code> function</li> <li>A new model instance is created with the modified architecture and trained on the data</li> <li>The model is evaluated on validation data and compared to the current best</li> <li>If performance improves, the new model becomes the best candidate; each model and its performance is logged for future iterations to reflect upon</li> </ul> </li> <li>Converge?: After all iterations (or early termination due to errors), the best-performing model and complete performance log are returned</li> </ol> <p><code>KerasCortex</code> requires validation data to evaluate the performance of different architectures. It uses this information to guide its self-improvement process. The <code>lm</code> parameter specifies the language model to be used for code generation, and <code>n_iterations</code> controls how many times the model attempts to refine itself. When <code>verbose=True</code>, you can observe the LLM's reasoning process and see how it decides to modify the architecture at each step. </p> <p></p> <p>This approach allows <code>KerasCortex</code> to explore different architectural modifications and converge towards a model that performs well on the given validation data. One could even finetune the prompts or LLM weights directly to improve the quality of the suggestions in its own meta-loop. Access to tools like the keras documentation or arxiv papers could be added as well.</p>"},{"location":"user-guide/keras-cortex/#usage","title":"Usage","text":"<p>While <code>KerasCortex</code> is an advanced tool, its scikit-learn compatible API makes it surprisingly easy to integrate into existing workflows.</p> <pre><code># Define a base Keras estimator, can be anything with a `build_model` method\nbase_mlp = MLPRegressor(\n    hidden_units=(64, 32),\n    dropout_rate=0.1,\n)\n\n# Initialize KerasCortex\n# Ensure your LM (e.g., OpenAI API key) is configured in your environment\ncortex = KerasCortex(\n    base_estimator=base_mlp,\n    n_iterations=5,  # Number of self-reflection iterations\n    lm=\"openai/gpt-4o-mini\", # Or any other dspy.LM compatible model\n    verbose=True\n)\n\n# Fit KerasCortex like any scikit-learn estimator\ncortex.fit(\n    X_train,\n    y_train,\n    validation_data=(X_val, y_val), # Needed for self-reflection\n    epochs=5, # Epochs for each iteration's model training\n    batch_size=128,\n)\n\n# Make predictions with the best model found\npredictions = cortex.predict(X_val)\n</code></pre> <p>View the KerasCortex tutorial for a more detailed example.</p>"},{"location":"user-guide/model-estimators/","title":"Model Estimators","text":"<p>Centimators ships with Keras-backed model estimators that implement the familiar scikit-learn API.  This means you can train state-of-the-art neural networks while still benefitting from the rich tooling ecosystem around scikit-learn \u2013 cross-validation, pipelines, grid-search and more.</p>"},{"location":"user-guide/model-estimators/#tabular-models","title":"Tabular Models","text":"<p>These models are designed for traditional tabular data where each row represents an independent observation.</p>"},{"location":"user-guide/model-estimators/#mlpregressor","title":"MLPRegressor","text":"<p><code>centimators.model_estimators.MLPRegressor</code> is a minimal, fully-connected multilayer perceptron that works out-of-the-box for any tabular regression task.</p> <pre><code>import numpy as np\nimport polars as pl\nfrom centimators.model_estimators import MLPRegressor\n\n# Dummy data: 1 000 samples \u00d7 20 features\ngrng = np.random.default_rng(seed=42)\nX = pl.DataFrame(grng.standard_normal((1000, 20)))\ny = pl.Series(grng.standard_normal(1000))\n\nestimator = MLPRegressor(\n    hidden_units=(128, 64), \n    dropout_rate=0.1,\n    activation=\"relu\",\n    learning_rate=0.001\n)\nestimator.fit(X, y, epochs=10)\n\npredictions = estimator.predict(X)\nprint(predictions[:5])\n</code></pre> <p>Because the estimator inherits from scikit-learn's <code>BaseEstimator</code>, you can seamlessly compose it with the feature transformers provided elsewhere in the library:</p> <pre><code>from sklearn.pipeline import make_pipeline\nfrom centimators.feature_transformers import RankTransformer\n\npipeline = make_pipeline(\n    RankTransformer(feature_names=X.columns),\n    MLPRegressor(hidden_units=(128, 64), epochs=10),\n)\n\npipeline.fit(X, y)\n</code></pre>"},{"location":"user-guide/model-estimators/#bottleneckencoder","title":"BottleneckEncoder","text":"<p><code>centimators.model_estimators.BottleneckEncoder</code> implements a bottleneck autoencoder that can learn latent representations and predict targets simultaneously. This estimator:</p> <ol> <li>Encodes input features to a lower-dimensional latent space</li> <li>Decodes the latent representation back to reconstruct the input  </li> <li>Uses an additional MLP branch to predict targets from the decoded features</li> </ol> <p>The model can be used both as a regressor (via <code>predict</code>) and as a transformer (via <code>transform</code>) to get latent space representations for dimensionality reduction.</p> <pre><code>from centimators.model_estimators import BottleneckEncoder\n\n# Create bottleneck autoencoder\nencoder = BottleneckEncoder(\n    gaussian_noise=0.035,\n    encoder_units=[(1024, 0.1)],  # [(units, dropout_rate), ...]\n    latent_units=(256, 0.1),       # (units, dropout_rate)\n    ae_units=[(96, 0.4)],          # prediction branch architecture\n    activation=\"swish\",\n    reconstruction_loss_weight=1.0,\n    target_loss_weight=1.0\n)\n\n# Fit the model (learns reconstruction + target prediction)\nencoder.fit(X, y, epochs=10)\n\n# Get target predictions\npredictions = encoder.predict(X)\n\n# Get latent space representations for dimensionality reduction\nlatent_features = encoder.transform(X)\nprint(f\"Latent shape: {latent_features.shape}\")  # (1000, 256)\n</code></pre>"},{"location":"user-guide/model-estimators/#sequence-models","title":"Sequence Models","text":"<p>These models are designed for sequential/time-series data where temporal dependencies matter.</p>"},{"location":"user-guide/model-estimators/#sequenceestimator","title":"SequenceEstimator","text":"<p><code>SequenceEstimator</code> is a base class that handles the reshaping of lagged features into the 3-D tensor format required by sequence models like LSTMs and CNNs. It's not meant to be used directly, but rather inherited from by specific sequence model implementations.</p> <p>Key responsibilities: - Reshapes flattened lag matrices into (batch, timesteps, features) tensors - Manages sequence length and feature dimensionality - Provides common sequence model functionality</p>"},{"location":"user-guide/model-estimators/#lstmregressor","title":"LSTMRegressor","text":"<p><code>centimators.model_estimators.LSTMRegressor</code> provides a ready-to-use LSTM implementation for time series regression. It supports stacked LSTM layers, bidirectional processing, and various normalization strategies.</p> <pre><code>from centimators.model_estimators import LSTMRegressor\nfrom centimators.feature_transformers import LagTransformer\n\n# Create lagged features\nlag_transformer = LagTransformer(windows=[1, 2, 3, 4, 5])\nX_lagged = lag_transformer.fit_transform(X, ticker_series=tickers)\n\n# Create LSTM model\nlstm = LSTMRegressor(\n    lag_windows=[1, 2, 3, 4, 5],       # Must match lag transformer\n    n_features_per_timestep=2,          # e.g., price and volume\n    lstm_units=[\n        (128, 0.2, 0.1),                # (units, dropout, recurrent_dropout)\n        (64, 0.1, 0.1),                 # Second LSTM layer\n    ],\n    bidirectional=True,                 # Use bidirectional LSTMs\n    use_layer_norm=True,                # Layer normalization after each LSTM\n    use_batch_norm=False,               # Batch normalization (usually not both)\n    learning_rate=0.001,\n    output_units=1\n)\n\n# Fit the model\nlstm.fit(X_lagged, y, epochs=50, batch_size=32)\n\n# Make predictions\npredictions = lstm.predict(X_lagged)\n</code></pre>"},{"location":"user-guide/model-estimators/#loss-functions","title":"Loss Functions","text":"<p>Centimators provides custom loss functions, alongside support for standard Keras losses.</p>"},{"location":"user-guide/model-estimators/#spearmancorrelation","title":"SpearmanCorrelation","text":"<p><code>centimators.losses.SpearmanCorrelation</code> is a differentiable loss function that optimizes for rank correlation rather than absolute error. This is particularly useful for: - Ranking tasks where relative ordering matters more than exact values - Financial signals where direction and magnitude are more important than precise predictions - Robust training in the presence of outliers</p> <pre><code>from centimators.losses import SpearmanCorrelation\nfrom centimators.model_estimators import MLPRegressor\n\n# Optimize for rank correlation\nmodel = MLPRegressor(\n    hidden_units=(128, 64),\n    loss_function=SpearmanCorrelation(regularization_strength=1e-3),\n    metrics=[\"mae\", \"mse\"]\n)\n</code></pre>"},{"location":"user-guide/model-estimators/#combinedloss","title":"CombinedLoss","text":"<p><code>centimators.losses.CombinedLoss</code> blends mean squared error with Spearman correlation, allowing you to optimize for both accurate predictions and correct ranking simultaneously:</p> <pre><code>from centimators.losses import CombinedLoss\nfrom centimators.model_estimators import LSTMRegressor\n\n# Balance between MSE and rank correlation\ncombined_loss = CombinedLoss(\n    mse_weight=2.0,           # Weight for mean squared error\n    spearman_weight=1.0,      # Weight for Spearman correlation\n    spearman_regularization=1e-3\n)\n\nlstm = LSTMRegressor(\n    lag_windows=[1, 2, 3, 4, 5],\n    n_features_per_timestep=2,\n    lstm_units=[(128, 0.2, 0.1)],\n    loss_function=combined_loss\n)\n</code></pre>"},{"location":"user-guide/model-estimators/#standard-keras-losses","title":"Standard Keras Losses","text":"<p>All estimators also support standard Keras loss functions:</p> <pre><code>from keras.losses import Huber\n\n# Using Huber loss for robust regression\nmodel = MLPRegressor(\n    hidden_units=(128, 64),\n    loss_function=Huber(delta=1.0),  # Or just \"huber\"\n    metrics=[\"mae\", \"mse\"]\n)\n</code></pre>"}]}