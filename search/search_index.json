{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Overview","text":""},{"location":"#centimators-essential-data-transformers-and-model-estimators-for-ml-and-data-science-competitions","title":"Centimators: essential data transformers and model estimators for ML and data science competitions","text":"<p><code>centimators</code> is an open-source python library built on scikit-learn, keras, DSPy, and narwhals: designed for building and sharing dataframe-agnostic (pandas/polars), multi-framework (jax/tf/pytorch/DSPy), sklearn-style (fit/transform/predict) transformers, meta-estimators, and machine learning models for data science competitions like Numerai, Kaggle, and the CrowdCent Challenge.</p>"},{"location":"#built-for","title":"Built for \u2026","text":"\ud83c\udfc6 Competition Data Scientists \ud83d\udcca Financial-ML Engineers \u26a1 Performance Hunters \ud83e\udde0 Neural Network Architects Iterate fast on Numerai, Kaggle, &amp; CrowdCent submissions without reinventing feature engineering each time. Production-ready transformers for ranking, lagging, returns, and rolling stats that respect time &amp; group boundaries. Swap Pandas \u2194 Polars with a one-liner, leverage JAX/TF/PyTorch back-ends, and squeeze every millisecond out of your pipeline. Build and improve upon architectures like bottleneck autoencoders and self-improving neural networks with the same scikit-learn-style API."},{"location":"#key-features","title":"Key Features","text":"<ul> <li> <p> Feature Transformers</p> <p>Time-series and cross-sectional transforms that are dataframe-agnostic (Pandas/Polars) and pipeline-ready.</p> <p>User Guide \u00b7 API</p> </li> <li> <p> Model Estimators</p> <p>Keras 3 estimators with scikit-learn API (MLP, BottleneckEncoder, LSTM, Transformer, and more) + custom losses.</p> <p>User Guide \u00b7 Losses \u00b7 API</p> </li> <li> <p> Meta\u2011Learning</p> <p>DSPyMator (LLM-as-estimator) and KerasCortex (LLM-guided architecture search).</p> <p>DSPyMator \u00b7 KerasCortex</p> </li> <li> <p> Advanced Pipelines</p> <p>Compose transformers and estimators with metadata routing, CV, and hyperparameter tuning.</p> <p>User Guide</p> </li> </ul> <p>Quick install with optional extras</p> <ul> <li>All features (Keras + DSPy + UMAP): <code>uv add \"centimators[all]\"</code></li> </ul>"},{"location":"#basic-usage","title":"Basic Usage","text":"<p>For common transformations like ranking (which groups by date) and lagging (which groups by ticker), use out-of-the-box transformers to create features out of your data, while easily handling group boundaries:</p> <pre><code>from centimators.feature_transformers import RankTransformer, LagTransformer\n\n# Cross-sectional ranking: rank features within each date\nrank_transformer = RankTransformer(feature_names=['close', 'volume'])\nranked_features = rank_transformer.fit_transform(\n    df[['close', 'volume']], \n    date_series=df['date']\n)\n\n# Time-series lagging: create lagged features within each ticker\nlag_transformer = LagTransformer(windows=[1, 5, 10])\nlagged_features = lag_transformer.fit_transform(\n    df[['close', 'volume']], \n    ticker_series=df['ticker']\n)\n</code></pre> <p>For modeling your features, use centimators's model estimators. A family of Keras-backed estimators are available, including MLPRegressor, BottleneckEncoder, LSTMRegressor for sequences, and always more to come.</p> <pre><code>from centimators.model_estimators import MLPRegressor, LSTMRegressor\n\n# For tabular data\nmodel = MLPRegressor()\nmodel.fit(df[feature_names], df['target'])\n\n# For sequential/time-series data\nlstm = LSTMRegressor(\n    lag_windows=[1, 2, 3, 4, 5],\n    n_features_per_timestep=len(feature_names),\n    lstm_units=[(64, 0.2, 0.1)],\n    bidirectional=True\n)\nlstm.fit(lagged_features, df['target'])\n</code></pre>"},{"location":"#getting-started","title":"Getting Started","text":"<p>Ready to dive in? Check out our Installation &amp; Quick Start guide to get up and running in minutes.</p> <p>For comprehensive examples and advanced usage patterns, explore our User Guide and Tutorials.</p>"},{"location":"#philosophy","title":"Philosophy","text":"<p><code>centimators</code> makes heavy use of advanced scikit-learn concepts such as metadata routing. This enables powerful workflows where auxiliary data (like dates or tickers) flow seamlessly through complex pipelines to the specific components that need them.</p> <p>The library is designed for practitioners who need production-ready, composable components for financial machine learning workflows while maintaining compatibility with the broader Python ML ecosystem.</p>"},{"location":"installation-quick-start/","title":"Installation &amp; Quick Start","text":""},{"location":"installation-quick-start/#installation","title":"Installation","text":"MinimalWith keras/jax [keras-jax]With DSPy [dspy]Everything [all] <pre><code>uv add centimators\n</code></pre> <pre><code>uv add \"centimators[keras-jax]\"\n</code></pre> <pre><code>uv add \"centimators[dspy]\"\n</code></pre> <pre><code>uv add \"centimators[all]\"\n</code></pre>"},{"location":"installation-quick-start/#quick-start","title":"Quick Start","text":"<p><code>centimators</code> transformers are dataframe-agnostic, powered by narwhals. You can use the same transformer (like <code>RankTransformer</code>) seamlessly with both Pandas and Polars DataFrames. This transformer calculates the normalized rank of features within each date group.</p> <p>First, let's define some common data: <pre><code>import pandas as pd\nimport polars as pl\n# Create sample OHLCV data for two stocks over four trading days\ndata = {\n    'date': ['2021-01-01', '2021-01-01', '2021-01-02', '2021-01-02', \n             '2021-01-03', '2021-01-03', '2021-01-04', '2021-01-04'],\n    'ticker': ['AAPL', 'MSFT', 'AAPL', 'MSFT', 'AAPL', 'MSFT', 'AAPL', 'MSFT'],\n    'open': [150.0, 280.0, 151.0, 282.0, 152.0, 283.0, 153.0, 284.0],    # Opening prices\n    'high': [152.0, 282.0, 153.0, 284.0, 154.0, 285.0, 155.0, 286.0],    # Daily highs\n    'low': [149.0, 278.0, 150.0, 280.0, 151.0, 281.0, 152.0, 282.0],     # Daily lows\n    'close': [151.0, 281.0, 152.0, 283.0, 153.0, 284.0, 154.0, 285.0],   # Closing prices\n    'volume': [1000000, 800000, 1200000, 900000, 1100000, 850000, 1050000, 820000]  # Trading volume\n}\n\n# Create both Pandas and Polars DataFrames\ndf_pd = pd.DataFrame(data)\ndf_pl = pl.DataFrame(data)\n\n# Define the OHLCV features we want to transform\nfeature_cols = ['volume', 'close']\n</code></pre></p> <p>Now, let's use the transformer: <pre><code>from centimators.feature_transformers import RankTransformer\n\ntransformer = RankTransformer(feature_names=feature_cols)\nresult_pd = transformer.fit_transform(df_pd[feature_cols], date_series=df_pd['date'])\nresult_pl = transformer.fit_transform(df_pl[feature_cols], date_series=df_pl['date'])\n</code></pre></p> <p>Both <code>result_pd</code> (from Pandas) and <code>result_pl</code> (from Polars) will contain the same transformed data in their native DataFrame formats. You may find significant performance gains using Polars for certain operations. </p>"},{"location":"api-reference/dspymator/","title":"DSPyMator","text":""},{"location":"api-reference/dspymator/#centimators.model_estimators.dspymator","title":"<code>centimators.model_estimators.dspymator</code>","text":"<p>DSPyMator: A scikit-learn compatible wrapper for DSPy modules.</p>"},{"location":"api-reference/dspymator/#centimators.model_estimators.dspymator.DSPyMator","title":"<code>DSPyMator</code>  <code>dataclass</code>","text":"<p>               Bases: <code>TransformerMixin</code>, <code>BaseEstimator</code></p> <p>DSPyMator is a scikit-learn compatible wrapper for DSPy modules.</p> <p>Integrates DSPy programs (e.g., ChainOfThought, Predict) into the centimators ecosystem, enabling LLM-based predictions that work seamlessly with sklearn pipelines, cross-validation, and other ML tooling. DSPyMator turns a DSPy <code>Module</code> (e.g., <code>ChainOfThought</code>, <code>Predict</code>) and optimizer (e.g., <code>GEPA</code>, <code>BootstrapFewShot</code>, <code>MIPROv2</code>) into a standard scikit-learn estimator/transformer that operates on tabular rows.</p> <p>The estimator is dataframe-agnostic through narwhals, accepting Polars, Pandas, or numpy arrays. Input features are automatically mapped to the DSPy program's signature fields based on <code>feature_names</code> or column names.</p> Execution Modes <p>By default, uses asynchronous execution (<code>use_async=True</code>) with bounded concurrency for efficient batch processing. Set <code>use_async=False</code> for synchronous execution. Async mode automatically handles nested event loops (e.g., in Jupyter notebooks) when <code>nest_asyncio</code> is installed. Current async support with asyncio means that concurrent requests are best handled for API requests, rather than for fine-tuning of local models' weights.</p> Output Methods <ul> <li><code>predict(X)</code>: Returns target predictions in the same format as input.   If input is numpy array, returns numpy array. If input is dataframe,   returns dataframe with target column(s). For single targets, returns   1D array or single-column dataframe. For multiple targets, returns   2D array or multi-column dataframe.</li> <li><code>transform(X)</code>: Returns all output fields from the DSPy program   (including reasoning, intermediate steps, etc.) as a dataframe in the   same backend as the input. Use this to access full program outputs.</li> </ul> Progress Tracking <p>When <code>verbose=True</code>, displays progress bars using tqdm. Requires <code>tqdm</code> for sync mode and <code>tqdm.asyncio</code> for async mode. Falls back gracefully if tqdm is not installed.</p> Optimization <p>DSPyMator supports automatic prompt optimization via any DSPy optimizer. Pass a configured optimizer instance (e.g., <code>dspy.GEPA</code>, <code>dspy.BootstrapFewShot</code>, <code>dspy.MIPROv2</code>, etc.) to <code>fit()</code> to optimize prompts during training.</p> <p>Different optimizers have different requirements:</p> <ul> <li> <p>Few-shot optimizers (e.g., <code>BootstrapFewShot</code>, <code>LabeledFewShot</code>):   Only need <code>trainset</code>. Pass <code>validation_data=None</code>.</p> </li> <li> <p>Instruction optimizers (e.g., <code>GEPA</code>, <code>MIPROv2</code>, <code>COPRO</code>):   Need both <code>trainset</code> and <code>valset</code>. Provide validation data via <code>validation_data</code>.</p> </li> <li> <p>Finetuning optimizers (e.g., <code>BootstrapFinetune</code>):   May have specific requirements. Consult optimizer documentation.</p> </li> </ul> <p>To use optimization:</p> <ol> <li>Create an optimizer instance:</li> </ol> <pre><code># Example: GEPA for instruction optimization\ngepa = dspy.GEPA(metric=my_metric, auto='light')\n\n# Example: BootstrapFewShot for few-shot learning\nbootstrap = dspy.BootstrapFewShot()\n\n# Example: MIPROv2 for instruction optimization\nmipro = dspy.MIPROv2(metric=my_metric)\n</code></pre> <ol> <li>Pass the optimizer to fit():</li> </ol> <pre><code># With validation split (for optimizers that need valset)\nestimator.fit(X_train, y_train, optimizer=gepa, validation_data=0.2)\n\n# With explicit validation set\nestimator.fit(X_train, y_train, optimizer=gepa, validation_data=(X_val, y_val))\n\n# Without validation (for optimizers that only need trainset)\nestimator.fit(X_train, y_train, optimizer=bootstrap, validation_data=None)\n\n# To use trainset as valset, pass it explicitly\nestimator.fit(X_train, y_train, optimizer=gepa, validation_data=(X_train, y_train))\n</code></pre> <p>After optimization, the original program is stored in <code>original_program_</code> and optimizer results are available in <code>optimizer_results_</code> for inspection (if the optimizer provides detailed results).</p> <p>For more details on optimizers, see: https://dspy.ai/learn/optimization/optimizers/</p> <p>Parameters:</p> Name Type Description Default <code>program</code> <code>Module</code> <p>DSPy module (e.g., dspy.ChainOfThought, dspy.Predict) with a signature defining input and output fields. The signature must be accessible via <code>.predict.signature</code> or <code>.signature</code>.</p> required <code>target_names</code> <code>str | list[str]</code> <p>Field name(s) from the program's output signature to use as predictions. Can be a single string or list of strings. These fields are extracted and returned by <code>predict()</code>.</p> required <code>feature_names</code> <code>list[str] | None</code> <p>Column names mapping input data to signature input fields. If None, inferred from dataframe columns or uses signature field names for numpy arrays. Must match the number of input fields in the signature.</p> <code>None</code> <code>lm</code> <code>str | LM</code> <p>Language model - either a string identifier (e.g., \"openai/gpt-4\") or a pre-configured <code>dspy.LM</code> object. Pass a <code>dspy.LM</code> directly when you need custom configuration like <code>api_key</code> or <code>api_base</code> for providers like OpenRouter. When passing an LM object, <code>temperature</code> and <code>max_tokens</code> are ignored. Defaults to \"openai/gpt-5-nano\".</p> <code>'openai/gpt-5-nano'</code> <code>temperature</code> <code>float</code> <p>Sampling temperature for the language model. Defaults to 1.0.</p> <code>1.0</code> <code>max_tokens</code> <code>int</code> <p>Maximum tokens in model responses. Defaults to 16000.</p> <code>16000</code> <code>use_async</code> <code>bool</code> <p>Whether to use asynchronous execution for batch predictions. Defaults to True. Set to False for synchronous execution.</p> <code>True</code> <code>max_concurrent</code> <code>int</code> <p>Maximum number of concurrent async requests when <code>use_async=True</code>. Defaults to 50.</p> <code>50</code> <code>verbose</code> <code>bool</code> <p>Whether to display progress bars during prediction. Defaults to True. Requires <code>tqdm</code> package for sync mode or <code>tqdm.asyncio</code> for async mode.</p> <code>True</code> <p>Examples:</p> <p>Basic usage with a ChainOfThought or Predict program:</p> <pre><code>import dspy\nfrom centimators.model_estimators import DSPyMator\n\n# Create a DSPy program (e.g., Predict, ChainOfThought, etc.)\nprogram = dspy.Predict(\"text -&gt; sentiment\")\n\n# Create estimator\nestimator = DSPyMator(\n    program=program,\n    target_names=\"sentiment\"\n)\n\nX_train = pl.DataFrame({\n    \"text\": [\"I love this product!\", \"This is terrible.\", \"It's okay.\"]\n})\ny_train = pl.Series([\"positive\", \"negative\", \"neutral\"])\n\n# Fit and predict (get only target predictions)\nestimator.fit(X_train, y_train)  # y_train can be None\npredictions = estimator.predict(X_test)  # returns same type as X_test\n\n# Get all outputs (including reasoning and other intermediate steps of the program)\nfull_outputs = estimator.transform(X_test)  # always returns dataframe\n\n# With optimization:\nimport dspy\n\ngepa = dspy.GEPA(metric=my_metric, auto='light')\nestimator.fit(X_train, y_train, optimizer=gepa, validation_data=0.2)\n</code></pre> Source code in <code>src/centimators/model_estimators/dspymator.py</code> <pre><code>@dataclass(kw_only=True)\nclass DSPyMator(TransformerMixin, BaseEstimator):\n    \"\"\"DSPyMator is a scikit-learn compatible wrapper for DSPy modules.\n\n    Integrates DSPy programs (e.g., ChainOfThought, Predict) into the centimators\n    ecosystem, enabling LLM-based predictions that work seamlessly with sklearn\n    pipelines, cross-validation, and other ML tooling. DSPyMator turns a\n    DSPy `Module` (e.g., `ChainOfThought`, `Predict`) and optimizer (e.g., `GEPA`,\n    `BootstrapFewShot`, `MIPROv2`) into a standard scikit-learn\n    estimator/transformer that operates on tabular rows.\n\n    The estimator is dataframe-agnostic through narwhals, accepting Polars,\n    Pandas, or numpy arrays. Input features are automatically mapped to the DSPy\n    program's signature fields based on `feature_names` or column names.\n\n    Execution Modes:\n        By default, uses asynchronous execution (`use_async=True`) with bounded\n        concurrency for efficient batch processing. Set `use_async=False` for\n        synchronous execution. Async mode automatically handles nested event loops\n        (e.g., in Jupyter notebooks) when `nest_asyncio` is installed. Current async\n        support with asyncio means that concurrent requests are best handled for API\n        requests, rather than for fine-tuning of local models' weights.\n\n    Output Methods:\n        - `predict(X)`: Returns target predictions in the same format as input.\n          If input is numpy array, returns numpy array. If input is dataframe,\n          returns dataframe with target column(s). For single targets, returns\n          1D array or single-column dataframe. For multiple targets, returns\n          2D array or multi-column dataframe.\n        - `transform(X)`: Returns all output fields from the DSPy program\n          (including reasoning, intermediate steps, etc.) as a dataframe in the\n          same backend as the input. Use this to access full program outputs.\n\n    Progress Tracking:\n        When `verbose=True`, displays progress bars using tqdm. Requires `tqdm`\n        for sync mode and `tqdm.asyncio` for async mode. Falls back gracefully\n        if tqdm is not installed.\n\n    Optimization:\n        DSPyMator supports automatic prompt optimization via any DSPy optimizer.\n        Pass a configured optimizer instance (e.g., `dspy.GEPA`, `dspy.BootstrapFewShot`,\n        `dspy.MIPROv2`, etc.) to `fit()` to optimize prompts during training.\n\n        Different optimizers have different requirements:\n\n        - **Few-shot optimizers** (e.g., `BootstrapFewShot`, `LabeledFewShot`):\n          Only need `trainset`. Pass `validation_data=None`.\n\n        - **Instruction optimizers** (e.g., `GEPA`, `MIPROv2`, `COPRO`):\n          Need both `trainset` and `valset`. Provide validation data via `validation_data`.\n\n        - **Finetuning optimizers** (e.g., `BootstrapFinetune`):\n          May have specific requirements. Consult optimizer documentation.\n\n        To use optimization:\n\n        1. Create an optimizer instance:\n\n        ```python\n        # Example: GEPA for instruction optimization\n        gepa = dspy.GEPA(metric=my_metric, auto='light')\n\n        # Example: BootstrapFewShot for few-shot learning\n        bootstrap = dspy.BootstrapFewShot()\n\n        # Example: MIPROv2 for instruction optimization\n        mipro = dspy.MIPROv2(metric=my_metric)\n        ```\n\n        2. Pass the optimizer to fit():\n\n        ```python\n        # With validation split (for optimizers that need valset)\n        estimator.fit(X_train, y_train, optimizer=gepa, validation_data=0.2)\n\n        # With explicit validation set\n        estimator.fit(X_train, y_train, optimizer=gepa, validation_data=(X_val, y_val))\n\n        # Without validation (for optimizers that only need trainset)\n        estimator.fit(X_train, y_train, optimizer=bootstrap, validation_data=None)\n\n        # To use trainset as valset, pass it explicitly\n        estimator.fit(X_train, y_train, optimizer=gepa, validation_data=(X_train, y_train))\n        ```\n\n        After optimization, the original program is stored in `original_program_`\n        and optimizer results are available in `optimizer_results_` for inspection\n        (if the optimizer provides detailed results).\n\n        For more details on optimizers, see: https://dspy.ai/learn/optimization/optimizers/\n\n    Parameters:\n        program: DSPy module (e.g., dspy.ChainOfThought, dspy.Predict) with a signature\n            defining input and output fields. The signature must be accessible\n            via `.predict.signature` or `.signature`.\n        target_names: Field name(s) from the program's output signature to use\n            as predictions. Can be a single string or list of strings. These\n            fields are extracted and returned by `predict()`.\n        feature_names: Column names mapping input data to signature input fields.\n            If None, inferred from dataframe columns or uses signature field names\n            for numpy arrays. Must match the number of input fields in the signature.\n        lm: Language model - either a string identifier (e.g., \"openai/gpt-4\") or a\n            pre-configured `dspy.LM` object. Pass a `dspy.LM` directly when you need\n            custom configuration like `api_key` or `api_base` for providers like OpenRouter.\n            When passing an LM object, `temperature` and `max_tokens` are ignored.\n            Defaults to \"openai/gpt-5-nano\".\n        temperature: Sampling temperature for the language model. Defaults to 1.0.\n        max_tokens: Maximum tokens in model responses. Defaults to 16000.\n        use_async: Whether to use asynchronous execution for batch predictions.\n            Defaults to True. Set to False for synchronous execution.\n        max_concurrent: Maximum number of concurrent async requests when\n            `use_async=True`. Defaults to 50.\n        verbose: Whether to display progress bars during prediction. Defaults to True.\n            Requires `tqdm` package for sync mode or `tqdm.asyncio` for async mode.\n\n    Examples:\n        Basic usage with a ChainOfThought or Predict program:\n\n        ```python\n        import dspy\n        from centimators.model_estimators import DSPyMator\n\n        # Create a DSPy program (e.g., Predict, ChainOfThought, etc.)\n        program = dspy.Predict(\"text -&gt; sentiment\")\n\n        # Create estimator\n        estimator = DSPyMator(\n            program=program,\n            target_names=\"sentiment\"\n        )\n\n        X_train = pl.DataFrame({\n            \"text\": [\"I love this product!\", \"This is terrible.\", \"It's okay.\"]\n        })\n        y_train = pl.Series([\"positive\", \"negative\", \"neutral\"])\n\n        # Fit and predict (get only target predictions)\n        estimator.fit(X_train, y_train)  # y_train can be None\n        predictions = estimator.predict(X_test)  # returns same type as X_test\n\n        # Get all outputs (including reasoning and other intermediate steps of the program)\n        full_outputs = estimator.transform(X_test)  # always returns dataframe\n\n        # With optimization:\n        import dspy\n\n        gepa = dspy.GEPA(metric=my_metric, auto='light')\n        estimator.fit(X_train, y_train, optimizer=gepa, validation_data=0.2)\n        ```\n    \"\"\"\n\n    program: dspy.Module\n    target_names: str | list[str]\n    feature_names: list[str] | None = None\n    lm: str | dspy.LM = \"openai/gpt-5-nano\"\n    temperature: float = 1.0\n    max_tokens: int = 16000\n    use_async: bool = True\n    max_concurrent: int = 50\n    verbose: bool = True\n\n    def _get_signature(self):\n        \"\"\"Extract signature from the DSPy program.\n\n        ChainOfThought stores signature in .predict.signature, while other\n        modules may expose it directly as .signature.\n        \"\"\"\n        if hasattr(self.program, \"predict\") and hasattr(\n            self.program.predict, \"signature\"\n        ):\n            return self.program.predict.signature\n        elif hasattr(self.program, \"signature\"):\n            return self.program.signature\n        else:\n            raise ValueError(\n                f\"Cannot extract signature from program of type {type(self.program)}. \"\n                \"Expected a DSPy module with .predict.signature or .signature attribute.\"\n            )\n\n    def __post_init__(self):\n        self.signature_ = self._get_signature()\n        if isinstance(self.target_names, str):\n            self._target_names = [self.target_names]\n        else:\n            self._target_names = list(self.target_names)\n\n        if not self._target_names:\n            raise ValueError(\"target_names must contain at least one field.\")\n\n    @nw.narwhalify\n    def fit(\n        self,\n        X,\n        y,\n        optimizer: Any | None = None,\n        validation_data: \"tuple[Any, Any] | float | None\" = None,\n        **kwargs,\n    ):\n        \"\"\"Fit the DSPyMator estimator.\n\n        Parameters:\n            X: Training data (dataframe or numpy array).\n            y: Target values (can be None for unsupervised tasks).\n            optimizer: Optional DSPy optimizer instance (e.g., dspy.GEPA, dspy.BootstrapFewShot,\n                dspy.MIPROv2). When provided, enables prompt optimization or finetuning during fit.\n            validation_data: Validation data for optimizers that require it.\n                - If tuple: Use as (X_val, y_val) directly.\n                - If float (0-1): Fraction of training data to use for validation.\n                - If None: No validation set (for optimizers that only need trainset).\n                  To use trainset as valset, pass `(X, y)` explicitly, although some\n                  optimizers may automatically use the trainset as valset if None is passed.\n\n        Returns:\n            self: The fitted estimator.\n\n        Examples:\n            Basic fitting without optimization:\n\n            ```python\n            estimator = DSPyMator(program=program, target_names='label')\n            estimator.fit(X_train, y_train)\n            ```\n\n            With optimizer using auto-split validation:\n\n            ```python\n            gepa_optimizer = dspy.GEPA(metric=my_metric, auto='light', ..., **kwargs)\n            estimator.fit(X_train, y_train, optimizer=gepa_optimizer, validation_data=0.2)\n            ```\n        \"\"\"\n        if isinstance(self.lm, dspy.LM):\n            self.lm_ = self.lm\n        else:\n            self.lm_ = dspy.LM(\n                self.lm, temperature=self.temperature, max_tokens=self.max_tokens\n            )\n\n        self.input_fields_ = list(self.signature_.input_fields.keys())\n\n        if self.feature_names is None:\n            if isinstance(X, numpy.ndarray):\n                self.feature_names = self.input_fields_\n            else:\n                self.feature_names = list(X.columns)\n\n        if len(self.feature_names) != len(self.input_fields_):\n            raise ValueError(\n                f\"Number of feature_names ({len(self.feature_names)}) must match \"\n                f\"number of input_fields ({len(self.input_fields_)})\"\n            )\n\n        # Optimization if requested\n        if optimizer is not None:\n            # Handle validation_data parameter\n            if isinstance(validation_data, float):\n                # Convert to numpy for sklearn compatibility\n                X_train, X_val, y_train, y_val = train_test_split(\n                    _ensure_numpy(X),\n                    _ensure_numpy(y, allow_series=True),\n                    test_size=validation_data,\n                    random_state=42,\n                )\n            elif validation_data is None:\n                # No validation set (for optimizers that only need trainset)\n                X_train, y_train = X, y\n                X_val, y_val = None, None\n            else:\n                # Use provided validation set\n                X_train, y_train = X, y\n                X_val, y_val = validation_data\n\n            # Store original program before optimization\n            self.original_program_ = self.program\n\n            # Convert data to DSPy Examples\n            train_examples = self._convert_to_examples(X_train, y_train)\n            val_examples = (\n                self._convert_to_examples(X_val, y_val) if X_val is not None else None\n            )\n\n            # Run optimizer compilation\n            compile_kwargs = {\n                \"trainset\": train_examples,\n                **({\"valset\": val_examples} if val_examples is not None else {}),\n                **kwargs,\n            }\n\n            with dspy.context(lm=self.lm_):\n                optimized_program = optimizer.compile(self.program, **compile_kwargs)\n\n            # Update program with optimized version\n            self.program = optimized_program\n\n            # Refresh signature and input_fields after optimization\n            self.signature_ = self._get_signature()\n            self.input_fields_ = list(self.signature_.input_fields.keys())\n\n            # Store optimizer results for inspection\n            if hasattr(optimized_program, \"detailed_results\"):\n                self.optimizer_results_ = optimized_program.detailed_results\n\n        self._is_fitted = True\n        return self\n\n    @nw.narwhalify\n    def _convert_to_examples(self, X, y):\n        \"\"\"Convert X, y data to DSPy Example objects.\n\n        Parameters:\n            X: Input features (dataframe or numpy array). Can be None.\n            y: Target values. Can be None.\n\n        Returns:\n            List of dspy.Example objects with inputs marked, or None if X is None.\n        \"\"\"\n        if X is None:\n            return None\n\n        examples = []\n\n        # Build input kwargs for each row\n        if isinstance(X, numpy.ndarray):\n            input_kwargs_list = [\n                {inp: val for inp, val in zip(self.input_fields_, row)} for row in X\n            ]\n        else:\n            input_kwargs_list = [\n                {\n                    inp: row[col]\n                    for inp, col in zip(self.input_fields_, self.feature_names)\n                }\n                for row in X.iter_rows(named=True)\n            ]\n\n        # Add targets and create examples\n        for kwargs, label in zip(input_kwargs_list, y):\n            for i, target_name in enumerate(self._target_names):\n                kwargs[target_name] = label[i] if len(self._target_names) &gt; 1 else label\n            examples.append(dspy.Example(**kwargs).with_inputs(*self.input_fields_))\n\n        return examples\n\n    @nw.narwhalify\n    def _iter_input_kwargs(self, X):\n        if isinstance(X, numpy.ndarray):\n            for row in X:\n                yield {inp: val for inp, val in zip(self.input_fields_, row)}\n        else:\n            for row in X.iter_rows(named=True):\n                yield {\n                    inp: row[col]\n                    for inp, col in zip(self.input_fields_, self.feature_names)\n                }\n\n    def _predict_raw_sync(self, X):\n        \"\"\"Synchronously predict all samples with optional progress bar.\"\"\"\n        input_kwargs = list(self._iter_input_kwargs(X))\n\n        if self.verbose:\n            try:\n                from tqdm import tqdm\n\n                return [\n                    self.program(**kwargs)\n                    for kwargs in tqdm(input_kwargs, desc=\"DSPyMator predicting\")\n                ]\n            except ImportError:\n                warnings.warn(\n                    \"tqdm not installed; progress bar unavailable. Install tqdm for progress tracking.\",\n                    stacklevel=2,\n                )\n                return [self.program(**kwargs) for kwargs in input_kwargs]\n        else:\n            return [self.program(**kwargs) for kwargs in input_kwargs]\n\n    async def _predict_raw_async(self, X):\n        \"\"\"Asynchronously predict all samples with bounded concurrency and optional progress bar.\"\"\"\n        input_kwargs = list(self._iter_input_kwargs(X))\n        semaphore = asyncio.Semaphore(self.max_concurrent)\n\n        async def run_one(kwargs):\n            async with semaphore:\n                return await self.program.acall(**kwargs)\n\n        tasks = [run_one(kwargs) for kwargs in input_kwargs]\n\n        if self.verbose:\n            try:\n                from tqdm.asyncio import tqdm as tqdm_asyncio\n\n                return await tqdm_asyncio.gather(*tasks, desc=\"DSPyMator predicting\")\n            except ImportError:\n                warnings.warn(\n                    \"tqdm not installed; progress bar unavailable. Install tqdm for progress tracking.\",\n                    stacklevel=2,\n                )\n                return await asyncio.gather(*tasks)\n        else:\n            return await asyncio.gather(*tasks)\n\n    def _predict_raw(self, X):\n        \"\"\"Route to sync or async prediction based on use_async flag.\"\"\"\n        with dspy.context(lm=self.lm_):\n            if not self.use_async:\n                return self._predict_raw_sync(X)\n\n            try:\n                asyncio.get_running_loop()\n                # Already in an event loop, use nest_asyncio to enable nested loops\n                try:\n                    import nest_asyncio\n\n                    nest_asyncio.apply()\n                    return asyncio.run(self._predict_raw_async(X))\n                except ImportError:\n                    warnings.warn(\n                        \"nest_asyncio not installed; falling back to synchronous. \"\n                        \"Install nest_asyncio to use async mode in notebooks/event loops.\",\n                        stacklevel=2,\n                    )\n                    return self._predict_raw_sync(X)\n            except RuntimeError:\n                # No event loop, safe to use asyncio.run\n                return asyncio.run(self._predict_raw_async(X))\n\n    @nw.narwhalify\n    def predict(self, X):\n        if not hasattr(self, \"_is_fitted\"):\n            raise ValueError(\"Classifier not fitted. Call fit() first.\")\n        preds = self._predict_raw(X)\n        fields = self._target_names\n\n        if not preds:\n            if len(fields) == 1:\n                empty = numpy.array([], dtype=object)\n            else:\n                empty = numpy.empty((0, len(fields)), dtype=object)\n\n            # Return numpy for numpy input, dataframe for dataframe input\n            if isinstance(X, numpy.ndarray):\n                return empty\n            col_names = fields if len(fields) &gt; 1 else [fields[0]]\n            return nw.from_dict(\n                {name: [] for name in col_names}, backend=nw.get_native_namespace(X)\n            )\n\n        labels = numpy.array(\n            [[getattr(pred, field) for field in fields] for pred in preds]\n        )\n        predictions = labels.squeeze(axis=1) if len(fields) == 1 else labels\n\n        # Return numpy for numpy input, dataframe for dataframe input\n        if isinstance(X, numpy.ndarray):\n            return predictions\n\n        if len(fields) == 1:\n            return nw.from_dict(\n                {fields[0]: predictions}, backend=nw.get_native_namespace(X)\n            )\n        else:\n            cols = {fields[i]: predictions[:, i] for i in range(len(fields))}\n            return nw.from_dict(cols, backend=nw.get_native_namespace(X))\n\n    def _get_output_fields(self):\n        \"\"\"Get all output fields for transform.\"\"\"\n        signature = self._get_signature()\n        output_fields = signature.output_fields.keys()\n\n        return output_fields\n\n    @nw.narwhalify\n    def transform(self, X, y=None):\n        if not hasattr(self, \"_is_fitted\"):\n            raise ValueError(\"Classifier not fitted. Call fit() first.\")\n\n        output_fields = self._get_output_fields()\n        preds = self._predict_raw(X)\n\n        # Build a dictionary of columns\n        data = {\n            field: [getattr(pred, field, None) for pred in preds]\n            for field in output_fields\n        }\n\n        # Create a dataframe in the same backend as input X\n        return nw.from_dict(data, backend=nw.get_native_namespace(X))\n\n    def fit_transform(self, X, y=None, **kwargs):\n        return self.fit(X, y).transform(X, y, **kwargs)\n\n    def get_feature_names_out(self, input_features=None):\n        return self._get_output_fields()\n\n    def __sklearn_is_fitted__(self):\n        return getattr(self, \"_is_fitted\", False)\n</code></pre>"},{"location":"api-reference/dspymator/#centimators.model_estimators.dspymator.DSPyMator.fit","title":"<code>fit(X, y, optimizer=None, validation_data=None, **kwargs)</code>","text":"<p>Fit the DSPyMator estimator.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <p>Training data (dataframe or numpy array).</p> required <code>y</code> <p>Target values (can be None for unsupervised tasks).</p> required <code>optimizer</code> <code>Any | None</code> <p>Optional DSPy optimizer instance (e.g., dspy.GEPA, dspy.BootstrapFewShot, dspy.MIPROv2). When provided, enables prompt optimization or finetuning during fit.</p> <code>None</code> <code>validation_data</code> <code>tuple[Any, Any] | float | None</code> <p>Validation data for optimizers that require it. - If tuple: Use as (X_val, y_val) directly. - If float (0-1): Fraction of training data to use for validation. - If None: No validation set (for optimizers that only need trainset).   To use trainset as valset, pass <code>(X, y)</code> explicitly, although some   optimizers may automatically use the trainset as valset if None is passed.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>self</code> <p>The fitted estimator.</p> <p>Examples:</p> <p>Basic fitting without optimization:</p> <pre><code>estimator = DSPyMator(program=program, target_names='label')\nestimator.fit(X_train, y_train)\n</code></pre> <p>With optimizer using auto-split validation:</p> <pre><code>gepa_optimizer = dspy.GEPA(metric=my_metric, auto='light', ..., **kwargs)\nestimator.fit(X_train, y_train, optimizer=gepa_optimizer, validation_data=0.2)\n</code></pre> Source code in <code>src/centimators/model_estimators/dspymator.py</code> <pre><code>@nw.narwhalify\ndef fit(\n    self,\n    X,\n    y,\n    optimizer: Any | None = None,\n    validation_data: \"tuple[Any, Any] | float | None\" = None,\n    **kwargs,\n):\n    \"\"\"Fit the DSPyMator estimator.\n\n    Parameters:\n        X: Training data (dataframe or numpy array).\n        y: Target values (can be None for unsupervised tasks).\n        optimizer: Optional DSPy optimizer instance (e.g., dspy.GEPA, dspy.BootstrapFewShot,\n            dspy.MIPROv2). When provided, enables prompt optimization or finetuning during fit.\n        validation_data: Validation data for optimizers that require it.\n            - If tuple: Use as (X_val, y_val) directly.\n            - If float (0-1): Fraction of training data to use for validation.\n            - If None: No validation set (for optimizers that only need trainset).\n              To use trainset as valset, pass `(X, y)` explicitly, although some\n              optimizers may automatically use the trainset as valset if None is passed.\n\n    Returns:\n        self: The fitted estimator.\n\n    Examples:\n        Basic fitting without optimization:\n\n        ```python\n        estimator = DSPyMator(program=program, target_names='label')\n        estimator.fit(X_train, y_train)\n        ```\n\n        With optimizer using auto-split validation:\n\n        ```python\n        gepa_optimizer = dspy.GEPA(metric=my_metric, auto='light', ..., **kwargs)\n        estimator.fit(X_train, y_train, optimizer=gepa_optimizer, validation_data=0.2)\n        ```\n    \"\"\"\n    if isinstance(self.lm, dspy.LM):\n        self.lm_ = self.lm\n    else:\n        self.lm_ = dspy.LM(\n            self.lm, temperature=self.temperature, max_tokens=self.max_tokens\n        )\n\n    self.input_fields_ = list(self.signature_.input_fields.keys())\n\n    if self.feature_names is None:\n        if isinstance(X, numpy.ndarray):\n            self.feature_names = self.input_fields_\n        else:\n            self.feature_names = list(X.columns)\n\n    if len(self.feature_names) != len(self.input_fields_):\n        raise ValueError(\n            f\"Number of feature_names ({len(self.feature_names)}) must match \"\n            f\"number of input_fields ({len(self.input_fields_)})\"\n        )\n\n    # Optimization if requested\n    if optimizer is not None:\n        # Handle validation_data parameter\n        if isinstance(validation_data, float):\n            # Convert to numpy for sklearn compatibility\n            X_train, X_val, y_train, y_val = train_test_split(\n                _ensure_numpy(X),\n                _ensure_numpy(y, allow_series=True),\n                test_size=validation_data,\n                random_state=42,\n            )\n        elif validation_data is None:\n            # No validation set (for optimizers that only need trainset)\n            X_train, y_train = X, y\n            X_val, y_val = None, None\n        else:\n            # Use provided validation set\n            X_train, y_train = X, y\n            X_val, y_val = validation_data\n\n        # Store original program before optimization\n        self.original_program_ = self.program\n\n        # Convert data to DSPy Examples\n        train_examples = self._convert_to_examples(X_train, y_train)\n        val_examples = (\n            self._convert_to_examples(X_val, y_val) if X_val is not None else None\n        )\n\n        # Run optimizer compilation\n        compile_kwargs = {\n            \"trainset\": train_examples,\n            **({\"valset\": val_examples} if val_examples is not None else {}),\n            **kwargs,\n        }\n\n        with dspy.context(lm=self.lm_):\n            optimized_program = optimizer.compile(self.program, **compile_kwargs)\n\n        # Update program with optimized version\n        self.program = optimized_program\n\n        # Refresh signature and input_fields after optimization\n        self.signature_ = self._get_signature()\n        self.input_fields_ = list(self.signature_.input_fields.keys())\n\n        # Store optimizer results for inspection\n        if hasattr(optimized_program, \"detailed_results\"):\n            self.optimizer_results_ = optimized_program.detailed_results\n\n    self._is_fitted = True\n    return self\n</code></pre>"},{"location":"api-reference/feature_transformers/","title":"Feature Transformers","text":""},{"location":"api-reference/feature_transformers/#centimators.feature_transformers.ranking","title":"<code>centimators.feature_transformers.ranking</code>","text":"<p>Ranking transformers for cross-sectional normalization.</p>"},{"location":"api-reference/feature_transformers/#centimators.feature_transformers.ranking.RankTransformer","title":"<code>RankTransformer</code>","text":"<p>               Bases: <code>_BaseFeatureTransformer</code></p> <p>RankTransformer transforms features into their normalized rank within groups defined by a date series.</p> <p>Parameters:</p> Name Type Description Default <code>feature_names</code> <code>list of str</code> <p>Names of columns to transform. If None, all columns of X are used.</p> <code>None</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; from centimators.feature_transformers import RankTransformer\n&gt;&gt;&gt; df = pd.DataFrame({\n...     'date': ['2021-01-01', '2021-01-01', '2021-01-02'],\n...     'feature1': [3, 1, 2],\n...     'feature2': [30, 20, 10]\n... })\n&gt;&gt;&gt; transformer = RankTransformer(feature_names=['feature1', 'feature2'])\n&gt;&gt;&gt; result = transformer.fit_transform(df[['feature1', 'feature2']], date_series=df['date'])\n&gt;&gt;&gt; print(result)\n   feature1_rank  feature2_rank\n0            0.5            0.5\n1            1.0            1.0\n2            1.0            1.0\n</code></pre> Source code in <code>src/centimators/feature_transformers/ranking.py</code> <pre><code>class RankTransformer(_BaseFeatureTransformer):\n    \"\"\"\n    RankTransformer transforms features into their normalized rank within groups defined by a date series.\n\n    Args:\n        feature_names (list of str, optional): Names of columns to transform.\n            If None, all columns of X are used.\n\n    Examples:\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; from centimators.feature_transformers import RankTransformer\n        &gt;&gt;&gt; df = pd.DataFrame({\n        ...     'date': ['2021-01-01', '2021-01-01', '2021-01-02'],\n        ...     'feature1': [3, 1, 2],\n        ...     'feature2': [30, 20, 10]\n        ... })\n        &gt;&gt;&gt; transformer = RankTransformer(feature_names=['feature1', 'feature2'])\n        &gt;&gt;&gt; result = transformer.fit_transform(df[['feature1', 'feature2']], date_series=df['date'])\n        &gt;&gt;&gt; print(result)\n           feature1_rank  feature2_rank\n        0            0.5            0.5\n        1            1.0            1.0\n        2            1.0            1.0\n    \"\"\"\n\n    def __init__(self, feature_names=None):\n        super().__init__(feature_names)\n\n    @nw.narwhalify(allow_series=True)\n    def transform(self, X: FrameT, y=None, date_series: IntoSeries = None) -&gt; FrameT:\n        \"\"\"Transforms features to their normalized rank.\n\n        Args:\n            X (FrameT): Input data frame.\n            y (Any, optional): Ignored. Kept for compatibility.\n            date_series (IntoSeries, optional): Series defining groups for ranking (e.g., dates).\n\n        Returns:\n            FrameT: Transformed data frame with ranked features.\n        \"\"\"\n        X, date_col_name = _attach_group(X, date_series, \"date\")\n\n        # compute absolute rank for each feature\n        rank_columns: list[nw.Expr] = [\n            nw.col(feature_name)\n            .rank()\n            .over(date_col_name)\n            .alias(f\"{feature_name}_rank_temp\")\n            for feature_name in self.feature_names\n        ]\n\n        # compute count for each feature\n        count_columns: list[nw.Expr] = [\n            nw.col(feature_name)\n            .count()\n            .over(date_col_name)\n            .alias(f\"{feature_name}_count\")\n            for feature_name in self.feature_names\n        ]\n\n        X = X.select([*rank_columns, *count_columns])\n\n        # compute normalized rank for each feature\n        final_columns: list[nw.Expr] = [\n            (\n                nw.col(f\"{feature_name}_rank_temp\") / nw.col(f\"{feature_name}_count\")\n            ).alias(f\"{feature_name}_rank\")\n            for feature_name in self.feature_names\n        ]\n\n        X = X.select(final_columns)\n\n        return X\n\n    def get_feature_names_out(self, input_features=None) -&gt; list[str]:\n        \"\"\"Returns the output feature names.\n\n        Args:\n            input_features (list[str], optional): Ignored. Kept for compatibility.\n\n        Returns:\n            list[str]: List of transformed feature names.\n        \"\"\"\n        return [f\"{feature_name}_rank\" for feature_name in self.feature_names]\n</code></pre>"},{"location":"api-reference/feature_transformers/#centimators.feature_transformers.ranking.RankTransformer.transform","title":"<code>transform(X, y=None, date_series=None)</code>","text":"<p>Transforms features to their normalized rank.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>FrameT</code> <p>Input data frame.</p> required <code>y</code> <code>Any</code> <p>Ignored. Kept for compatibility.</p> <code>None</code> <code>date_series</code> <code>IntoSeries</code> <p>Series defining groups for ranking (e.g., dates).</p> <code>None</code> <p>Returns:</p> Name Type Description <code>FrameT</code> <code>FrameT</code> <p>Transformed data frame with ranked features.</p> Source code in <code>src/centimators/feature_transformers/ranking.py</code> <pre><code>@nw.narwhalify(allow_series=True)\ndef transform(self, X: FrameT, y=None, date_series: IntoSeries = None) -&gt; FrameT:\n    \"\"\"Transforms features to their normalized rank.\n\n    Args:\n        X (FrameT): Input data frame.\n        y (Any, optional): Ignored. Kept for compatibility.\n        date_series (IntoSeries, optional): Series defining groups for ranking (e.g., dates).\n\n    Returns:\n        FrameT: Transformed data frame with ranked features.\n    \"\"\"\n    X, date_col_name = _attach_group(X, date_series, \"date\")\n\n    # compute absolute rank for each feature\n    rank_columns: list[nw.Expr] = [\n        nw.col(feature_name)\n        .rank()\n        .over(date_col_name)\n        .alias(f\"{feature_name}_rank_temp\")\n        for feature_name in self.feature_names\n    ]\n\n    # compute count for each feature\n    count_columns: list[nw.Expr] = [\n        nw.col(feature_name)\n        .count()\n        .over(date_col_name)\n        .alias(f\"{feature_name}_count\")\n        for feature_name in self.feature_names\n    ]\n\n    X = X.select([*rank_columns, *count_columns])\n\n    # compute normalized rank for each feature\n    final_columns: list[nw.Expr] = [\n        (\n            nw.col(f\"{feature_name}_rank_temp\") / nw.col(f\"{feature_name}_count\")\n        ).alias(f\"{feature_name}_rank\")\n        for feature_name in self.feature_names\n    ]\n\n    X = X.select(final_columns)\n\n    return X\n</code></pre>"},{"location":"api-reference/feature_transformers/#centimators.feature_transformers.ranking.RankTransformer.get_feature_names_out","title":"<code>get_feature_names_out(input_features=None)</code>","text":"<p>Returns the output feature names.</p> <p>Parameters:</p> Name Type Description Default <code>input_features</code> <code>list[str]</code> <p>Ignored. Kept for compatibility.</p> <code>None</code> <p>Returns:</p> Type Description <code>list[str]</code> <p>list[str]: List of transformed feature names.</p> Source code in <code>src/centimators/feature_transformers/ranking.py</code> <pre><code>def get_feature_names_out(self, input_features=None) -&gt; list[str]:\n    \"\"\"Returns the output feature names.\n\n    Args:\n        input_features (list[str], optional): Ignored. Kept for compatibility.\n\n    Returns:\n        list[str]: List of transformed feature names.\n    \"\"\"\n    return [f\"{feature_name}_rank\" for feature_name in self.feature_names]\n</code></pre>"},{"location":"api-reference/feature_transformers/#centimators.feature_transformers.time_series","title":"<code>centimators.feature_transformers.time_series</code>","text":"<p>Time-series feature transformers for grouped temporal operations.</p>"},{"location":"api-reference/feature_transformers/#centimators.feature_transformers.time_series.LagTransformer","title":"<code>LagTransformer</code>","text":"<p>               Bases: <code>_BaseFeatureTransformer</code></p> <p>LagTransformer shifts features by specified lag windows within groups defined by a ticker series.</p> <p>Parameters:</p> Name Type Description Default <code>windows</code> <code>iterable of int</code> <p>Lag periods to compute. Each feature will have shifted versions for each lag.</p> required <code>feature_names</code> <code>list of str</code> <p>Names of columns to transform. If None, all columns of X are used.</p> <code>None</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; from centimators.feature_transformers import LagTransformer\n&gt;&gt;&gt; df = pd.DataFrame({\n...     'ticker': ['A', 'A', 'A', 'B', 'B'],\n...     'price': [10, 11, 12, 20, 21]\n... })\n&gt;&gt;&gt; transformer = LagTransformer(windows=[1, 2], feature_names=['price'])\n&gt;&gt;&gt; result = transformer.fit_transform(df[['price']], ticker_series=df['ticker'])\n&gt;&gt;&gt; print(result)\n   price_lag1  price_lag2\n0         NaN         NaN\n1        10.0         NaN\n2        11.0        10.0\n3         NaN         NaN\n4        20.0         NaN\n</code></pre> Source code in <code>src/centimators/feature_transformers/time_series.py</code> <pre><code>class LagTransformer(_BaseFeatureTransformer):\n    \"\"\"\n    LagTransformer shifts features by specified lag windows within groups defined by a ticker series.\n\n    Args:\n        windows (iterable of int): Lag periods to compute. Each feature will have\n            shifted versions for each lag.\n        feature_names (list of str, optional): Names of columns to transform.\n            If None, all columns of X are used.\n\n    Examples:\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; from centimators.feature_transformers import LagTransformer\n        &gt;&gt;&gt; df = pd.DataFrame({\n        ...     'ticker': ['A', 'A', 'A', 'B', 'B'],\n        ...     'price': [10, 11, 12, 20, 21]\n        ... })\n        &gt;&gt;&gt; transformer = LagTransformer(windows=[1, 2], feature_names=['price'])\n        &gt;&gt;&gt; result = transformer.fit_transform(df[['price']], ticker_series=df['ticker'])\n        &gt;&gt;&gt; print(result)\n           price_lag1  price_lag2\n        0         NaN         NaN\n        1        10.0         NaN\n        2        11.0        10.0\n        3         NaN         NaN\n        4        20.0         NaN\n    \"\"\"\n\n    def __init__(self, windows, feature_names=None):\n        self.windows = sorted(windows, reverse=True)\n        super().__init__(feature_names)\n\n    @nw.narwhalify(allow_series=True)\n    def transform(\n        self,\n        X: FrameT,\n        y=None,\n        ticker_series: IntoSeries = None,\n    ) -&gt; FrameT:\n        \"\"\"Applies lag transformation to the features.\n\n        Args:\n            X (FrameT): Input data frame.\n            y (Any, optional): Ignored. Kept for compatibility.\n            ticker_series (IntoSeries, optional): Series defining groups for lagging (e.g., tickers).\n\n        Returns:\n            FrameT: Transformed data frame with lagged features. Columns are ordered\n                by lag (as in `self.windows`), then by feature (as in `self.feature_names`).\n                For example, with `windows=[2,1]` and `feature_names=['A','B']`,\n                the output columns will be `A_lag2, B_lag2, A_lag1, B_lag1`.\n        \"\"\"\n        X, ticker_col_name = _attach_group(X, ticker_series, \"ticker\")\n\n        lag_columns = [\n            nw.col(feature_name)\n            .shift(lag)\n            .alias(f\"{feature_name}_lag{lag}\")\n            .over(ticker_col_name)\n            for lag in self.windows  # Iterate over lags first\n            for feature_name in self.feature_names  # Then over feature names\n        ]\n\n        X = X.select(lag_columns)\n\n        return X\n\n    def get_feature_names_out(self, input_features=None) -&gt; list[str]:\n        \"\"\"Returns the output feature names.\n\n        Args:\n            input_features (list[str], optional): Ignored. Kept for compatibility.\n\n        Returns:\n            list[str]: List of transformed feature names, ordered by lag, then by feature.\n        \"\"\"\n        return [\n            f\"{feature_name}_lag{lag}\"\n            for lag in self.windows  # Iterate over lags first\n            for feature_name in self.feature_names  # Then over feature names\n        ]\n</code></pre>"},{"location":"api-reference/feature_transformers/#centimators.feature_transformers.time_series.LagTransformer.transform","title":"<code>transform(X, y=None, ticker_series=None)</code>","text":"<p>Applies lag transformation to the features.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>FrameT</code> <p>Input data frame.</p> required <code>y</code> <code>Any</code> <p>Ignored. Kept for compatibility.</p> <code>None</code> <code>ticker_series</code> <code>IntoSeries</code> <p>Series defining groups for lagging (e.g., tickers).</p> <code>None</code> <p>Returns:</p> Name Type Description <code>FrameT</code> <code>FrameT</code> <p>Transformed data frame with lagged features. Columns are ordered by lag (as in <code>self.windows</code>), then by feature (as in <code>self.feature_names</code>). For example, with <code>windows=[2,1]</code> and <code>feature_names=['A','B']</code>, the output columns will be <code>A_lag2, B_lag2, A_lag1, B_lag1</code>.</p> Source code in <code>src/centimators/feature_transformers/time_series.py</code> <pre><code>@nw.narwhalify(allow_series=True)\ndef transform(\n    self,\n    X: FrameT,\n    y=None,\n    ticker_series: IntoSeries = None,\n) -&gt; FrameT:\n    \"\"\"Applies lag transformation to the features.\n\n    Args:\n        X (FrameT): Input data frame.\n        y (Any, optional): Ignored. Kept for compatibility.\n        ticker_series (IntoSeries, optional): Series defining groups for lagging (e.g., tickers).\n\n    Returns:\n        FrameT: Transformed data frame with lagged features. Columns are ordered\n            by lag (as in `self.windows`), then by feature (as in `self.feature_names`).\n            For example, with `windows=[2,1]` and `feature_names=['A','B']`,\n            the output columns will be `A_lag2, B_lag2, A_lag1, B_lag1`.\n    \"\"\"\n    X, ticker_col_name = _attach_group(X, ticker_series, \"ticker\")\n\n    lag_columns = [\n        nw.col(feature_name)\n        .shift(lag)\n        .alias(f\"{feature_name}_lag{lag}\")\n        .over(ticker_col_name)\n        for lag in self.windows  # Iterate over lags first\n        for feature_name in self.feature_names  # Then over feature names\n    ]\n\n    X = X.select(lag_columns)\n\n    return X\n</code></pre>"},{"location":"api-reference/feature_transformers/#centimators.feature_transformers.time_series.LagTransformer.get_feature_names_out","title":"<code>get_feature_names_out(input_features=None)</code>","text":"<p>Returns the output feature names.</p> <p>Parameters:</p> Name Type Description Default <code>input_features</code> <code>list[str]</code> <p>Ignored. Kept for compatibility.</p> <code>None</code> <p>Returns:</p> Type Description <code>list[str]</code> <p>list[str]: List of transformed feature names, ordered by lag, then by feature.</p> Source code in <code>src/centimators/feature_transformers/time_series.py</code> <pre><code>def get_feature_names_out(self, input_features=None) -&gt; list[str]:\n    \"\"\"Returns the output feature names.\n\n    Args:\n        input_features (list[str], optional): Ignored. Kept for compatibility.\n\n    Returns:\n        list[str]: List of transformed feature names, ordered by lag, then by feature.\n    \"\"\"\n    return [\n        f\"{feature_name}_lag{lag}\"\n        for lag in self.windows  # Iterate over lags first\n        for feature_name in self.feature_names  # Then over feature names\n    ]\n</code></pre>"},{"location":"api-reference/feature_transformers/#centimators.feature_transformers.time_series.MovingAverageTransformer","title":"<code>MovingAverageTransformer</code>","text":"<p>               Bases: <code>_BaseFeatureTransformer</code></p> <p>MovingAverageTransformer computes the moving average of a feature over a specified window.</p> <p>Parameters:</p> Name Type Description Default <code>windows</code> <code>list of int</code> <p>The windows over which to compute the moving average.</p> required <code>feature_names</code> <code>list of str</code> <p>The names of the features to compute the moving average for.</p> <code>None</code> Source code in <code>src/centimators/feature_transformers/time_series.py</code> <pre><code>class MovingAverageTransformer(_BaseFeatureTransformer):\n    \"\"\"\n    MovingAverageTransformer computes the moving average of a feature over a specified window.\n\n    Args:\n        windows (list of int): The windows over which to compute the moving average.\n        feature_names (list of str, optional): The names of the features to compute\n            the moving average for.\n    \"\"\"\n\n    def __init__(self, windows, feature_names=None):\n        self.windows = windows\n        super().__init__(feature_names)\n\n    @nw.narwhalify(allow_series=True)\n    def transform(self, X: FrameT, y=None, ticker_series: IntoSeries = None) -&gt; FrameT:\n        \"\"\"Applies moving average transformation to the features.\n\n        Args:\n            X (FrameT): Input data frame.\n            y (Any, optional): Ignored. Kept for compatibility.\n            ticker_series (IntoSeries, optional): Series defining groups for moving average (e.g., tickers).\n\n        Returns:\n            FrameT: Transformed data frame with moving average features.\n        \"\"\"\n        X, ticker_col_name = _attach_group(X, ticker_series, \"ticker\")\n\n        ma_columns = [\n            nw.col(feature_name)\n            .rolling_mean(window_size=window)\n            .over(ticker_col_name)\n            .alias(f\"{feature_name}_ma{window}\")\n            for feature_name in self.feature_names\n            for window in self.windows\n        ]\n\n        X = X.select(ma_columns)\n\n        return X\n\n    def get_feature_names_out(self, input_features=None) -&gt; list[str]:\n        \"\"\"Returns the output feature names.\n\n        Args:\n            input_features (list[str], optional): Ignored. Kept for compatibility.\n\n        Returns:\n            list[str]: List of transformed feature names.\n        \"\"\"\n        return [\n            f\"{feature_name}_ma{window}\"\n            for feature_name in self.feature_names\n            for window in self.windows\n        ]\n</code></pre>"},{"location":"api-reference/feature_transformers/#centimators.feature_transformers.time_series.MovingAverageTransformer.transform","title":"<code>transform(X, y=None, ticker_series=None)</code>","text":"<p>Applies moving average transformation to the features.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>FrameT</code> <p>Input data frame.</p> required <code>y</code> <code>Any</code> <p>Ignored. Kept for compatibility.</p> <code>None</code> <code>ticker_series</code> <code>IntoSeries</code> <p>Series defining groups for moving average (e.g., tickers).</p> <code>None</code> <p>Returns:</p> Name Type Description <code>FrameT</code> <code>FrameT</code> <p>Transformed data frame with moving average features.</p> Source code in <code>src/centimators/feature_transformers/time_series.py</code> <pre><code>@nw.narwhalify(allow_series=True)\ndef transform(self, X: FrameT, y=None, ticker_series: IntoSeries = None) -&gt; FrameT:\n    \"\"\"Applies moving average transformation to the features.\n\n    Args:\n        X (FrameT): Input data frame.\n        y (Any, optional): Ignored. Kept for compatibility.\n        ticker_series (IntoSeries, optional): Series defining groups for moving average (e.g., tickers).\n\n    Returns:\n        FrameT: Transformed data frame with moving average features.\n    \"\"\"\n    X, ticker_col_name = _attach_group(X, ticker_series, \"ticker\")\n\n    ma_columns = [\n        nw.col(feature_name)\n        .rolling_mean(window_size=window)\n        .over(ticker_col_name)\n        .alias(f\"{feature_name}_ma{window}\")\n        for feature_name in self.feature_names\n        for window in self.windows\n    ]\n\n    X = X.select(ma_columns)\n\n    return X\n</code></pre>"},{"location":"api-reference/feature_transformers/#centimators.feature_transformers.time_series.MovingAverageTransformer.get_feature_names_out","title":"<code>get_feature_names_out(input_features=None)</code>","text":"<p>Returns the output feature names.</p> <p>Parameters:</p> Name Type Description Default <code>input_features</code> <code>list[str]</code> <p>Ignored. Kept for compatibility.</p> <code>None</code> <p>Returns:</p> Type Description <code>list[str]</code> <p>list[str]: List of transformed feature names.</p> Source code in <code>src/centimators/feature_transformers/time_series.py</code> <pre><code>def get_feature_names_out(self, input_features=None) -&gt; list[str]:\n    \"\"\"Returns the output feature names.\n\n    Args:\n        input_features (list[str], optional): Ignored. Kept for compatibility.\n\n    Returns:\n        list[str]: List of transformed feature names.\n    \"\"\"\n    return [\n        f\"{feature_name}_ma{window}\"\n        for feature_name in self.feature_names\n        for window in self.windows\n    ]\n</code></pre>"},{"location":"api-reference/feature_transformers/#centimators.feature_transformers.time_series.LogReturnTransformer","title":"<code>LogReturnTransformer</code>","text":"<p>               Bases: <code>_BaseFeatureTransformer</code></p> <p>LogReturnTransformer computes the log return of a feature.</p> <p>Parameters:</p> Name Type Description Default <code>feature_names</code> <code>list of str</code> <p>Names of columns to transform. If None, all columns of X are used.</p> <code>None</code> Source code in <code>src/centimators/feature_transformers/time_series.py</code> <pre><code>class LogReturnTransformer(_BaseFeatureTransformer):\n    \"\"\"\n    LogReturnTransformer computes the log return of a feature.\n\n    Args:\n        feature_names (list of str, optional): Names of columns to transform.\n            If None, all columns of X are used.\n    \"\"\"\n\n    def __init__(self, feature_names=None):\n        super().__init__(feature_names)\n\n    @nw.narwhalify(allow_series=True)\n    def transform(self, X: FrameT, y=None, ticker_series: IntoSeries = None) -&gt; FrameT:\n        \"\"\"Applies log return transformation to the features.\n\n        Args:\n            X (FrameT): Input data frame.\n            y (Any, optional): Ignored. Kept for compatibility.\n            ticker_series (IntoSeries, optional): Series defining groups for log return (e.g., tickers).\n\n        Returns:\n            FrameT: Transformed data frame with log return features.\n        \"\"\"\n        X, ticker_col_name = _attach_group(X, ticker_series, \"ticker\")\n\n        log_return_columns = [\n            nw.col(feature_name)\n            .log()\n            .diff()\n            .over(ticker_col_name)\n            .alias(f\"{feature_name}_logreturn\")\n            for feature_name in self.feature_names\n        ]\n\n        X = X.select(log_return_columns)\n\n        return X\n\n    def get_feature_names_out(self, input_features=None) -&gt; list[str]:\n        \"\"\"Returns the output feature names.\n\n        Args:\n            input_features (list[str], optional): Ignored. Kept for compatibility.\n\n        Returns:\n            list[str]: List of transformed feature names.\n        \"\"\"\n        return [f\"{feature_name}_logreturn\" for feature_name in self.feature_names]\n</code></pre>"},{"location":"api-reference/feature_transformers/#centimators.feature_transformers.time_series.LogReturnTransformer.transform","title":"<code>transform(X, y=None, ticker_series=None)</code>","text":"<p>Applies log return transformation to the features.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>FrameT</code> <p>Input data frame.</p> required <code>y</code> <code>Any</code> <p>Ignored. Kept for compatibility.</p> <code>None</code> <code>ticker_series</code> <code>IntoSeries</code> <p>Series defining groups for log return (e.g., tickers).</p> <code>None</code> <p>Returns:</p> Name Type Description <code>FrameT</code> <code>FrameT</code> <p>Transformed data frame with log return features.</p> Source code in <code>src/centimators/feature_transformers/time_series.py</code> <pre><code>@nw.narwhalify(allow_series=True)\ndef transform(self, X: FrameT, y=None, ticker_series: IntoSeries = None) -&gt; FrameT:\n    \"\"\"Applies log return transformation to the features.\n\n    Args:\n        X (FrameT): Input data frame.\n        y (Any, optional): Ignored. Kept for compatibility.\n        ticker_series (IntoSeries, optional): Series defining groups for log return (e.g., tickers).\n\n    Returns:\n        FrameT: Transformed data frame with log return features.\n    \"\"\"\n    X, ticker_col_name = _attach_group(X, ticker_series, \"ticker\")\n\n    log_return_columns = [\n        nw.col(feature_name)\n        .log()\n        .diff()\n        .over(ticker_col_name)\n        .alias(f\"{feature_name}_logreturn\")\n        for feature_name in self.feature_names\n    ]\n\n    X = X.select(log_return_columns)\n\n    return X\n</code></pre>"},{"location":"api-reference/feature_transformers/#centimators.feature_transformers.time_series.LogReturnTransformer.get_feature_names_out","title":"<code>get_feature_names_out(input_features=None)</code>","text":"<p>Returns the output feature names.</p> <p>Parameters:</p> Name Type Description Default <code>input_features</code> <code>list[str]</code> <p>Ignored. Kept for compatibility.</p> <code>None</code> <p>Returns:</p> Type Description <code>list[str]</code> <p>list[str]: List of transformed feature names.</p> Source code in <code>src/centimators/feature_transformers/time_series.py</code> <pre><code>def get_feature_names_out(self, input_features=None) -&gt; list[str]:\n    \"\"\"Returns the output feature names.\n\n    Args:\n        input_features (list[str], optional): Ignored. Kept for compatibility.\n\n    Returns:\n        list[str]: List of transformed feature names.\n    \"\"\"\n    return [f\"{feature_name}_logreturn\" for feature_name in self.feature_names]\n</code></pre>"},{"location":"api-reference/feature_transformers/#centimators.feature_transformers.stats","title":"<code>centimators.feature_transformers.stats</code>","text":"<p>Statistical transformers for horizontal aggregations.</p>"},{"location":"api-reference/feature_transformers/#centimators.feature_transformers.stats.GroupStatsTransformer","title":"<code>GroupStatsTransformer</code>","text":"<p>               Bases: <code>_BaseFeatureTransformer</code></p> <p>GroupStatsTransformer calculates statistical measures for defined feature groups.</p> <p>This transformer computes mean, standard deviation, and skewness for each group of features specified in the feature_group_mapping.</p> <p>Parameters:</p> Name Type Description Default <code>feature_group_mapping</code> <code>dict</code> <p>Dictionary mapping group names to lists of feature columns. Example: {'group1': ['feature1', 'feature2'], 'group2': ['feature3', 'feature4']}</p> required <code>stats</code> <code>list of str</code> <p>List of statistics to compute for each group. If None, all statistics are computed. Valid options are 'mean', 'std', 'skew', 'kurt', 'range', and 'cv'.</p> <code>['mean', 'std', 'skew', 'kurt', 'range', 'cv']</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; from centimators.feature_transformers import GroupStatsTransformer\n&gt;&gt;&gt; df = pd.DataFrame({\n...     'feature1': [1, 2, 3],\n...     'feature2': [4, 5, 6],\n...     'feature3': [7, 8, 9],\n...     'feature4': [10, 11, 12]\n... })\n&gt;&gt;&gt; mapping = {'group1': ['feature1', 'feature2'], 'group2': ['feature3', 'feature4']}\n&gt;&gt;&gt; transformer = GroupStatsTransformer(feature_group_mapping=mapping)\n&gt;&gt;&gt; result = transformer.fit_transform(df)\n&gt;&gt;&gt; print(result)\n   group1_groupstats_mean  group1_groupstats_std  group1_groupstats_skew  group2_groupstats_mean  group2_groupstats_std  group2_groupstats_skew\n0                  2.5                 1.5                  0.0                  8.5                 1.5                  0.0\n1                  3.5                 1.5                  0.0                  9.5                 1.5                  0.0\n2                  4.5                 1.5                  0.0                 10.5                 1.5                  0.0\n&gt;&gt;&gt; transformer_mean_only = GroupStatsTransformer(feature_group_mapping=mapping, stats=['mean'])\n&gt;&gt;&gt; result_mean_only = transformer_mean_only.fit_transform(df)\n&gt;&gt;&gt; print(result_mean_only)\n   group1_groupstats_mean  group2_groupstats_mean\n0                  2.5                  8.5\n1                  3.5                  9.5\n2                  4.5                 10.5\n</code></pre> Source code in <code>src/centimators/feature_transformers/stats.py</code> <pre><code>class GroupStatsTransformer(_BaseFeatureTransformer):\n    \"\"\"\n    GroupStatsTransformer calculates statistical measures for defined feature groups.\n\n    This transformer computes mean, standard deviation, and skewness for each\n    group of features specified in the feature_group_mapping.\n\n    Args:\n        feature_group_mapping (dict): Dictionary mapping group names to lists of\n            feature columns. Example: {'group1': ['feature1', 'feature2'],\n            'group2': ['feature3', 'feature4']}\n        stats (list of str, optional): List of statistics to compute for each group.\n            If None, all statistics are computed. Valid options are 'mean', 'std',\n            'skew', 'kurt', 'range', and 'cv'.\n\n    Examples:\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; from centimators.feature_transformers import GroupStatsTransformer\n        &gt;&gt;&gt; df = pd.DataFrame({\n        ...     'feature1': [1, 2, 3],\n        ...     'feature2': [4, 5, 6],\n        ...     'feature3': [7, 8, 9],\n        ...     'feature4': [10, 11, 12]\n        ... })\n        &gt;&gt;&gt; mapping = {'group1': ['feature1', 'feature2'], 'group2': ['feature3', 'feature4']}\n        &gt;&gt;&gt; transformer = GroupStatsTransformer(feature_group_mapping=mapping)\n        &gt;&gt;&gt; result = transformer.fit_transform(df)\n        &gt;&gt;&gt; print(result)\n           group1_groupstats_mean  group1_groupstats_std  group1_groupstats_skew  group2_groupstats_mean  group2_groupstats_std  group2_groupstats_skew\n        0                  2.5                 1.5                  0.0                  8.5                 1.5                  0.0\n        1                  3.5                 1.5                  0.0                  9.5                 1.5                  0.0\n        2                  4.5                 1.5                  0.0                 10.5                 1.5                  0.0\n        &gt;&gt;&gt; transformer_mean_only = GroupStatsTransformer(feature_group_mapping=mapping, stats=['mean'])\n        &gt;&gt;&gt; result_mean_only = transformer_mean_only.fit_transform(df)\n        &gt;&gt;&gt; print(result_mean_only)\n           group1_groupstats_mean  group2_groupstats_mean\n        0                  2.5                  8.5\n        1                  3.5                  9.5\n        2                  4.5                 10.5\n    \"\"\"\n\n    def __init__(\n        self,\n        feature_group_mapping: dict,\n        stats: list[str] = [\"mean\", \"std\", \"skew\", \"kurt\", \"range\", \"cv\"],\n    ):\n        super().__init__(feature_names=None)\n        self.feature_group_mapping = feature_group_mapping\n        self.groups = list(feature_group_mapping.keys())\n        # Supported statistics\n        valid_stats = [\"mean\", \"std\", \"skew\", \"kurt\", \"range\", \"cv\"]\n        if not all(stat in valid_stats for stat in stats):\n            raise ValueError(\n                f\"stats must be a list containing only {valid_stats}. Got {stats}\"\n            )\n        self.stats = stats\n\n    @nw.narwhalify(allow_series=True)\n    def transform(self, X: FrameT, y=None) -&gt; FrameT:\n        \"\"\"Calculates group statistics on the features.\n\n        Args:\n            X (FrameT): Input data frame.\n            y (Any, optional): Ignored. Kept for compatibility.\n\n        Returns:\n            FrameT: Transformed data frame with group statistics features.\n        \"\"\"\n        _expr_factories: dict[str, Callable[[list[str]], nw.Expr]] = {\n            \"mean\": lambda cols: nw.mean_horizontal(*cols),\n            \"std\": lambda cols: std_horizontal(*cols, ddof=1),\n            \"skew\": lambda cols: skew_horizontal(*cols),\n            \"kurt\": lambda cols: kurtosis_horizontal(*cols),\n            \"range\": lambda cols: range_horizontal(*cols),\n            \"cv\": lambda cols: coefficient_of_variation_horizontal(*cols),\n        }\n\n        _min_required_cols: dict[str, int] = {\n            \"mean\": 1,\n            \"range\": 1,\n            \"std\": 2,  # ddof=1 \u21d2 need at least 2 values for a finite result\n            \"cv\": 2,  # depends on std\n            \"skew\": 3,  # bias-corrected formula needs \u22653\n            \"kurt\": 4,  # bias-corrected formula needs \u22654\n        }\n\n        stat_expressions: list[nw.Expr] = []\n\n        for group, cols in self.feature_group_mapping.items():\n            if not cols:\n                raise ValueError(\n                    f\"No valid columns found for group '{group}' in the input frame.\"\n                )\n\n            n_cols = len(cols)\n\n            for stat in self.stats:\n                # Warn early if result is guaranteed to be NaN\n                min_required = _min_required_cols[stat]\n                if n_cols &lt; min_required:\n                    warnings.warn(\n                        (\n                            f\"{self.__class__.__name__}: statistic '{stat}' for group \"\n                            f\"'{group}' requires at least {min_required} feature column(s) \"\n                            f\"but only {n_cols} provided \u2013 the resulting column will be NaN.\"\n                        ),\n                        RuntimeWarning,\n                        stacklevel=2,\n                    )\n\n                expr = _expr_factories[stat](cols).alias(f\"{group}_groupstats_{stat}\")\n                stat_expressions.append(expr)\n\n        return X.select(stat_expressions)\n\n    def get_feature_names_out(self, input_features=None) -&gt; list[str]:\n        \"\"\"Return feature names for all groups.\n\n        Args:\n            input_features (list[str], optional): Ignored. Kept for compatibility.\n\n        Returns:\n            list[str]: List of transformed feature names.\n        \"\"\"\n        return [\n            f\"{group}_groupstats_{stat}\" for group in self.groups for stat in self.stats\n        ]\n</code></pre>"},{"location":"api-reference/feature_transformers/#centimators.feature_transformers.stats.GroupStatsTransformer.transform","title":"<code>transform(X, y=None)</code>","text":"<p>Calculates group statistics on the features.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>FrameT</code> <p>Input data frame.</p> required <code>y</code> <code>Any</code> <p>Ignored. Kept for compatibility.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>FrameT</code> <code>FrameT</code> <p>Transformed data frame with group statistics features.</p> Source code in <code>src/centimators/feature_transformers/stats.py</code> <pre><code>@nw.narwhalify(allow_series=True)\ndef transform(self, X: FrameT, y=None) -&gt; FrameT:\n    \"\"\"Calculates group statistics on the features.\n\n    Args:\n        X (FrameT): Input data frame.\n        y (Any, optional): Ignored. Kept for compatibility.\n\n    Returns:\n        FrameT: Transformed data frame with group statistics features.\n    \"\"\"\n    _expr_factories: dict[str, Callable[[list[str]], nw.Expr]] = {\n        \"mean\": lambda cols: nw.mean_horizontal(*cols),\n        \"std\": lambda cols: std_horizontal(*cols, ddof=1),\n        \"skew\": lambda cols: skew_horizontal(*cols),\n        \"kurt\": lambda cols: kurtosis_horizontal(*cols),\n        \"range\": lambda cols: range_horizontal(*cols),\n        \"cv\": lambda cols: coefficient_of_variation_horizontal(*cols),\n    }\n\n    _min_required_cols: dict[str, int] = {\n        \"mean\": 1,\n        \"range\": 1,\n        \"std\": 2,  # ddof=1 \u21d2 need at least 2 values for a finite result\n        \"cv\": 2,  # depends on std\n        \"skew\": 3,  # bias-corrected formula needs \u22653\n        \"kurt\": 4,  # bias-corrected formula needs \u22654\n    }\n\n    stat_expressions: list[nw.Expr] = []\n\n    for group, cols in self.feature_group_mapping.items():\n        if not cols:\n            raise ValueError(\n                f\"No valid columns found for group '{group}' in the input frame.\"\n            )\n\n        n_cols = len(cols)\n\n        for stat in self.stats:\n            # Warn early if result is guaranteed to be NaN\n            min_required = _min_required_cols[stat]\n            if n_cols &lt; min_required:\n                warnings.warn(\n                    (\n                        f\"{self.__class__.__name__}: statistic '{stat}' for group \"\n                        f\"'{group}' requires at least {min_required} feature column(s) \"\n                        f\"but only {n_cols} provided \u2013 the resulting column will be NaN.\"\n                    ),\n                    RuntimeWarning,\n                    stacklevel=2,\n                )\n\n            expr = _expr_factories[stat](cols).alias(f\"{group}_groupstats_{stat}\")\n            stat_expressions.append(expr)\n\n    return X.select(stat_expressions)\n</code></pre>"},{"location":"api-reference/feature_transformers/#centimators.feature_transformers.stats.GroupStatsTransformer.get_feature_names_out","title":"<code>get_feature_names_out(input_features=None)</code>","text":"<p>Return feature names for all groups.</p> <p>Parameters:</p> Name Type Description Default <code>input_features</code> <code>list[str]</code> <p>Ignored. Kept for compatibility.</p> <code>None</code> <p>Returns:</p> Type Description <code>list[str]</code> <p>list[str]: List of transformed feature names.</p> Source code in <code>src/centimators/feature_transformers/stats.py</code> <pre><code>def get_feature_names_out(self, input_features=None) -&gt; list[str]:\n    \"\"\"Return feature names for all groups.\n\n    Args:\n        input_features (list[str], optional): Ignored. Kept for compatibility.\n\n    Returns:\n        list[str]: List of transformed feature names.\n    \"\"\"\n    return [\n        f\"{group}_groupstats_{stat}\" for group in self.groups for stat in self.stats\n    ]\n</code></pre>"},{"location":"api-reference/feature_transformers/#centimators.feature_transformers.neutralization","title":"<code>centimators.feature_transformers.neutralization</code>","text":"<p>Neutralization transformers for reducing feature exposure.</p>"},{"location":"api-reference/feature_transformers/#centimators.feature_transformers.neutralization.FeatureNeutralizer","title":"<code>FeatureNeutralizer</code>","text":"<p>               Bases: <code>_BaseFeatureTransformer</code></p> <p>Classic feature neutralization by subtracting a linear model to reduce feature exposure.</p> <p>This transformer neutralizes predictions by removing their linear relationship with specified features. For each era, it: 1. Gaussianizes the predictions (rank -&gt; normalize -&gt; inverse CDF) 2. Fits a linear model: prediction ~ features 3. Subtracts proportion * exposure from predictions 4. Re-normalizes and scales to [0, 1]</p> <p>Parameters:</p> Name Type Description Default <code>proportion</code> <code>float or list of float</code> <p>How much to neutralize in range [0, 1]. 0 = no neutralization, 1 = full neutralization. If list, creates multiple output columns (one per proportion).</p> <code>0.5</code> <code>pred_name</code> <code>str or list of str</code> <p>Name(s) of prediction column(s) to neutralize. Used for generating output column names.</p> <code>'prediction'</code> <code>feature_names</code> <code>list of str</code> <p>Names of feature columns to neutralize against. If None, all columns of X are used.</p> <code>None</code> <code>suffix</code> <code>str</code> <p>Suffix to append to output column names.</p> <code>None</code> <code>n_jobs</code> <code>int</code> <p>Number of parallel jobs. 1 = sequential (default), -1 = all cores.</p> <code>1</code> <code>verbose</code> <code>bool</code> <p>Show progress bar over eras. Default False.</p> <code>False</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; from centimators.feature_transformers import FeatureNeutralizer\n&gt;&gt;&gt; # Sample data with eras, features, and predictions\n&gt;&gt;&gt; df = pd.DataFrame({\n...     'era': ['era1', 'era1', 'era1', 'era2', 'era2', 'era2'],\n...     'feature1': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6],\n...     'feature2': [0.6, 0.5, 0.4, 0.3, 0.2, 0.1],\n...     'prediction': [0.7, 0.8, 0.9, 0.6, 0.7, 0.8]\n... })\n&gt;&gt;&gt; neutralizer = FeatureNeutralizer(\n...     proportion=0.5,\n...     pred_name='prediction',\n...     feature_names=['feature1', 'feature2']\n... )\n&gt;&gt;&gt; # Predictions to neutralize (can be separate from features)\n&gt;&gt;&gt; result = neutralizer.fit_transform(\n...     df[['prediction']],\n...     features=df[['feature1', 'feature2']],\n...     era_series=df['era']\n... )\n</code></pre> Source code in <code>src/centimators/feature_transformers/neutralization.py</code> <pre><code>class FeatureNeutralizer(_BaseFeatureTransformer):\n    \"\"\"\n    Classic feature neutralization by subtracting a linear model to reduce feature exposure.\n\n    This transformer neutralizes predictions by removing their linear relationship with specified\n    features. For each era, it:\n    1. Gaussianizes the predictions (rank -&gt; normalize -&gt; inverse CDF)\n    2. Fits a linear model: prediction ~ features\n    3. Subtracts proportion * exposure from predictions\n    4. Re-normalizes and scales to [0, 1]\n\n    Args:\n        proportion (float or list of float): How much to neutralize in range [0, 1].\n            0 = no neutralization, 1 = full neutralization.\n            If list, creates multiple output columns (one per proportion).\n        pred_name (str or list of str): Name(s) of prediction column(s) to neutralize.\n            Used for generating output column names.\n        feature_names (list of str, optional): Names of feature columns to neutralize against.\n            If None, all columns of X are used.\n        suffix (str, optional): Suffix to append to output column names.\n        n_jobs (int): Number of parallel jobs. 1 = sequential (default), -1 = all cores.\n        verbose (bool): Show progress bar over eras. Default False.\n\n    Examples:\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; from centimators.feature_transformers import FeatureNeutralizer\n        &gt;&gt;&gt; # Sample data with eras, features, and predictions\n        &gt;&gt;&gt; df = pd.DataFrame({\n        ...     'era': ['era1', 'era1', 'era1', 'era2', 'era2', 'era2'],\n        ...     'feature1': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6],\n        ...     'feature2': [0.6, 0.5, 0.4, 0.3, 0.2, 0.1],\n        ...     'prediction': [0.7, 0.8, 0.9, 0.6, 0.7, 0.8]\n        ... })\n        &gt;&gt;&gt; neutralizer = FeatureNeutralizer(\n        ...     proportion=0.5,\n        ...     pred_name='prediction',\n        ...     feature_names=['feature1', 'feature2']\n        ... )\n        &gt;&gt;&gt; # Predictions to neutralize (can be separate from features)\n        &gt;&gt;&gt; result = neutralizer.fit_transform(\n        ...     df[['prediction']],\n        ...     features=df[['feature1', 'feature2']],\n        ...     era_series=df['era']\n        ... )\n    \"\"\"\n\n    def __init__(\n        self,\n        proportion: float | list[float] = 0.5,\n        pred_name: str | list[str] = \"prediction\",\n        feature_names: list[str] | None = None,\n        suffix: str | None = None,\n        n_jobs: int = 1,\n        verbose: bool = False,\n    ):\n        # Normalize inputs to lists\n        self.pred_names = [pred_name] if isinstance(pred_name, str) else pred_name\n        self.proportions = [proportion] if isinstance(proportion, float) else proportion\n\n        # Validate\n        assert len(self.pred_names) == len(set(self.pred_names)), (\n            \"Duplicate pred_names found.\"\n        )\n        for prop in self.proportions:\n            assert 0.0 &lt;= prop &lt;= 1.0, f\"proportion should be in [0, 1]. Got {prop}.\"\n\n        self.suffix = suffix\n        self.n_jobs = n_jobs\n        self.verbose = verbose\n\n        # Generate output column names\n        self._output_names = [\n            (\n                f\"{pname}_neutralized_{prop}_{suffix}\"\n                if suffix\n                else f\"{pname}_neutralized_{prop}\"\n            )\n            for pname in self.pred_names\n            for prop in self.proportions\n        ]\n\n        # Initialize with feature_names for the features to neutralize against\n        super().__init__(feature_names)\n\n    @nw.narwhalify(allow_series=True)\n    def transform(\n        self,\n        X: FrameT,\n        y=None,\n        features: FrameT | None = None,\n        era_series: IntoSeries | None = None,\n    ) -&gt; FrameT:\n        \"\"\"Neutralizes predictions against features.\n\n        Args:\n            X: Input predictions to neutralize (shape: n_samples x n_predictions).\n            y: Ignored. Kept for sklearn compatibility.\n            features: DataFrame with features for neutralization.\n                If None, uses X as both predictions and features.\n            era_series: Series with era labels for grouping.\n                If None, treats all data as a single era.\n\n        Returns:\n            DataFrame with neutralized predictions, scaled to [0, 1].\n        \"\"\"\n        # If features not provided, use X as features\n        if features is None:\n            features = X\n\n        # Convert to numpy for numerical operations\n        predictions = _ensure_numpy(X)\n        feature_array = _ensure_numpy(features)\n\n        # Ensure predictions is 2D\n        if predictions.ndim == 1:\n            assert len(self.pred_names) == 1, (\n                \"predictions is 1D but multiple pred_names given\"\n            )\n            predictions = predictions.reshape(-1, 1)\n        else:\n            assert predictions.shape[1] == len(self.pred_names), (\n                f\"predictions has {predictions.shape[1]} cols but {len(self.pred_names)} pred_names\"\n            )\n\n        # Convert era_series to numpy\n        if era_series is not None:\n            eras = _ensure_numpy(era_series, allow_series=True)\n        else:\n            warnings.warn(\n                \"era_series not provided. Treating all data as a single era. \"\n                \"This is fine for live inference (1 era) but may be incorrect \"\n                \"for training data with multiple eras.\",\n                UserWarning,\n            )\n            eras = np.array([\"X\"] * len(predictions))\n\n        # Process each prediction column and proportion\n        if self.n_jobs == 1:\n            # Sequential\n            results = [\n                self._neutralize_by_era(\n                    predictions[:, pred_idx], feature_array, eras, prop, self.verbose\n                )\n                for pred_idx in range(len(self.pred_names))\n                for prop in self.proportions\n            ]\n        else:\n            # Parallel via joblib (disable verbose)\n            tasks = [\n                delayed(self._neutralize_by_era)(\n                    predictions[:, pred_idx], feature_array, eras, prop, False\n                )\n                for pred_idx in range(len(self.pred_names))\n                for prop in self.proportions\n            ]\n            results = Parallel(n_jobs=self.n_jobs)(tasks)\n\n        # Stack results and convert back to dataframe with native type\n        result_array = np.column_stack(results)\n\n        # Create dictionary for dataframe construction (works with both pandas and polars)\n        result_dict = {\n            col_name: result_array[:, i]\n            for i, col_name in enumerate(self._output_names)\n        }\n\n        # Get the native namespace to create the appropriate dataframe type\n        native_namespace = nw.get_native_namespace(X)\n        result_df = nw.from_native(\n            native_namespace.DataFrame(result_dict),\n            eager_only=True,\n        )\n\n        return result_df\n\n    def _neutralize_by_era(\n        self,\n        predictions: np.ndarray,\n        features: np.ndarray,\n        eras: np.ndarray,\n        proportion: float,\n        verbose: bool = False,\n    ) -&gt; np.ndarray:\n        \"\"\"Neutralize predictions era by era.\"\"\"\n        unique_eras = np.unique(eras)\n        neutralized = np.zeros_like(predictions)\n\n        era_iter = tqdm(unique_eras, desc=f\"prop={proportion}\", disable=not verbose)\n        for era in era_iter:\n            mask = eras == era\n            era_pred = predictions[mask]\n            era_features = features[mask]\n\n            # Gaussianize then neutralize\n            era_pred_norm = _gaussianize(era_pred)\n            era_pred_neut = self._neutralize(era_pred_norm, era_features, proportion)\n            neutralized[mask] = era_pred_neut\n\n        # Scale all neutralized predictions to [0, 1]\n        return _min_max_scale(neutralized)\n\n    @staticmethod\n    def _neutralize(\n        predictions: np.ndarray, features: np.ndarray, proportion: float\n    ) -&gt; np.ndarray:\n        \"\"\"Neutralize predictions by removing linear exposure to features.\n\n        Args:\n            predictions: Gaussianized predictions (1D)\n            features: Feature matrix (2D)\n            proportion: How much to neutralize [0, 1]\n\n        Returns:\n            Neutralized predictions, standardized to mean=0, std=1\n        \"\"\"\n        # Fit linear model: predictions = features @ coeffs\n        # Use lstsq to solve: features @ coeffs = predictions\n        coeffs, _, _, _ = np.linalg.lstsq(features, predictions, rcond=None)\n\n        # Compute exposure: features @ coeffs\n        exposure = features @ coeffs\n\n        # Subtract proportion of exposure\n        neutralized = predictions - proportion * exposure\n\n        # Standardize\n        return neutralized / np.std(neutralized)\n</code></pre>"},{"location":"api-reference/feature_transformers/#centimators.feature_transformers.neutralization.FeatureNeutralizer.transform","title":"<code>transform(X, y=None, features=None, era_series=None)</code>","text":"<p>Neutralizes predictions against features.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>FrameT</code> <p>Input predictions to neutralize (shape: n_samples x n_predictions).</p> required <code>y</code> <p>Ignored. Kept for sklearn compatibility.</p> <code>None</code> <code>features</code> <code>FrameT | None</code> <p>DataFrame with features for neutralization. If None, uses X as both predictions and features.</p> <code>None</code> <code>era_series</code> <code>IntoSeries | None</code> <p>Series with era labels for grouping. If None, treats all data as a single era.</p> <code>None</code> <p>Returns:</p> Type Description <code>FrameT</code> <p>DataFrame with neutralized predictions, scaled to [0, 1].</p> Source code in <code>src/centimators/feature_transformers/neutralization.py</code> <pre><code>@nw.narwhalify(allow_series=True)\ndef transform(\n    self,\n    X: FrameT,\n    y=None,\n    features: FrameT | None = None,\n    era_series: IntoSeries | None = None,\n) -&gt; FrameT:\n    \"\"\"Neutralizes predictions against features.\n\n    Args:\n        X: Input predictions to neutralize (shape: n_samples x n_predictions).\n        y: Ignored. Kept for sklearn compatibility.\n        features: DataFrame with features for neutralization.\n            If None, uses X as both predictions and features.\n        era_series: Series with era labels for grouping.\n            If None, treats all data as a single era.\n\n    Returns:\n        DataFrame with neutralized predictions, scaled to [0, 1].\n    \"\"\"\n    # If features not provided, use X as features\n    if features is None:\n        features = X\n\n    # Convert to numpy for numerical operations\n    predictions = _ensure_numpy(X)\n    feature_array = _ensure_numpy(features)\n\n    # Ensure predictions is 2D\n    if predictions.ndim == 1:\n        assert len(self.pred_names) == 1, (\n            \"predictions is 1D but multiple pred_names given\"\n        )\n        predictions = predictions.reshape(-1, 1)\n    else:\n        assert predictions.shape[1] == len(self.pred_names), (\n            f\"predictions has {predictions.shape[1]} cols but {len(self.pred_names)} pred_names\"\n        )\n\n    # Convert era_series to numpy\n    if era_series is not None:\n        eras = _ensure_numpy(era_series, allow_series=True)\n    else:\n        warnings.warn(\n            \"era_series not provided. Treating all data as a single era. \"\n            \"This is fine for live inference (1 era) but may be incorrect \"\n            \"for training data with multiple eras.\",\n            UserWarning,\n        )\n        eras = np.array([\"X\"] * len(predictions))\n\n    # Process each prediction column and proportion\n    if self.n_jobs == 1:\n        # Sequential\n        results = [\n            self._neutralize_by_era(\n                predictions[:, pred_idx], feature_array, eras, prop, self.verbose\n            )\n            for pred_idx in range(len(self.pred_names))\n            for prop in self.proportions\n        ]\n    else:\n        # Parallel via joblib (disable verbose)\n        tasks = [\n            delayed(self._neutralize_by_era)(\n                predictions[:, pred_idx], feature_array, eras, prop, False\n            )\n            for pred_idx in range(len(self.pred_names))\n            for prop in self.proportions\n        ]\n        results = Parallel(n_jobs=self.n_jobs)(tasks)\n\n    # Stack results and convert back to dataframe with native type\n    result_array = np.column_stack(results)\n\n    # Create dictionary for dataframe construction (works with both pandas and polars)\n    result_dict = {\n        col_name: result_array[:, i]\n        for i, col_name in enumerate(self._output_names)\n    }\n\n    # Get the native namespace to create the appropriate dataframe type\n    native_namespace = nw.get_native_namespace(X)\n    result_df = nw.from_native(\n        native_namespace.DataFrame(result_dict),\n        eager_only=True,\n    )\n\n    return result_df\n</code></pre>"},{"location":"api-reference/feature_transformers/#centimators.feature_transformers.embedding","title":"<code>centimators.feature_transformers.embedding</code>","text":"<p>Embedding transformers for text and categorical features using DSPy.</p>"},{"location":"api-reference/feature_transformers/#centimators.feature_transformers.embedding.EmbeddingTransformer","title":"<code>EmbeddingTransformer</code>","text":"<p>               Bases: <code>_BaseFeatureTransformer</code></p> <p>EmbeddingTransformer embeds text and categorical features using DSPy's Embedder.</p> <p>This transformer converts text or categorical columns into dense vector embeddings using either hosted embedding models (e.g., OpenAI) or custom embedding functions (e.g., local SentenceTransformers). The embeddings are expanded into multiple columns for sklearn compatibility.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>str or Callable</code> <p>The embedding model to use. Can be: - A string for hosted models (e.g., \"openai/text-embedding-3-small\") - A callable function (e.g., SentenceTransformer.encode)</p> required <code>feature_names</code> <code>list[str] | None</code> <p>Names of columns to embed. If None, all columns are embedded.</p> <code>None</code> <code>categorical_mapping</code> <code>dict[str, str] | None</code> <p>Optional mapping from categorical column names to text templates. For example: {\"sector\": \"Company sector: {}\"} will format the sector value as \"Company sector: Technology\" before embedding.</p> <code>None</code> <code>batch_size</code> <code>int</code> <p>Batch size for embedding computation. Default: 200.</p> <code>200</code> <code>caching</code> <code>bool</code> <p>Whether to cache embeddings (for hosted models). Default: True.</p> <code>True</code> <code>**embedder_kwargs</code> <p>Additional keyword arguments passed to dspy.Embedder.</p> <code>{}</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import polars as pl\n&gt;&gt;&gt; from centimators.feature_transformers import EmbeddingTransformer\n&gt;&gt;&gt; from sentence_transformers import SentenceTransformer\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Example 1: Using a local model\n&gt;&gt;&gt; model = SentenceTransformer('all-MiniLM-L6-v2')\n&gt;&gt;&gt; df = pl.DataFrame({\n...     'text': ['AI company', 'Bank', 'Pharma firm'],\n...     'sector': ['Technology', 'Finance', 'Healthcare']\n... })\n&gt;&gt;&gt;\n&gt;&gt;&gt; transformer = EmbeddingTransformer(\n...     model=model.encode,\n...     feature_names=['text', 'sector'],\n...     categorical_mapping={'sector': 'Company sector: {}'}\n... )\n&gt;&gt;&gt; embedded = transformer.fit_transform(df[['text', 'sector']])\n&gt;&gt;&gt; print(embedded.columns)  # text_embed_0, text_embed_1, ..., sector_embed_0, ...\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Example 2: Using a hosted model\n&gt;&gt;&gt; transformer = EmbeddingTransformer(\n...     model=\"openai/text-embedding-3-small\",\n...     feature_names=['text']\n... )\n&gt;&gt;&gt; embedded = transformer.fit_transform(df[['text']])\n</code></pre> Notes <ul> <li>Null values are skipped and filled with zero vectors</li> <li>Embedding dimension is inferred from the first batch</li> <li>Output columns follow the pattern: <code>{feature_name}_embed_{dim_idx}</code></li> <li>Requires <code>centimators[dspy]</code> installation</li> </ul> Source code in <code>src/centimators/feature_transformers/embedding.py</code> <pre><code>class EmbeddingTransformer(_BaseFeatureTransformer):\n    \"\"\"\n    EmbeddingTransformer embeds text and categorical features using DSPy's Embedder.\n\n    This transformer converts text or categorical columns into dense vector embeddings\n    using either hosted embedding models (e.g., OpenAI) or custom embedding functions\n    (e.g., local SentenceTransformers). The embeddings are expanded into multiple\n    columns for sklearn compatibility.\n\n    Args:\n        model (str or Callable): The embedding model to use. Can be:\n            - A string for hosted models (e.g., \"openai/text-embedding-3-small\")\n            - A callable function (e.g., SentenceTransformer.encode)\n        feature_names (list[str] | None): Names of columns to embed. If None,\n            all columns are embedded.\n        categorical_mapping (dict[str, str] | None): Optional mapping from categorical\n            column names to text templates. For example:\n            {\"sector\": \"Company sector: {}\"} will format the sector value as\n            \"Company sector: Technology\" before embedding.\n        batch_size (int): Batch size for embedding computation. Default: 200.\n        caching (bool): Whether to cache embeddings (for hosted models). Default: True.\n        **embedder_kwargs: Additional keyword arguments passed to dspy.Embedder.\n\n    Examples:\n        &gt;&gt;&gt; import polars as pl\n        &gt;&gt;&gt; from centimators.feature_transformers import EmbeddingTransformer\n        &gt;&gt;&gt; from sentence_transformers import SentenceTransformer\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Example 1: Using a local model\n        &gt;&gt;&gt; model = SentenceTransformer('all-MiniLM-L6-v2')\n        &gt;&gt;&gt; df = pl.DataFrame({\n        ...     'text': ['AI company', 'Bank', 'Pharma firm'],\n        ...     'sector': ['Technology', 'Finance', 'Healthcare']\n        ... })\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; transformer = EmbeddingTransformer(\n        ...     model=model.encode,\n        ...     feature_names=['text', 'sector'],\n        ...     categorical_mapping={'sector': 'Company sector: {}'}\n        ... )\n        &gt;&gt;&gt; embedded = transformer.fit_transform(df[['text', 'sector']])\n        &gt;&gt;&gt; print(embedded.columns)  # text_embed_0, text_embed_1, ..., sector_embed_0, ...\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Example 2: Using a hosted model\n        &gt;&gt;&gt; transformer = EmbeddingTransformer(\n        ...     model=\"openai/text-embedding-3-small\",\n        ...     feature_names=['text']\n        ... )\n        &gt;&gt;&gt; embedded = transformer.fit_transform(df[['text']])\n\n    Notes:\n        - Null values are skipped and filled with zero vectors\n        - Embedding dimension is inferred from the first batch\n        - Output columns follow the pattern: `{feature_name}_embed_{dim_idx}`\n        - Requires `centimators[dspy]` installation\n    \"\"\"\n\n    def __init__(\n        self,\n        model,\n        feature_names: list[str] | None = None,\n        categorical_mapping: dict[str, str] | None = None,\n        batch_size: int = 200,\n        caching: bool = True,\n        **embedder_kwargs,\n    ):\n        super().__init__(feature_names=feature_names)\n        self.model = model\n        self.categorical_mapping = categorical_mapping or {}\n        self.batch_size = batch_size\n        self.caching = caching\n        self.embedder_kwargs = embedder_kwargs\n        self._embedder = None\n        self._embedding_dims = {}  # Track dimension per feature\n\n    def fit(self, X: FrameT, y=None):\n        \"\"\"Fit the transformer and initialize the embedder.\n\n        Args:\n            X (FrameT): Input data frame.\n            y: Ignored. Kept for compatibility.\n\n        Returns:\n            EmbeddingTransformer: The fitted transformer.\n        \"\"\"\n        super().fit(X, y)\n\n        # Initialize DSPy embedder\n        self._embedder = dspy.Embedder(\n            model=self.model,\n            batch_size=self.batch_size,\n            caching=self.caching,\n            **self.embedder_kwargs,\n        )\n\n        return self\n\n    @nw.narwhalify(allow_series=True)\n    def transform(self, X: FrameT, y=None) -&gt; FrameT:\n        \"\"\"Transform features by embedding them into dense vectors.\n\n        Args:\n            X (FrameT): Input data frame.\n            y: Ignored. Kept for compatibility.\n\n        Returns:\n            FrameT: Transformed data frame with embedding columns expanded.\n                Each input feature becomes multiple columns:\n                {feature_name}_embed_0, {feature_name}_embed_1, etc.\n        \"\"\"\n        if self._embedder is None:\n            raise ValueError(\"Transformer not fitted. Call fit() first.\")\n\n        all_embedding_cols = []\n\n        for feature_name in self.feature_names:\n            # Extract column values\n            col_values = X.select(nw.col(feature_name)).to_native()\n\n            # Convert to list of strings\n            if hasattr(col_values, \"to_list\"):\n                values_list = col_values[feature_name].to_list()\n            elif hasattr(col_values, \"tolist\"):\n                values_list = col_values[feature_name].tolist()\n            else:\n                values_list = list(col_values[feature_name])\n\n            # Apply categorical mapping if specified\n            if feature_name in self.categorical_mapping:\n                template = self.categorical_mapping[feature_name]\n                values_list = [\n                    template.format(val) if val is not None else None\n                    for val in values_list\n                ]\n            else:\n                # Convert to string\n                values_list = [\n                    str(val) if val is not None else None for val in values_list\n                ]\n\n            # Separate null and non-null indices\n            non_null_indices = [\n                i for i, val in enumerate(values_list) if val is not None\n            ]\n            non_null_values = [values_list[i] for i in non_null_indices]\n\n            # Compute embeddings for non-null values\n            if non_null_values:\n                embeddings = self._embedder(non_null_values)\n                embedding_dim = embeddings.shape[1]\n\n                # Store dimension for this feature\n                self._embedding_dims[feature_name] = embedding_dim\n\n                # Create full embedding matrix with zeros for nulls\n                full_embeddings = np.zeros(\n                    (len(values_list), embedding_dim), dtype=np.float32\n                )\n                full_embeddings[non_null_indices] = embeddings\n            else:\n                # All nulls - can't infer dimension\n                if feature_name not in self._embedding_dims:\n                    raise ValueError(\n                        f\"Cannot determine embedding dimension for '{feature_name}' - \"\n                        f\"all values are null. Ensure at least one non-null value exists.\"\n                    )\n                embedding_dim = self._embedding_dims[feature_name]\n                full_embeddings = np.zeros(\n                    (len(values_list), embedding_dim), dtype=np.float32\n                )\n\n            # Store embeddings for this feature\n            for dim_idx in range(embedding_dim):\n                col_name = f\"{feature_name}_embed_{dim_idx}\"\n                all_embedding_cols.append(\n                    (col_name, full_embeddings[:, dim_idx].tolist())\n                )\n\n        # Build a df from all embedding columns and return as the same backend as X\n        if all_embedding_cols:\n            columns_dict = {col_name: values for col_name, values in all_embedding_cols}\n            return nw.from_dict(columns_dict, backend=nw.get_native_namespace(X))\n        else:\n            # Return empty frame with correct number of rows\n            return nw.from_dict(\n                {\"_empty\": [None] * len(X)}, backend=nw.get_native_namespace(X)\n            )\n\n    def get_feature_names_out(self, input_features=None) -&gt; list[str]:\n        \"\"\"Return the output feature names.\n\n        Args:\n            input_features (list[str], optional): Ignored. Kept for compatibility.\n\n        Returns:\n            list[str]: List of transformed feature names in the format\n                {feature_name}_embed_{dim_idx}.\n\n        Raises:\n            ValueError: If called before transform() when dimensions are unknown.\n        \"\"\"\n        output_names = []\n        for feature_name in self.feature_names:\n            if feature_name not in self._embedding_dims:\n                raise ValueError(\n                    f\"Cannot determine output feature names for '{feature_name}' - \"\n                    f\"call transform() first to infer embedding dimensions.\"\n                )\n            embedding_dim = self._embedding_dims[feature_name]\n            for dim_idx in range(embedding_dim):\n                output_names.append(f\"{feature_name}_embed_{dim_idx}\")\n        return output_names\n</code></pre>"},{"location":"api-reference/feature_transformers/#centimators.feature_transformers.embedding.EmbeddingTransformer.fit","title":"<code>fit(X, y=None)</code>","text":"<p>Fit the transformer and initialize the embedder.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>FrameT</code> <p>Input data frame.</p> required <code>y</code> <p>Ignored. Kept for compatibility.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>EmbeddingTransformer</code> <p>The fitted transformer.</p> Source code in <code>src/centimators/feature_transformers/embedding.py</code> <pre><code>def fit(self, X: FrameT, y=None):\n    \"\"\"Fit the transformer and initialize the embedder.\n\n    Args:\n        X (FrameT): Input data frame.\n        y: Ignored. Kept for compatibility.\n\n    Returns:\n        EmbeddingTransformer: The fitted transformer.\n    \"\"\"\n    super().fit(X, y)\n\n    # Initialize DSPy embedder\n    self._embedder = dspy.Embedder(\n        model=self.model,\n        batch_size=self.batch_size,\n        caching=self.caching,\n        **self.embedder_kwargs,\n    )\n\n    return self\n</code></pre>"},{"location":"api-reference/feature_transformers/#centimators.feature_transformers.embedding.EmbeddingTransformer.transform","title":"<code>transform(X, y=None)</code>","text":"<p>Transform features by embedding them into dense vectors.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>FrameT</code> <p>Input data frame.</p> required <code>y</code> <p>Ignored. Kept for compatibility.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>FrameT</code> <code>FrameT</code> <p>Transformed data frame with embedding columns expanded. Each input feature becomes multiple columns: {feature_name}_embed_0, {feature_name}_embed_1, etc.</p> Source code in <code>src/centimators/feature_transformers/embedding.py</code> <pre><code>@nw.narwhalify(allow_series=True)\ndef transform(self, X: FrameT, y=None) -&gt; FrameT:\n    \"\"\"Transform features by embedding them into dense vectors.\n\n    Args:\n        X (FrameT): Input data frame.\n        y: Ignored. Kept for compatibility.\n\n    Returns:\n        FrameT: Transformed data frame with embedding columns expanded.\n            Each input feature becomes multiple columns:\n            {feature_name}_embed_0, {feature_name}_embed_1, etc.\n    \"\"\"\n    if self._embedder is None:\n        raise ValueError(\"Transformer not fitted. Call fit() first.\")\n\n    all_embedding_cols = []\n\n    for feature_name in self.feature_names:\n        # Extract column values\n        col_values = X.select(nw.col(feature_name)).to_native()\n\n        # Convert to list of strings\n        if hasattr(col_values, \"to_list\"):\n            values_list = col_values[feature_name].to_list()\n        elif hasattr(col_values, \"tolist\"):\n            values_list = col_values[feature_name].tolist()\n        else:\n            values_list = list(col_values[feature_name])\n\n        # Apply categorical mapping if specified\n        if feature_name in self.categorical_mapping:\n            template = self.categorical_mapping[feature_name]\n            values_list = [\n                template.format(val) if val is not None else None\n                for val in values_list\n            ]\n        else:\n            # Convert to string\n            values_list = [\n                str(val) if val is not None else None for val in values_list\n            ]\n\n        # Separate null and non-null indices\n        non_null_indices = [\n            i for i, val in enumerate(values_list) if val is not None\n        ]\n        non_null_values = [values_list[i] for i in non_null_indices]\n\n        # Compute embeddings for non-null values\n        if non_null_values:\n            embeddings = self._embedder(non_null_values)\n            embedding_dim = embeddings.shape[1]\n\n            # Store dimension for this feature\n            self._embedding_dims[feature_name] = embedding_dim\n\n            # Create full embedding matrix with zeros for nulls\n            full_embeddings = np.zeros(\n                (len(values_list), embedding_dim), dtype=np.float32\n            )\n            full_embeddings[non_null_indices] = embeddings\n        else:\n            # All nulls - can't infer dimension\n            if feature_name not in self._embedding_dims:\n                raise ValueError(\n                    f\"Cannot determine embedding dimension for '{feature_name}' - \"\n                    f\"all values are null. Ensure at least one non-null value exists.\"\n                )\n            embedding_dim = self._embedding_dims[feature_name]\n            full_embeddings = np.zeros(\n                (len(values_list), embedding_dim), dtype=np.float32\n            )\n\n        # Store embeddings for this feature\n        for dim_idx in range(embedding_dim):\n            col_name = f\"{feature_name}_embed_{dim_idx}\"\n            all_embedding_cols.append(\n                (col_name, full_embeddings[:, dim_idx].tolist())\n            )\n\n    # Build a df from all embedding columns and return as the same backend as X\n    if all_embedding_cols:\n        columns_dict = {col_name: values for col_name, values in all_embedding_cols}\n        return nw.from_dict(columns_dict, backend=nw.get_native_namespace(X))\n    else:\n        # Return empty frame with correct number of rows\n        return nw.from_dict(\n            {\"_empty\": [None] * len(X)}, backend=nw.get_native_namespace(X)\n        )\n</code></pre>"},{"location":"api-reference/feature_transformers/#centimators.feature_transformers.embedding.EmbeddingTransformer.get_feature_names_out","title":"<code>get_feature_names_out(input_features=None)</code>","text":"<p>Return the output feature names.</p> <p>Parameters:</p> Name Type Description Default <code>input_features</code> <code>list[str]</code> <p>Ignored. Kept for compatibility.</p> <code>None</code> <p>Returns:</p> Type Description <code>list[str]</code> <p>list[str]: List of transformed feature names in the format {feature_name}embed.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If called before transform() when dimensions are unknown.</p> Source code in <code>src/centimators/feature_transformers/embedding.py</code> <pre><code>def get_feature_names_out(self, input_features=None) -&gt; list[str]:\n    \"\"\"Return the output feature names.\n\n    Args:\n        input_features (list[str], optional): Ignored. Kept for compatibility.\n\n    Returns:\n        list[str]: List of transformed feature names in the format\n            {feature_name}_embed_{dim_idx}.\n\n    Raises:\n        ValueError: If called before transform() when dimensions are unknown.\n    \"\"\"\n    output_names = []\n    for feature_name in self.feature_names:\n        if feature_name not in self._embedding_dims:\n            raise ValueError(\n                f\"Cannot determine output feature names for '{feature_name}' - \"\n                f\"call transform() first to infer embedding dimensions.\"\n            )\n        embedding_dim = self._embedding_dims[feature_name]\n        for dim_idx in range(embedding_dim):\n            output_names.append(f\"{feature_name}_embed_{dim_idx}\")\n    return output_names\n</code></pre>"},{"location":"api-reference/feature_transformers/#centimators.feature_transformers.dimreduction","title":"<code>centimators.feature_transformers.dimreduction</code>","text":"<p>Dimensionality reduction transformers for feature compression.</p>"},{"location":"api-reference/feature_transformers/#centimators.feature_transformers.dimreduction.DimReducer","title":"<code>DimReducer</code>","text":"<p>               Bases: <code>_BaseFeatureTransformer</code></p> <p>DimReducer applies dimensionality reduction to features using PCA, t-SNE, or UMAP.</p> <p>This transformer reduces the dimensionality of input features by projecting them into a lower-dimensional space using one of three methods: Principal Component Analysis (PCA), t-distributed Stochastic Neighbor Embedding (t-SNE), or Uniform Manifold Approximation and Projection (UMAP).</p> <p>Parameters:</p> Name Type Description Default <code>method</code> <code>str</code> <p>The dimensionality reduction method to use. Options are: - 'pca': Principal Component Analysis (linear, preserves global structure) - 'tsne': t-SNE (non-linear, preserves local structure, visualization) - 'umap': UMAP (non-linear, preserves local + global structure) Default: 'pca'</p> <code>'pca'</code> <code>n_components</code> <code>int</code> <p>Number of dimensions in the reduced space. Default: 2</p> <code>2</code> <code>feature_names</code> <code>list[str] | None</code> <p>Names of columns to reduce. If None, all columns are used.</p> <code>None</code> <code>**reducer_kwargs</code> <p>Additional keyword arguments passed to the underlying reducer (sklearn.decomposition.PCA, sklearn.manifold.TSNE, or umap.UMAP).</p> <code>{}</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import polars as pl\n&gt;&gt;&gt; from centimators.feature_transformers import DimReducer\n&gt;&gt;&gt; df = pl.DataFrame({\n...     'feature1': [1.0, 2.0, 3.0, 4.0],\n...     'feature2': [4.0, 5.0, 6.0, 7.0],\n...     'feature3': [7.0, 8.0, 9.0, 10.0],\n... })\n&gt;&gt;&gt;\n&gt;&gt;&gt; # PCA reduction\n&gt;&gt;&gt; reducer = DimReducer(method='pca', n_components=2)\n&gt;&gt;&gt; reduced = reducer.fit_transform(df)\n&gt;&gt;&gt; print(reduced.columns)  # ['dim_0', 'dim_1']\n&gt;&gt;&gt;\n&gt;&gt;&gt; # t-SNE for visualization\n&gt;&gt;&gt; reducer = DimReducer(method='tsne', n_components=2, random_state=42)\n&gt;&gt;&gt; reduced = reducer.fit_transform(df)\n&gt;&gt;&gt;\n&gt;&gt;&gt; # UMAP (requires umap-learn)\n&gt;&gt;&gt; reducer = DimReducer(method='umap', n_components=2, random_state=42)\n&gt;&gt;&gt; reduced = reducer.fit_transform(df)\n</code></pre> Notes <ul> <li>PCA is deterministic and fast, suitable for preprocessing</li> <li>t-SNE is stochastic and slower, primarily for visualization (does not support   separate transform - uses fit_transform internally)</li> <li>UMAP balances speed and quality, good for both preprocessing and visualization</li> <li>UMAP requires the umap-learn package: <code>uv add 'centimators[all]'</code></li> <li>All methods work with any narwhals-compatible backend (pandas, polars, etc.)</li> </ul> Source code in <code>src/centimators/feature_transformers/dimreduction.py</code> <pre><code>class DimReducer(_BaseFeatureTransformer):\n    \"\"\"\n    DimReducer applies dimensionality reduction to features using PCA, t-SNE, or UMAP.\n\n    This transformer reduces the dimensionality of input features by projecting them\n    into a lower-dimensional space using one of three methods: Principal Component\n    Analysis (PCA), t-distributed Stochastic Neighbor Embedding (t-SNE), or Uniform\n    Manifold Approximation and Projection (UMAP).\n\n    Args:\n        method (str): The dimensionality reduction method to use. Options are:\n            - 'pca': Principal Component Analysis (linear, preserves global structure)\n            - 'tsne': t-SNE (non-linear, preserves local structure, visualization)\n            - 'umap': UMAP (non-linear, preserves local + global structure)\n            Default: 'pca'\n        n_components (int): Number of dimensions in the reduced space. Default: 2\n        feature_names (list[str] | None): Names of columns to reduce. If None,\n            all columns are used.\n        **reducer_kwargs: Additional keyword arguments passed to the underlying\n            reducer (sklearn.decomposition.PCA, sklearn.manifold.TSNE, or umap.UMAP).\n\n    Examples:\n        &gt;&gt;&gt; import polars as pl\n        &gt;&gt;&gt; from centimators.feature_transformers import DimReducer\n        &gt;&gt;&gt; df = pl.DataFrame({\n        ...     'feature1': [1.0, 2.0, 3.0, 4.0],\n        ...     'feature2': [4.0, 5.0, 6.0, 7.0],\n        ...     'feature3': [7.0, 8.0, 9.0, 10.0],\n        ... })\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # PCA reduction\n        &gt;&gt;&gt; reducer = DimReducer(method='pca', n_components=2)\n        &gt;&gt;&gt; reduced = reducer.fit_transform(df)\n        &gt;&gt;&gt; print(reduced.columns)  # ['dim_0', 'dim_1']\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # t-SNE for visualization\n        &gt;&gt;&gt; reducer = DimReducer(method='tsne', n_components=2, random_state=42)\n        &gt;&gt;&gt; reduced = reducer.fit_transform(df)\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # UMAP (requires umap-learn)\n        &gt;&gt;&gt; reducer = DimReducer(method='umap', n_components=2, random_state=42)\n        &gt;&gt;&gt; reduced = reducer.fit_transform(df)\n\n    Notes:\n        - PCA is deterministic and fast, suitable for preprocessing\n        - t-SNE is stochastic and slower, primarily for visualization (does not support\n          separate transform - uses fit_transform internally)\n        - UMAP balances speed and quality, good for both preprocessing and visualization\n        - UMAP requires the umap-learn package: `uv add 'centimators[all]'`\n        - All methods work with any narwhals-compatible backend (pandas, polars, etc.)\n    \"\"\"\n\n    def __init__(\n        self,\n        method: str = \"pca\",\n        n_components: int = 2,\n        feature_names: list[str] | None = None,\n        **reducer_kwargs,\n    ):\n        super().__init__(feature_names=feature_names)\n\n        valid_methods = [\"pca\", \"tsne\", \"umap\"]\n        if method not in valid_methods:\n            raise ValueError(f\"method must be one of {valid_methods}, got '{method}'\")\n\n        self.method = method\n        self.n_components = n_components\n        self.reducer_kwargs = reducer_kwargs\n        self._reducer = None\n\n    def fit(self, X: FrameT, y=None):\n        \"\"\"Fit the dimensionality reduction model.\n\n        Args:\n            X (FrameT): Input data frame.\n            y: Ignored. Kept for compatibility.\n\n        Returns:\n            DimReducer: The fitted transformer.\n        \"\"\"\n        super().fit(X, y)\n\n        # Initialize the appropriate reducer\n        if self.method == \"pca\":\n            self._reducer = PCA(n_components=self.n_components, **self.reducer_kwargs)\n        elif self.method == \"tsne\":\n            self._reducer = TSNE(n_components=self.n_components, **self.reducer_kwargs)\n        elif self.method == \"umap\":\n            try:\n                import umap\n            except ImportError as e:\n                raise ImportError(\n                    \"DimReducer with method='umap' requires umap-learn. Install with:\\n\"\n                    \"  uv add 'centimators[all]'\\n\"\n                    \"or:\\n\"\n                    \"  pip install 'centimators[all]'\"\n                ) from e\n            self._reducer = umap.UMAP(\n                n_components=self.n_components, **self.reducer_kwargs\n            )\n\n        # Fit the reducer on the selected features\n        X_native = nw.from_native(X)\n        X_subset = X_native.select(self.feature_names)\n        X_numpy = X_subset.to_numpy()\n\n        # For t-SNE, we skip fit since it doesn't support separate fit/transform\n        if self.method != \"tsne\":\n            self._reducer.fit(X_numpy)\n\n        return self\n\n    @nw.narwhalify(allow_series=True)\n    def transform(self, X: FrameT, y=None) -&gt; FrameT:\n        \"\"\"Transform features by reducing their dimensionality.\n\n        Args:\n            X (FrameT): Input data frame.\n            y: Ignored. Kept for compatibility.\n\n        Returns:\n            FrameT: Transformed data frame with reduced dimensionality.\n                Columns are named 'dim_0', 'dim_1', ..., 'dim_{n_components-1}'.\n        \"\"\"\n        if self._reducer is None:\n            raise ValueError(\"Transformer not fitted. Call fit() first.\")\n\n        # Extract features and convert to numpy\n        X_subset = X.select(self.feature_names)\n        X_numpy = X_subset.to_numpy()\n\n        # Apply dimensionality reduction\n        # Note: t-SNE doesn't support transform(), so we use fit_transform\n        if self.method == \"tsne\":\n            X_reduced = self._reducer.fit_transform(X_numpy)\n        else:\n            X_reduced = self._reducer.transform(X_numpy)\n\n        # Create output column names\n        output_cols = {f\"dim_{i}\": X_reduced[:, i] for i in range(self.n_components)}\n\n        # Return as narwhals DataFrame with the same backend as input\n        return nw.from_dict(output_cols, backend=nw.get_native_namespace(X))\n\n    def get_feature_names_out(self, input_features=None) -&gt; list[str]:\n        \"\"\"Return the output feature names.\n\n        Args:\n            input_features (list[str], optional): Ignored. Kept for compatibility.\n\n        Returns:\n            list[str]: List of output feature names: ['dim_0', 'dim_1', ...].\n        \"\"\"\n        return [f\"dim_{i}\" for i in range(self.n_components)]\n</code></pre>"},{"location":"api-reference/feature_transformers/#centimators.feature_transformers.dimreduction.DimReducer.fit","title":"<code>fit(X, y=None)</code>","text":"<p>Fit the dimensionality reduction model.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>FrameT</code> <p>Input data frame.</p> required <code>y</code> <p>Ignored. Kept for compatibility.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>DimReducer</code> <p>The fitted transformer.</p> Source code in <code>src/centimators/feature_transformers/dimreduction.py</code> <pre><code>def fit(self, X: FrameT, y=None):\n    \"\"\"Fit the dimensionality reduction model.\n\n    Args:\n        X (FrameT): Input data frame.\n        y: Ignored. Kept for compatibility.\n\n    Returns:\n        DimReducer: The fitted transformer.\n    \"\"\"\n    super().fit(X, y)\n\n    # Initialize the appropriate reducer\n    if self.method == \"pca\":\n        self._reducer = PCA(n_components=self.n_components, **self.reducer_kwargs)\n    elif self.method == \"tsne\":\n        self._reducer = TSNE(n_components=self.n_components, **self.reducer_kwargs)\n    elif self.method == \"umap\":\n        try:\n            import umap\n        except ImportError as e:\n            raise ImportError(\n                \"DimReducer with method='umap' requires umap-learn. Install with:\\n\"\n                \"  uv add 'centimators[all]'\\n\"\n                \"or:\\n\"\n                \"  pip install 'centimators[all]'\"\n            ) from e\n        self._reducer = umap.UMAP(\n            n_components=self.n_components, **self.reducer_kwargs\n        )\n\n    # Fit the reducer on the selected features\n    X_native = nw.from_native(X)\n    X_subset = X_native.select(self.feature_names)\n    X_numpy = X_subset.to_numpy()\n\n    # For t-SNE, we skip fit since it doesn't support separate fit/transform\n    if self.method != \"tsne\":\n        self._reducer.fit(X_numpy)\n\n    return self\n</code></pre>"},{"location":"api-reference/feature_transformers/#centimators.feature_transformers.dimreduction.DimReducer.transform","title":"<code>transform(X, y=None)</code>","text":"<p>Transform features by reducing their dimensionality.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>FrameT</code> <p>Input data frame.</p> required <code>y</code> <p>Ignored. Kept for compatibility.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>FrameT</code> <code>FrameT</code> <p>Transformed data frame with reduced dimensionality. Columns are named 'dim_0', 'dim_1', ..., 'dim_{n_components-1}'.</p> Source code in <code>src/centimators/feature_transformers/dimreduction.py</code> <pre><code>@nw.narwhalify(allow_series=True)\ndef transform(self, X: FrameT, y=None) -&gt; FrameT:\n    \"\"\"Transform features by reducing their dimensionality.\n\n    Args:\n        X (FrameT): Input data frame.\n        y: Ignored. Kept for compatibility.\n\n    Returns:\n        FrameT: Transformed data frame with reduced dimensionality.\n            Columns are named 'dim_0', 'dim_1', ..., 'dim_{n_components-1}'.\n    \"\"\"\n    if self._reducer is None:\n        raise ValueError(\"Transformer not fitted. Call fit() first.\")\n\n    # Extract features and convert to numpy\n    X_subset = X.select(self.feature_names)\n    X_numpy = X_subset.to_numpy()\n\n    # Apply dimensionality reduction\n    # Note: t-SNE doesn't support transform(), so we use fit_transform\n    if self.method == \"tsne\":\n        X_reduced = self._reducer.fit_transform(X_numpy)\n    else:\n        X_reduced = self._reducer.transform(X_numpy)\n\n    # Create output column names\n    output_cols = {f\"dim_{i}\": X_reduced[:, i] for i in range(self.n_components)}\n\n    # Return as narwhals DataFrame with the same backend as input\n    return nw.from_dict(output_cols, backend=nw.get_native_namespace(X))\n</code></pre>"},{"location":"api-reference/feature_transformers/#centimators.feature_transformers.dimreduction.DimReducer.get_feature_names_out","title":"<code>get_feature_names_out(input_features=None)</code>","text":"<p>Return the output feature names.</p> <p>Parameters:</p> Name Type Description Default <code>input_features</code> <code>list[str]</code> <p>Ignored. Kept for compatibility.</p> <code>None</code> <p>Returns:</p> Type Description <code>list[str]</code> <p>list[str]: List of output feature names: ['dim_0', 'dim_1', ...].</p> Source code in <code>src/centimators/feature_transformers/dimreduction.py</code> <pre><code>def get_feature_names_out(self, input_features=None) -&gt; list[str]:\n    \"\"\"Return the output feature names.\n\n    Args:\n        input_features (list[str], optional): Ignored. Kept for compatibility.\n\n    Returns:\n        list[str]: List of output feature names: ['dim_0', 'dim_1', ...].\n    \"\"\"\n    return [f\"dim_{i}\" for i in range(self.n_components)]\n</code></pre>"},{"location":"api-reference/feature_transformers/#centimators.feature_transformers.penalization","title":"<code>centimators.feature_transformers.penalization</code>","text":"<p>Feature penalization transformers using iterative optimization (requires JAX).</p>"},{"location":"api-reference/feature_transformers/#centimators.feature_transformers.penalization.FeaturePenalizer","title":"<code>FeaturePenalizer</code>","text":"<p>               Bases: <code>_BaseFeatureTransformer</code></p> <p>Feature penalization using iterative optimization to cap feature exposure.</p> <p>Unlike FeatureNeutralizer which subtracts a fixed proportion of linear exposure, this transformer uses gradient descent to find the minimal adjustment that caps all feature exposures below a threshold. This preserves more of the original signal while ensuring no single feature dominates.</p> <p>For each era, it: 1. Gaussianizes the predictions (rank -&gt; normalize -&gt; inverse CDF) 2. Trains a linear model to subtract from predictions such that    |exposure to any feature| &lt;= max_exposure 3. Re-normalizes and scales to [0, 1]</p> <p>Parameters:</p> Name Type Description Default <code>max_exposure</code> <code>float or list of float</code> <p>Maximum allowed feature exposure in [0, 1]. Lower = more aggressive penalization. If list, creates multiple outputs.</p> <code>0.1</code> <code>pred_name</code> <code>str or list of str</code> <p>Name(s) of prediction column(s) to penalize.</p> <code>'prediction'</code> <code>feature_names</code> <code>list of str</code> <p>Names of feature columns.</p> <code>None</code> <code>suffix</code> <code>str</code> <p>Suffix to append to output column names.</p> <code>None</code> <code>lr</code> <code>float</code> <p>Learning rate for Adamax optimizer. Default 1e-3.</p> <code>0.001</code> <code>max_iters</code> <code>int</code> <p>Maximum optimization iterations per era.</p> <code>100000</code> <code>tol</code> <code>float</code> <p>Early stopping tolerance for loss.</p> <code>1e-07</code> <code>n_jobs</code> <code>int</code> <p>Number of parallel jobs. 1 = sequential, -1 = all cores.</p> <code>1</code> <code>verbose</code> <code>bool</code> <p>Show progress bar over eras. Default False.</p> <code>False</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; from centimators.feature_transformers import FeaturePenalizer\n&gt;&gt;&gt; df = pd.DataFrame({\n...     'era': ['era1'] * 50 + ['era2'] * 50,\n...     'feature1': np.random.randn(100),\n...     'feature2': np.random.randn(100),\n...     'prediction': np.random.randn(100)\n... })\n&gt;&gt;&gt; penalizer = FeaturePenalizer(\n...     max_exposure=0.1,\n...     pred_name='prediction',\n...     feature_names=['feature1', 'feature2']\n... )\n&gt;&gt;&gt; result = penalizer.fit_transform(\n...     df[['prediction']],\n...     features=df[['feature1', 'feature2']],\n...     era_series=df['era']\n... )\n</code></pre> Source code in <code>src/centimators/feature_transformers/penalization.py</code> <pre><code>class FeaturePenalizer(_BaseFeatureTransformer):\n    \"\"\"\n    Feature penalization using iterative optimization to cap feature exposure.\n\n    Unlike FeatureNeutralizer which subtracts a fixed proportion of linear exposure,\n    this transformer uses gradient descent to find the minimal adjustment that caps\n    all feature exposures below a threshold. This preserves more of the original\n    signal while ensuring no single feature dominates.\n\n    For each era, it:\n    1. Gaussianizes the predictions (rank -&gt; normalize -&gt; inverse CDF)\n    2. Trains a linear model to subtract from predictions such that\n       |exposure to any feature| &lt;= max_exposure\n    3. Re-normalizes and scales to [0, 1]\n\n    Args:\n        max_exposure (float or list of float): Maximum allowed feature exposure in [0, 1].\n            Lower = more aggressive penalization. If list, creates multiple outputs.\n        pred_name (str or list of str): Name(s) of prediction column(s) to penalize.\n        feature_names (list of str, optional): Names of feature columns.\n        suffix (str, optional): Suffix to append to output column names.\n        lr (float): Learning rate for Adamax optimizer. Default 1e-3.\n        max_iters (int): Maximum optimization iterations per era.\n        tol (float): Early stopping tolerance for loss.\n        n_jobs (int): Number of parallel jobs. 1 = sequential, -1 = all cores.\n        verbose (bool): Show progress bar over eras. Default False.\n\n    Examples:\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; from centimators.feature_transformers import FeaturePenalizer\n        &gt;&gt;&gt; df = pd.DataFrame({\n        ...     'era': ['era1'] * 50 + ['era2'] * 50,\n        ...     'feature1': np.random.randn(100),\n        ...     'feature2': np.random.randn(100),\n        ...     'prediction': np.random.randn(100)\n        ... })\n        &gt;&gt;&gt; penalizer = FeaturePenalizer(\n        ...     max_exposure=0.1,\n        ...     pred_name='prediction',\n        ...     feature_names=['feature1', 'feature2']\n        ... )\n        &gt;&gt;&gt; result = penalizer.fit_transform(\n        ...     df[['prediction']],\n        ...     features=df[['feature1', 'feature2']],\n        ...     era_series=df['era']\n        ... )\n    \"\"\"\n\n    def __init__(\n        self,\n        max_exposure: float | list[float] = 0.1,\n        pred_name: str | list[str] = \"prediction\",\n        feature_names: list[str] | None = None,\n        suffix: str | None = None,\n        lr: float = 1e-3,\n        max_iters: int = 100_000,\n        tol: float = 1e-7,\n        n_jobs: int = 1,\n        verbose: bool = False,\n    ):\n        # Normalize inputs to lists\n        self.pred_names = [pred_name] if isinstance(pred_name, str) else pred_name\n        self.max_exposures = (\n            [max_exposure] if isinstance(max_exposure, float) else max_exposure\n        )\n\n        # Validate\n        assert len(self.pred_names) == len(set(self.pred_names)), (\n            \"Duplicate pred_names found.\"\n        )\n        for exp in self.max_exposures:\n            assert 0.0 &lt;= exp &lt;= 1.0, f\"max_exposure should be in [0, 1]. Got {exp}.\"\n\n        self.suffix = suffix\n        self.lr = lr\n        self.max_iters = max_iters\n        self.tol = tol\n        self.n_jobs = n_jobs\n        self.verbose = verbose\n\n        # Generate output column names\n        self._output_names = [\n            (\n                f\"{pname}_penalized_{exp}_{suffix}\"\n                if suffix\n                else f\"{pname}_penalized_{exp}\"\n            )\n            for pname in self.pred_names\n            for exp in self.max_exposures\n        ]\n\n        super().__init__(feature_names)\n\n    @nw.narwhalify(allow_series=True)\n    def transform(\n        self,\n        X: FrameT,\n        y=None,\n        features: FrameT | None = None,\n        era_series: IntoSeries | None = None,\n    ) -&gt; FrameT:\n        \"\"\"Penalize predictions to cap feature exposure.\n\n        Args:\n            X: Input predictions to penalize (shape: n_samples x n_predictions).\n            y: Ignored. Kept for sklearn compatibility.\n            features: DataFrame with features for penalization.\n            era_series: Series with era labels for grouping.\n\n        Returns:\n            DataFrame with penalized predictions, scaled to [0, 1].\n        \"\"\"\n        if features is None:\n            features = X\n\n        predictions = _ensure_numpy(X)\n        feature_array = _ensure_numpy(features)\n\n        if predictions.ndim == 1:\n            assert len(self.pred_names) == 1\n            predictions = predictions.reshape(-1, 1)\n        else:\n            assert predictions.shape[1] == len(self.pred_names)\n\n        if era_series is not None:\n            eras = _ensure_numpy(era_series, allow_series=True)\n        else:\n            warnings.warn(\n                \"era_series not provided. Treating all data as a single era. \"\n                \"This is fine for live inference (1 era) but may be incorrect \"\n                \"for training data with multiple eras.\",\n                UserWarning,\n            )\n            eras = np.array([\"X\"] * len(predictions))\n\n        # Process each prediction column and max_exposure\n        if self.n_jobs == 1:\n            results = [\n                self._penalize_by_era(\n                    predictions[:, pred_idx], feature_array, eras, max_exp, self.verbose\n                )\n                for pred_idx in range(len(self.pred_names))\n                for max_exp in self.max_exposures\n            ]\n        else:\n            # Disable verbose in parallel mode\n            tasks = [\n                delayed(self._penalize_by_era)(\n                    predictions[:, pred_idx], feature_array, eras, max_exp, False\n                )\n                for pred_idx in range(len(self.pred_names))\n                for max_exp in self.max_exposures\n            ]\n            results = Parallel(n_jobs=self.n_jobs)(tasks)\n\n        result_array = np.column_stack(results)\n        result_dict = {\n            col_name: result_array[:, i]\n            for i, col_name in enumerate(self._output_names)\n        }\n\n        native_namespace = nw.get_native_namespace(X)\n        return nw.from_native(\n            native_namespace.DataFrame(result_dict),\n            eager_only=True,\n        )\n\n    def _penalize_by_era(\n        self,\n        predictions: np.ndarray,\n        features: np.ndarray,\n        eras: np.ndarray,\n        max_exposure: float,\n        verbose: bool = False,\n    ) -&gt; np.ndarray:\n        \"\"\"Penalize predictions era by era.\"\"\"\n        unique_eras = np.unique(eras)\n        penalized = np.zeros_like(predictions)\n\n        era_iter = tqdm(\n            unique_eras, desc=f\"max_exp={max_exposure}\", disable=not verbose\n        )\n        for era in era_iter:\n            mask = eras == era\n            era_pred = predictions[mask]\n            era_features = features[mask]\n\n            # Gaussianize then penalize\n            era_pred_norm = _gaussianize(era_pred)\n            era_pred_pen = self._reduce_exposure(\n                era_pred_norm, era_features, max_exposure\n            )\n            # Standardize within era\n            era_pred_pen = era_pred_pen / np.std(era_pred_pen)\n            penalized[mask] = era_pred_pen\n\n        return _min_max_scale(penalized)\n\n    def _reduce_exposure(\n        self,\n        prediction: np.ndarray,\n        features: np.ndarray,\n        max_exp: float,\n    ) -&gt; np.ndarray:\n        \"\"\"\n        Learn a linear adjustment to predictions that caps feature exposure.\n\n        Uses Adamax optimization with full JIT compilation via lax.while_loop\n        to find weights such that:\n            neutralized = prediction - features @ weights\n        has |exposure to any feature| &lt;= max_exp.\n        \"\"\"\n        feats = jnp.asarray(features - 0.5, dtype=jnp.float32)\n        pred = jnp.asarray(prediction, dtype=jnp.float32)[:, None]\n        n_features = feats.shape[1]\n\n        # Target: clamp current exposures to [-max_exp, max_exp]\n        target_exp = jnp.clip(self._exposures(feats, pred), -max_exp, max_exp)\n\n        # Adamax hyperparameters\n        beta1, beta2 = 0.9, 0.999\n        eps = 1e-7\n        lr = self.lr\n        tol = self.tol\n        max_iters = self.max_iters\n\n        def loss_fn(w):\n            neutralized = pred - feats @ w\n            exps = self._exposures(feats, neutralized)\n            pos_excess = jax.nn.relu(jax.nn.relu(exps) - jax.nn.relu(target_exp))\n            neg_excess = jax.nn.relu(jax.nn.relu(-exps) - jax.nn.relu(-target_exp))\n            return jnp.sum(pos_excess + neg_excess)\n\n        def cond_fn(state):\n            w, m, u, t, loss = state\n            return (loss &gt;= tol) &amp; (t &lt; max_iters)\n\n        def body_fn(state):\n            w, m, u, t, _ = state\n            loss, grads = jax.value_and_grad(loss_fn)(w)\n            m_new = beta1 * m + (1 - beta1) * grads\n            u_new = jnp.maximum(beta2 * u, jnp.abs(grads))\n            m_hat = m_new / (1 - beta1 ** (t + 1))\n            w_new = w - lr * m_hat / (u_new + eps)\n            return w_new, m_new, u_new, t + 1, loss\n\n        @jax.jit\n        def optimize():\n            init_state = (\n                jnp.zeros((n_features, 1)),  # weights\n                jnp.zeros((n_features, 1)),  # m (first moment)\n                jnp.zeros((n_features, 1)),  # u (infinity norm)\n                jnp.array(0),  # t (iteration)\n                jnp.array(float(\"inf\")),  # loss\n            )\n            w, m, u, t, loss = lax.while_loop(cond_fn, body_fn, init_state)\n            return pred - feats @ w\n\n        neutralized = optimize()\n        return np.asarray(neutralized).squeeze()\n\n    @staticmethod\n    def _exposures(x: jnp.ndarray, y: jnp.ndarray) -&gt; jnp.ndarray:\n        \"\"\"Correlation between features (x) and predictions (y).\"\"\"\n        x = x - jnp.mean(x, axis=0)\n        x = x / jnp.linalg.norm(x, axis=0)\n        y = y - jnp.mean(y, axis=0)\n        y = y / jnp.linalg.norm(y, axis=0)\n        return x.T @ y\n</code></pre>"},{"location":"api-reference/feature_transformers/#centimators.feature_transformers.penalization.FeaturePenalizer.transform","title":"<code>transform(X, y=None, features=None, era_series=None)</code>","text":"<p>Penalize predictions to cap feature exposure.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>FrameT</code> <p>Input predictions to penalize (shape: n_samples x n_predictions).</p> required <code>y</code> <p>Ignored. Kept for sklearn compatibility.</p> <code>None</code> <code>features</code> <code>FrameT | None</code> <p>DataFrame with features for penalization.</p> <code>None</code> <code>era_series</code> <code>IntoSeries | None</code> <p>Series with era labels for grouping.</p> <code>None</code> <p>Returns:</p> Type Description <code>FrameT</code> <p>DataFrame with penalized predictions, scaled to [0, 1].</p> Source code in <code>src/centimators/feature_transformers/penalization.py</code> <pre><code>@nw.narwhalify(allow_series=True)\ndef transform(\n    self,\n    X: FrameT,\n    y=None,\n    features: FrameT | None = None,\n    era_series: IntoSeries | None = None,\n) -&gt; FrameT:\n    \"\"\"Penalize predictions to cap feature exposure.\n\n    Args:\n        X: Input predictions to penalize (shape: n_samples x n_predictions).\n        y: Ignored. Kept for sklearn compatibility.\n        features: DataFrame with features for penalization.\n        era_series: Series with era labels for grouping.\n\n    Returns:\n        DataFrame with penalized predictions, scaled to [0, 1].\n    \"\"\"\n    if features is None:\n        features = X\n\n    predictions = _ensure_numpy(X)\n    feature_array = _ensure_numpy(features)\n\n    if predictions.ndim == 1:\n        assert len(self.pred_names) == 1\n        predictions = predictions.reshape(-1, 1)\n    else:\n        assert predictions.shape[1] == len(self.pred_names)\n\n    if era_series is not None:\n        eras = _ensure_numpy(era_series, allow_series=True)\n    else:\n        warnings.warn(\n            \"era_series not provided. Treating all data as a single era. \"\n            \"This is fine for live inference (1 era) but may be incorrect \"\n            \"for training data with multiple eras.\",\n            UserWarning,\n        )\n        eras = np.array([\"X\"] * len(predictions))\n\n    # Process each prediction column and max_exposure\n    if self.n_jobs == 1:\n        results = [\n            self._penalize_by_era(\n                predictions[:, pred_idx], feature_array, eras, max_exp, self.verbose\n            )\n            for pred_idx in range(len(self.pred_names))\n            for max_exp in self.max_exposures\n        ]\n    else:\n        # Disable verbose in parallel mode\n        tasks = [\n            delayed(self._penalize_by_era)(\n                predictions[:, pred_idx], feature_array, eras, max_exp, False\n            )\n            for pred_idx in range(len(self.pred_names))\n            for max_exp in self.max_exposures\n        ]\n        results = Parallel(n_jobs=self.n_jobs)(tasks)\n\n    result_array = np.column_stack(results)\n    result_dict = {\n        col_name: result_array[:, i]\n        for i, col_name in enumerate(self._output_names)\n    }\n\n    native_namespace = nw.get_native_namespace(X)\n    return nw.from_native(\n        native_namespace.DataFrame(result_dict),\n        eager_only=True,\n    )\n</code></pre>"},{"location":"api-reference/keras_cortex/","title":"KerasCortex","text":""},{"location":"api-reference/keras_cortex/#centimators.model_estimators.keras_cortex","title":"<code>centimators.model_estimators.keras_cortex</code>","text":"<p>Keras Cortex: A self-improving Keras estimator wrapper using DSPy to self-reflect and improve its architecture.</p> <p>This module provides KerasCortex, a scikit-learn compatible meta-estimator.</p>"},{"location":"api-reference/keras_cortex/#centimators.model_estimators.keras_cortex.KerasCodeRefinements","title":"<code>KerasCodeRefinements</code>","text":"<p>               Bases: <code>Signature</code></p> <p>Suggest modifications to build_model code to improve performance.</p> Source code in <code>src/centimators/model_estimators/keras_cortex.py</code> <pre><code>class KerasCodeRefinements(Signature):\n    \"\"\"Suggest modifications to build_model code to improve performance.\"\"\"\n\n    current_keras_code = InputField(desc=\"Source code of build_model method.\")\n    performance_log = InputField(desc=\"History of (code, metric) pairs.\")\n    optimization_goal = InputField(desc=\"Objective, e.g., 'improve validation scores'.\")\n    suggested_keras_code_modification = OutputField(\n        desc=(\n            \"Modified build_model method body as code. No code fences. \"\n            \"You must start with only 'def build_model(self):'\"\n        )\n    )\n</code></pre>"},{"location":"api-reference/keras_cortex/#centimators.model_estimators.keras_cortex.Think","title":"<code>Think</code>","text":"<p>               Bases: <code>Module</code></p> <p>DSPy Module for suggesting Keras model code modifications.</p> Source code in <code>src/centimators/model_estimators/keras_cortex.py</code> <pre><code>class Think(Module):\n    \"\"\"DSPy Module for suggesting Keras model code modifications.\"\"\"\n\n    def __init__(self, verbose=False):\n        super().__init__()\n        self.suggest_code = ChainOfThought(KerasCodeRefinements)\n        self.verbose = verbose\n\n    def forward(self, current_keras_code, performance_log, optimization_goal):\n        prediction = self.suggest_code(\n            current_keras_code=current_keras_code,\n            performance_log=performance_log,\n            optimization_goal=optimization_goal,\n        )\n        if self.verbose:\n            print(f\"Reasoning: \\n{prediction.reasoning}\")\n            print(f\"Suggested code: \\n{prediction.suggested_keras_code_modification}\")\n        return prediction.suggested_keras_code_modification\n</code></pre>"},{"location":"api-reference/keras_cortex/#centimators.model_estimators.keras_cortex.KerasCortex","title":"<code>KerasCortex</code>","text":"<p>               Bases: <code>RegressorMixin</code>, <code>BaseEstimator</code></p> <p>A scikit-learn meta-estimator that iteratively refines a Keras model.</p> Source code in <code>src/centimators/model_estimators/keras_cortex.py</code> <pre><code>class KerasCortex(RegressorMixin, BaseEstimator):\n    \"\"\"A scikit-learn meta-estimator that iteratively refines a Keras model.\"\"\"\n\n    def __init__(\n        self,\n        base_estimator=None,\n        n_iterations=5,\n        lm=\"openai/gpt-4o-mini\",\n        verbose=False,\n    ):\n        if base_estimator is None:\n            base_estimator = MLPRegressor()\n        self.base_estimator = base_estimator\n        self.n_iterations = n_iterations\n        self.lm = dspy.LM(lm)\n        dspy.configure(lm=self.lm)\n        self.verbose = verbose\n\n    def think_loop(\n        self, base_estimator, X, y, validation_data, n_iterations=5, **kwargs\n    ) -&gt; tuple[BaseEstimator, list[tuple[str, float]]]:\n        baseline_model = clone(base_estimator)\n        baseline_model.fit(X, y, **kwargs)\n\n        X_val, y_val = validation_data\n        best_metric = baseline_model.score(X_val, y_val)\n        current_code = inspect.getsource(type(baseline_model).build_model)\n        performance_log = [(current_code, best_metric)]\n\n        best_model = baseline_model\n        suggestion = current_code\n\n        think = Think(verbose=self.verbose)\n        for i in range(n_iterations):\n            print(f\"\\n--- Iteration {i + 1} ---\")\n            try:\n                suggestion = think.forward(\n                    current_keras_code=suggestion,\n                    performance_log=performance_log,\n                    optimization_goal=\"improve validation metrics (R2)\",\n                )\n                namespace = {}\n                exec(suggestion, globals(), namespace)\n                build_model_fn = namespace[\"build_model\"]\n\n                new_model = clone(base_estimator)\n                new_model.build_model = types.MethodType(build_model_fn, new_model)\n                new_model.fit(X, y, **kwargs)\n                metric = new_model.score(X_val, y_val)\n\n                performance_log.append((suggestion, metric))\n                if metric &gt; best_metric:\n                    print(\n                        f\"Improvement! New validation score: {metric:.4f} &gt; {best_metric:.4f}\"\n                    )\n                    best_metric = metric\n                    best_model = new_model\n                else:\n                    print(\n                        f\"No improvement ({metric:.4f} &lt;= {best_metric:.4f}), keeping best code.\"\n                    )\n            except Exception as e:\n                print(\"Error during optimization iteration:\", e)\n                break\n\n        return best_model, performance_log\n\n    def fit(\n        self,\n        X,\n        y,\n        epochs: int = 100,\n        batch_size: int = 32,\n        validation_data: tuple[Any, Any] | None = None,\n        callbacks: list[Any] | None = None,\n        verbose: int = 1,\n        sample_weight: Any | None = None,\n        **kwargs: Any,\n    ) -&gt; \"KerasCortex\":\n        self.best_model_, self.performance_log_ = self.think_loop(\n            base_estimator=self.base_estimator,\n            X=X,\n            y=y,\n            validation_data=validation_data,\n            n_iterations=self.n_iterations,\n            epochs=epochs,\n            batch_size=batch_size,\n            callbacks=callbacks,\n            verbose=verbose,\n            sample_weight=sample_weight,\n            **kwargs,\n        )\n        return self\n\n    def predict(self, X):\n        if not hasattr(self, \"best_model_\"):\n            raise ValueError(\"Estimator not fitted. Call 'fit' first.\")\n        return self.best_model_.predict(X)\n</code></pre>"},{"location":"api-reference/losses/","title":"Losses","text":""},{"location":"api-reference/losses/#centimators.losses","title":"<code>centimators.losses</code>","text":"<p>Custom loss functions for neural network training.</p> <p>This module provides specialized loss functions that extend beyond standard metrics. The main focus is on rank-based losses that better capture relative ordering patterns in predictions, which can be particularly useful for financial or ranking tasks.</p> Highlights <ul> <li>SpearmanCorrelation \u2013 Differentiable approximation of Spearman's rank   correlation coefficient that can be used as a loss function.</li> <li>CombinedLoss \u2013 Weighted combination of MSE and Spearman correlation   losses for balancing absolute accuracy with rank preservation.</li> </ul>"},{"location":"api-reference/losses/#centimators.losses.SpearmanCorrelation","title":"<code>SpearmanCorrelation</code>","text":"<p>               Bases: <code>Loss</code></p> <p>Differentiable Spearman rank correlation loss.</p> <p>This loss function computes a soft approximation of Spearman's rank correlation coefficient between predictions and targets. Unlike the standard non-differentiable rank correlation, this implementation uses sigmoid-based soft rankings that allow gradient flow during backpropagation.</p> <p>The loss is computed as the negative correlation (to minimize during training) between the soft ranks of predictions and targets.</p> <p>Parameters:</p> Name Type Description Default <code>regularization_strength</code> <code>float, default=1e-3</code> <p>Temperature parameter for the sigmoid function used in soft ranking. Smaller values create sharper (more discrete) rankings, while larger values create smoother approximations. Typically ranges from 1e-4 to 1e-1.</p> <code>0.001</code> <code>name</code> <code>str, default=\"spearman_correlation\"</code> <p>Name of the loss function.</p> <code>'spearman_correlation'</code> <code>**kwargs</code> <p>Additional keyword arguments passed to the base Loss class.</p> <code>{}</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import keras\n&gt;&gt;&gt; loss_fn = SpearmanCorrelation(regularization_strength=0.01)\n&gt;&gt;&gt; model = keras.Sequential([...])\n&gt;&gt;&gt; model.compile(optimizer='adam', loss=loss_fn)\n</code></pre> Source code in <code>src/centimators/losses.py</code> <pre><code>@register_keras_serializable(package=\"centimators\")\nclass SpearmanCorrelation(Loss):\n    \"\"\"Differentiable Spearman rank correlation loss.\n\n    This loss function computes a soft approximation of Spearman's rank\n    correlation coefficient between predictions and targets. Unlike the\n    standard non-differentiable rank correlation, this implementation uses\n    sigmoid-based soft rankings that allow gradient flow during backpropagation.\n\n    The loss is computed as the negative correlation (to minimize during training)\n    between the soft ranks of predictions and targets.\n\n    Args:\n        regularization_strength (float, default=1e-3): Temperature parameter for\n            the sigmoid function used in soft ranking. Smaller values create\n            sharper (more discrete) rankings, while larger values create smoother\n            approximations. Typically ranges from 1e-4 to 1e-1.\n        name (str, default=\"spearman_correlation\"): Name of the loss function.\n        **kwargs: Additional keyword arguments passed to the base Loss class.\n\n    Examples:\n        &gt;&gt;&gt; import keras\n        &gt;&gt;&gt; loss_fn = SpearmanCorrelation(regularization_strength=0.01)\n        &gt;&gt;&gt; model = keras.Sequential([...])\n        &gt;&gt;&gt; model.compile(optimizer='adam', loss=loss_fn)\n    \"\"\"\n\n    def __init__(\n        self, regularization_strength=1e-3, name=\"spearman_correlation\", **kwargs\n    ):\n        super().__init__(name=name, **kwargs)\n        self.regularization_strength = regularization_strength\n\n    def call(self, y_true, y_pred):\n        \"\"\"Compute the Spearman correlation loss.\n\n        Args:\n            y_true: Ground truth values of shape (batch_size,) or (batch_size, 1).\n            y_pred: Predicted values of shape (batch_size,) or (batch_size, 1).\n\n        Returns:\n            Scalar loss value (negative correlation).\n        \"\"\"\n        # Reshape inputs to ensure 2D\n        y_true = K.reshape(y_true, (-1, 1))\n        y_pred = K.reshape(y_pred, (-1, 1))\n\n        # Calculate soft ranks for both true and predicted values\n        true_ranks = self._soft_rank(y_true)\n        pred_ranks = self._soft_rank(y_pred)\n\n        # Calculate correlation between ranks\n        return -self._correlation(true_ranks, pred_ranks)\n\n    def _soft_rank(self, x):\n        \"\"\"Compute differentiable soft ranks using sigmoid approximation.\n\n        Args:\n            x: Input tensor of shape (batch_size, 1).\n\n        Returns:\n            Soft ranks tensor of shape (batch_size, 1).\n        \"\"\"\n        # Create pairwise differences matrix\n        x_expanded1 = K.expand_dims(x, 1)\n        x_expanded2 = K.expand_dims(x, 0)\n        diff = x_expanded1 - x_expanded2\n\n        # Apply soft step function\n        soft_step = K.sigmoid(diff / self.regularization_strength)\n\n        # Sum over rows to get ranks\n        ranks = K.sum(soft_step, axis=1)\n        return ranks\n\n    def _correlation(self, x, y):\n        \"\"\"Compute Pearson correlation between two tensors.\n\n        Args:\n            x: First tensor of shape (batch_size, 1).\n            y: Second tensor of shape (batch_size, 1).\n\n        Returns:\n            Scalar correlation value in range [-1, 1].\n        \"\"\"\n        # Mean center\n        x_centered = x - K.mean(x)\n        y_centered = y - K.mean(y)\n\n        # Calculate correlation\n        numerator = K.sum(x_centered * y_centered)\n        denominator = K.sqrt(\n            K.sum(K.square(x_centered)) * K.sum(K.square(y_centered)) + epsilon()\n        )\n\n        return numerator / denominator\n\n    def get_config(self):\n        config = super().get_config()\n        config.update({\"regularization_strength\": self.regularization_strength})\n        return config\n</code></pre>"},{"location":"api-reference/losses/#centimators.losses.SpearmanCorrelation.call","title":"<code>call(y_true, y_pred)</code>","text":"<p>Compute the Spearman correlation loss.</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <p>Ground truth values of shape (batch_size,) or (batch_size, 1).</p> required <code>y_pred</code> <p>Predicted values of shape (batch_size,) or (batch_size, 1).</p> required <p>Returns:</p> Type Description <p>Scalar loss value (negative correlation).</p> Source code in <code>src/centimators/losses.py</code> <pre><code>def call(self, y_true, y_pred):\n    \"\"\"Compute the Spearman correlation loss.\n\n    Args:\n        y_true: Ground truth values of shape (batch_size,) or (batch_size, 1).\n        y_pred: Predicted values of shape (batch_size,) or (batch_size, 1).\n\n    Returns:\n        Scalar loss value (negative correlation).\n    \"\"\"\n    # Reshape inputs to ensure 2D\n    y_true = K.reshape(y_true, (-1, 1))\n    y_pred = K.reshape(y_pred, (-1, 1))\n\n    # Calculate soft ranks for both true and predicted values\n    true_ranks = self._soft_rank(y_true)\n    pred_ranks = self._soft_rank(y_pred)\n\n    # Calculate correlation between ranks\n    return -self._correlation(true_ranks, pred_ranks)\n</code></pre>"},{"location":"api-reference/losses/#centimators.losses.CombinedLoss","title":"<code>CombinedLoss</code>","text":"<p>               Bases: <code>Loss</code></p> <p>Weighted combination of MSE and Spearman correlation losses.</p> <p>This loss function combines mean squared error (for absolute accuracy) with Spearman correlation loss (for rank preservation). This can be particularly useful when both the exact values and their relative ordering are important.</p> <p>Parameters:</p> Name Type Description Default <code>mse_weight</code> <code>float, default=2.0</code> <p>Weight applied to the MSE component. Higher values prioritize absolute accuracy.</p> <code>2.0</code> <code>spearman_weight</code> <code>float, default=1.0</code> <p>Weight applied to the Spearman correlation component. Higher values prioritize rank preservation.</p> <code>1.0</code> <code>spearman_regularization</code> <code>float, default=1e-3</code> <p>Regularization strength passed to the SpearmanCorrelation loss.</p> <code>0.001</code> <code>name</code> <code>str, default=\"combined_loss\"</code> <p>Name of the loss function.</p> <code>'combined_loss'</code> <code>**kwargs</code> <p>Additional keyword arguments passed to the base Loss class.</p> <code>{}</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # Prioritize ranking accuracy over absolute values\n&gt;&gt;&gt; loss_fn = CombinedLoss(mse_weight=0.5, spearman_weight=2.0)\n&gt;&gt;&gt; model.compile(optimizer='adam', loss=loss_fn)\n</code></pre> Source code in <code>src/centimators/losses.py</code> <pre><code>@register_keras_serializable(package=\"centimators\")\nclass CombinedLoss(Loss):\n    \"\"\"Weighted combination of MSE and Spearman correlation losses.\n\n    This loss function combines mean squared error (for absolute accuracy)\n    with Spearman correlation loss (for rank preservation). This can be\n    particularly useful when both the exact values and their relative\n    ordering are important.\n\n    Args:\n        mse_weight (float, default=2.0): Weight applied to the MSE component.\n            Higher values prioritize absolute accuracy.\n        spearman_weight (float, default=1.0): Weight applied to the Spearman\n            correlation component. Higher values prioritize rank preservation.\n        spearman_regularization (float, default=1e-3): Regularization strength\n            passed to the SpearmanCorrelation loss.\n        name (str, default=\"combined_loss\"): Name of the loss function.\n        **kwargs: Additional keyword arguments passed to the base Loss class.\n\n    Examples:\n        &gt;&gt;&gt; # Prioritize ranking accuracy over absolute values\n        &gt;&gt;&gt; loss_fn = CombinedLoss(mse_weight=0.5, spearman_weight=2.0)\n        &gt;&gt;&gt; model.compile(optimizer='adam', loss=loss_fn)\n    \"\"\"\n\n    def __init__(\n        self,\n        mse_weight=2.0,\n        spearman_weight=1.0,\n        spearman_regularization=1e-3,\n        name=\"combined_loss\",\n        **kwargs,\n    ):\n        super().__init__(name=name, **kwargs)\n        self.mse_weight = mse_weight\n        self.spearman_weight = spearman_weight\n        self.spearman_loss = SpearmanCorrelation(\n            regularization_strength=spearman_regularization\n        )\n\n    def call(self, y_true, y_pred):\n        \"\"\"Compute the combined loss.\n\n        Args:\n            y_true: Ground truth values of shape (batch_size,) or (batch_size, 1).\n            y_pred: Predicted values of shape (batch_size,) or (batch_size, 1).\n\n        Returns:\n            Scalar loss value (weighted sum of MSE and negative Spearman correlation).\n        \"\"\"\n        mse = K.mean(K.square(y_pred - y_true))\n        spearman = self.spearman_loss(y_true, y_pred)\n\n        return self.mse_weight * mse + self.spearman_weight * spearman\n\n    def get_config(self):\n        config = super().get_config()\n        config.update(\n            {\n                \"mse_weight\": self.mse_weight,\n                \"spearman_weight\": self.spearman_weight,\n                \"spearman_regularization\": self.spearman_loss.regularization_strength,\n            }\n        )\n        return config\n</code></pre>"},{"location":"api-reference/losses/#centimators.losses.CombinedLoss.call","title":"<code>call(y_true, y_pred)</code>","text":"<p>Compute the combined loss.</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <p>Ground truth values of shape (batch_size,) or (batch_size, 1).</p> required <code>y_pred</code> <p>Predicted values of shape (batch_size,) or (batch_size, 1).</p> required <p>Returns:</p> Type Description <p>Scalar loss value (weighted sum of MSE and negative Spearman correlation).</p> Source code in <code>src/centimators/losses.py</code> <pre><code>def call(self, y_true, y_pred):\n    \"\"\"Compute the combined loss.\n\n    Args:\n        y_true: Ground truth values of shape (batch_size,) or (batch_size, 1).\n        y_pred: Predicted values of shape (batch_size,) or (batch_size, 1).\n\n    Returns:\n        Scalar loss value (weighted sum of MSE and negative Spearman correlation).\n    \"\"\"\n    mse = K.mean(K.square(y_pred - y_true))\n    spearman = self.spearman_loss(y_true, y_pred)\n\n    return self.mse_weight * mse + self.spearman_weight * spearman\n</code></pre>"},{"location":"api-reference/model_estimators/","title":"Model Estimators","text":""},{"location":"api-reference/model_estimators/#centimators.model_estimators.keras_estimators","title":"<code>centimators.model_estimators.keras_estimators</code>","text":"<p>Keras-based model estimators with scikit-learn compatible API.</p> Organized by architectural family <ul> <li>base: BaseKerasEstimator and shared utilities</li> <li>dense: Simple feedforward networks (MLPRegressor)</li> <li>autoencoder: Reconstruction-based architectures (BottleneckEncoder)</li> <li>sequence: Sequence models for temporal data (SequenceEstimator, LSTMRegressor)</li> </ul>"},{"location":"api-reference/model_estimators/#centimators.model_estimators.keras_estimators.BaseKerasEstimator","title":"<code>BaseKerasEstimator</code>  <code>dataclass</code>","text":"<p>               Bases: <code>TransformerMixin</code>, <code>BaseEstimator</code>, <code>ABC</code></p> <p>Meta-estimator for Keras models following the scikit-learn API.</p> <p>Parameters:</p> Name Type Description Default <code>output_units</code> <code>int, default=1</code> <p>Number of output units in the final layer.</p> <code>1</code> <code>optimizer</code> <code>Type[optimizers.Optimizer], default=Adam</code> <p>Keras optimizer class to use for training.</p> <code>Adam</code> <code>learning_rate</code> <code>float, default=0.001</code> <p>Learning rate for the optimizer.</p> <code>0.001</code> <code>loss_function</code> <code>str, default=\"mse\"</code> <p>Loss function name passed to model.compile().</p> <code>'mse'</code> <code>metrics</code> <code>list[str] | None, default=None</code> <p>List of metric names to track during training.</p> <code>None</code> <code>model</code> <code>Any, default=None</code> <p>The underlying Keras model (populated by build_model).</p> <code>None</code> <code>distribution_strategy</code> <code>str | None, default=None</code> <p>If set, enables DataParallel distribution for multi-device training.</p> <code>None</code> <code>target_scaler</code> <code>sklearn transformer | None, default=None</code> <p>Scaler for target values. Neural networks converge better when targets are normalized. Subclasses may override the default (e.g., regressors default to StandardScaler).</p> <code>None</code> Source code in <code>src/centimators/model_estimators/keras_estimators/base.py</code> <pre><code>@dataclass(kw_only=True)\nclass BaseKerasEstimator(TransformerMixin, BaseEstimator, ABC):\n    \"\"\"Meta-estimator for Keras models following the scikit-learn API.\n\n    Args:\n        output_units (int, default=1): Number of output units in the final layer.\n        optimizer (Type[optimizers.Optimizer], default=Adam): Keras optimizer class\n            to use for training.\n        learning_rate (float, default=0.001): Learning rate for the optimizer.\n        loss_function (str, default=\"mse\"): Loss function name passed to model.compile().\n        metrics (list[str] | None, default=None): List of metric names to track\n            during training.\n        model (Any, default=None): The underlying Keras model (populated by build_model).\n        distribution_strategy (str | None, default=None): If set, enables DataParallel\n            distribution for multi-device training.\n        target_scaler (sklearn transformer | None, default=None): Scaler for target\n            values. Neural networks converge better when targets are normalized.\n            Subclasses may override the default (e.g., regressors default to StandardScaler).\n    \"\"\"\n\n    output_units: int = 1\n    optimizer: Type[optimizers.Optimizer] = optimizers.Adam\n    learning_rate: float = 0.001\n    loss_function: str = \"mse\"\n    metrics: list[str] | None = None\n    model: Any = None\n    distribution_strategy: str | None = None\n    target_scaler: Any = None\n\n    @abstractmethod\n    def build_model(self):\n        pass\n\n    def _setup_distribution_strategy(self) -&gt; None:\n        strategy = distribution.DataParallel()\n        distribution.set_distribution(strategy)\n\n    def fit(\n        self,\n        X,\n        y,\n        epochs: int = 100,\n        batch_size: int = 32,\n        validation_data: tuple[Any, Any] | None = None,\n        callbacks: list[Any] | None = None,\n        verbose: int = 1,\n        sample_weight: Any | None = None,\n        **kwargs: Any,\n    ) -&gt; \"BaseKerasEstimator\":\n        self._n_features_in_ = X.shape[1]\n\n        if self.distribution_strategy:\n            self._setup_distribution_strategy()\n\n        # Convert inputs to numpy\n        X_np = _ensure_numpy(X)\n        y_np = _ensure_numpy(y, allow_series=True)\n\n        # Ensure y is 2D for scaler\n        y_was_1d = y_np.ndim == 1\n        if y_was_1d:\n            y_np = y_np.reshape(-1, 1)\n\n        # Scale targets for better neural network convergence\n        if self.target_scaler:\n            y_np = self.target_scaler.fit_transform(y_np).astype(\"float32\")\n\n            # Scale validation targets too\n            if validation_data is not None:\n                val_X, val_y = validation_data\n                val_y_np = _ensure_numpy(val_y, allow_series=True)\n                if val_y_np.ndim == 1:\n                    val_y_np = val_y_np.reshape(-1, 1)\n                val_y_scaled = self.target_scaler.transform(val_y_np).astype(\"float32\")\n                validation_data = (_ensure_numpy(val_X), val_y_scaled)\n\n        if not self.model:\n            self.build_model()\n\n        self.model.fit(\n            X_np,\n            y=y_np,\n            batch_size=batch_size,\n            epochs=epochs,\n            validation_data=validation_data,\n            callbacks=callbacks,\n            verbose=verbose,\n            sample_weight=sample_weight,\n            **kwargs,\n        )\n        self._is_fitted = True\n        return self\n\n    @nw.narwhalify\n    def predict(self, X, batch_size: int = 512, verbose: int = 1, **kwargs: Any) -&gt; Any:\n        if not self.model:\n            raise ValueError(\"Model not built. Call `build_model` first.\")\n\n        predictions = self.model.predict(\n            _ensure_numpy(X), batch_size=batch_size, verbose=verbose, **kwargs\n        )\n\n        # Inverse transform predictions back to original scale\n        if self.target_scaler:\n            predictions = self.target_scaler.inverse_transform(predictions)\n\n        # Return numpy arrays for numpy input\n        if isinstance(X, numpy.ndarray):\n            return predictions\n\n        # Return dataframe for dataframe input\n        if predictions.ndim == 1:\n            return nw.from_dict(\n                {\"prediction\": predictions}, backend=nw.get_native_namespace(X)\n            )\n        elif predictions.shape[1] == 1:\n            return nw.from_dict(\n                {\"prediction\": predictions[:, 0]}, backend=nw.get_native_namespace(X)\n            )\n        else:\n            cols = {\n                f\"prediction_{i}\": predictions[:, i]\n                for i in range(predictions.shape[1])\n            }\n            return nw.from_dict(cols, backend=nw.get_native_namespace(X))\n\n    def transform(self, X, **kwargs):\n        return self.predict(X, **kwargs)\n\n    def __sklearn_is_fitted__(self) -&gt; bool:\n        return getattr(self, \"_is_fitted\", False)\n</code></pre>"},{"location":"api-reference/model_estimators/#centimators.model_estimators.keras_estimators.MLPRegressor","title":"<code>MLPRegressor</code>  <code>dataclass</code>","text":"<p>               Bases: <code>RegressorMixin</code>, <code>BaseKerasEstimator</code></p> <p>A minimal fully-connected multi-layer perceptron for tabular data.</p> Source code in <code>src/centimators/model_estimators/keras_estimators/dense.py</code> <pre><code>@dataclass(kw_only=True)\nclass MLPRegressor(RegressorMixin, BaseKerasEstimator):\n    \"\"\"A minimal fully-connected multi-layer perceptron for tabular data.\"\"\"\n\n    hidden_units: tuple[int, ...] = (64, 64)\n    activation: str = \"relu\"\n    dropout_rate: float = 0.0\n    metrics: list[str] | None = field(default_factory=lambda: [\"mse\"])\n    target_scaler: Any = field(default_factory=StandardScaler)\n\n    def build_model(self):\n        inputs = layers.Input(shape=(self._n_features_in_,), name=\"features\")\n        x = inputs\n        for units in self.hidden_units:\n            x = layers.Dense(units, activation=self.activation)(x)\n            if self.dropout_rate &gt; 0:\n                x = layers.Dropout(self.dropout_rate)(x)\n        outputs = layers.Dense(self.output_units, activation=\"linear\")(x)\n        self.model = models.Model(inputs=inputs, outputs=outputs, name=\"mlp_regressor\")\n\n        self.model.compile(\n            optimizer=self.optimizer(learning_rate=self.learning_rate),\n            loss=self.loss_function,\n            metrics=self.metrics,\n        )\n        return self\n</code></pre>"},{"location":"api-reference/model_estimators/#centimators.model_estimators.keras_estimators.BottleneckEncoder","title":"<code>BottleneckEncoder</code>  <code>dataclass</code>","text":"<p>               Bases: <code>BaseKerasEstimator</code></p> <p>A bottleneck autoencoder that can learn latent representations and predict targets.</p> Source code in <code>src/centimators/model_estimators/keras_estimators/autoencoder.py</code> <pre><code>@dataclass(kw_only=True)\nclass BottleneckEncoder(BaseKerasEstimator):\n    \"\"\"A bottleneck autoencoder that can learn latent representations and predict targets.\"\"\"\n\n    gaussian_noise: float = 0.035\n    encoder_units: list[tuple[int, float]] = field(\n        default_factory=lambda: [(1024, 0.1)]\n    )\n    latent_units: tuple[int, float] = (256, 0.1)\n    ae_units: list[tuple[int, float]] = field(default_factory=lambda: [(96, 0.4)])\n    activation: str = \"swish\"\n    reconstruction_loss_weight: float = 1.0\n    target_loss_weight: float = 1.0\n    encoder: Any = None\n\n    def build_model(self):\n        if self._n_features_in_ is None:\n            raise ValueError(\"Must call fit() before building the model\")\n\n        inputs = layers.Input(shape=(self._n_features_in_,), name=\"features\")\n        x0 = layers.BatchNormalization()(inputs)\n\n        encoder = layers.GaussianNoise(self.gaussian_noise)(x0)\n        for units, dropout in self.encoder_units:\n            encoder = layers.Dense(units)(encoder)\n            encoder = layers.BatchNormalization()(encoder)\n            encoder = layers.Activation(self.activation)(encoder)\n            encoder = layers.Dropout(dropout)(encoder)\n\n        latent_units, latent_dropout = self.latent_units\n        latent = layers.Dense(latent_units)(encoder)\n        latent = layers.BatchNormalization()(latent)\n        latent = layers.Activation(self.activation)(latent)\n        latent_output = layers.Dropout(latent_dropout)(latent)\n\n        self.encoder = models.Model(\n            inputs=inputs, outputs=latent_output, name=\"encoder\"\n        )\n\n        decoder = latent_output\n        for units, dropout in reversed(self.encoder_units):\n            decoder = layers.Dense(units)(decoder)\n            decoder = layers.BatchNormalization()(decoder)\n            decoder = layers.Activation(self.activation)(decoder)\n            decoder = layers.Dropout(dropout)(decoder)\n\n        reconstruction = layers.Dense(self._n_features_in_, name=\"reconstruction\")(\n            decoder\n        )\n\n        target_pred = reconstruction\n        for units, dropout in self.ae_units:\n            target_pred = layers.Dense(units)(target_pred)\n            target_pred = layers.BatchNormalization()(target_pred)\n            target_pred = layers.Activation(self.activation)(target_pred)\n            target_pred = layers.Dropout(dropout)(target_pred)\n\n        target_output = layers.Dense(\n            self.output_units, activation=\"linear\", name=\"target_prediction\"\n        )(target_pred)\n\n        self.model = models.Model(\n            inputs=inputs,\n            outputs=[reconstruction, target_output],\n            name=\"bottleneck_encoder\",\n        )\n\n        self.model.compile(\n            optimizer=self.optimizer(learning_rate=self.learning_rate),\n            loss={\"reconstruction\": \"mse\", \"target_prediction\": self.loss_function},\n            loss_weights={\n                \"reconstruction\": self.reconstruction_loss_weight,\n                \"target_prediction\": self.target_loss_weight,\n            },\n            metrics={\"target_prediction\": self.metrics or [\"mse\"]},\n        )\n        return self\n\n    def fit(\n        self,\n        X,\n        y,\n        epochs: int = 100,\n        batch_size: int = 32,\n        validation_data: tuple[Any, Any] | None = None,\n        callbacks: list[Any] | None = None,\n        verbose: int = 1,\n        sample_weight: Any | None = None,\n        **kwargs: Any,\n    ) -&gt; \"BottleneckEncoder\":\n        self._n_features_in_ = X.shape[1]\n\n        if self.distribution_strategy:\n            self._setup_distribution_strategy()\n\n        if not self.model:\n            self.build_model()\n\n        X_np = _ensure_numpy(X)\n        y_np = _ensure_numpy(y, allow_series=True)\n\n        y_dict = {\"reconstruction\": X_np, \"target_prediction\": y_np}\n\n        if validation_data is not None:\n            X_val, y_val = validation_data\n            X_val_np = _ensure_numpy(X_val)\n            y_val_np = _ensure_numpy(y_val, allow_series=True)\n            validation_data = (\n                X_val_np,\n                {\"reconstruction\": X_val_np, \"target_prediction\": y_val_np},\n            )\n\n        self.model.fit(\n            X_np,\n            y_dict,\n            batch_size=batch_size,\n            epochs=epochs,\n            validation_data=validation_data,\n            callbacks=callbacks,\n            verbose=verbose,\n            sample_weight=sample_weight,\n            **kwargs,\n        )\n\n        self._is_fitted = True\n        return self\n\n    def predict(self, X, batch_size: int = 512, verbose: int = 1, **kwargs: Any) -&gt; Any:\n        if not self.model:\n            raise ValueError(\"Model not built. Call 'fit' first.\")\n        X_np = _ensure_numpy(X)\n        predictions = self.model.predict(\n            X_np, batch_size=batch_size, verbose=verbose, **kwargs\n        )\n        return predictions[1] if isinstance(predictions, list) else predictions\n\n    def transform(\n        self, X, batch_size: int = 512, verbose: int = 1, **kwargs: Any\n    ) -&gt; Any:\n        if not self.encoder:\n            raise ValueError(\"Encoder not built. Call 'fit' first.\")\n        X_np = _ensure_numpy(X)\n        return self.encoder.predict(\n            X_np, batch_size=batch_size, verbose=verbose, **kwargs\n        )\n\n    def fit_transform(self, X, y, **kwargs) -&gt; Any:\n        return self.fit(X, y, **kwargs).transform(X)\n\n    def get_feature_names_out(self, input_features=None) -&gt; list[str]:\n        latent_dim = self.latent_units[0]\n        return [f\"latent_{i}\" for i in range(latent_dim)]\n</code></pre>"},{"location":"api-reference/model_estimators/#centimators.model_estimators.keras_estimators.SequenceEstimator","title":"<code>SequenceEstimator</code>  <code>dataclass</code>","text":"<p>               Bases: <code>BaseKerasEstimator</code></p> <p>Estimator for models that consume sequential data.</p> Source code in <code>src/centimators/model_estimators/keras_estimators/sequence.py</code> <pre><code>@dataclass(kw_only=True)\nclass SequenceEstimator(BaseKerasEstimator):\n    \"\"\"Estimator for models that consume sequential data.\"\"\"\n\n    lag_windows: list[int]\n    n_features_per_timestep: int\n\n    def __post_init__(self):\n        self.seq_length = len(self.lag_windows)\n\n    def _reshape(self, X: IntoFrame, validation_data: tuple[Any, Any] | None = None):\n        X = _ensure_numpy(X)\n        X_reshaped = X.reshape(\n            (X.shape[0], self.seq_length, self.n_features_per_timestep)\n        )\n\n        if validation_data:\n            X_val, y_val = validation_data\n            X_val = _ensure_numpy(X_val)\n            X_val_reshaped = X_val.reshape(\n                (X_val.shape[0], self.seq_length, self.n_features_per_timestep),\n            )\n            validation_data = X_val_reshaped, _ensure_numpy(y_val)\n\n        return X_reshaped, validation_data\n\n    def fit(\n        self,\n        X,\n        y,\n        epochs: int = 100,\n        batch_size: int = 32,\n        validation_data: tuple[Any, Any] | None = None,\n        callbacks: list[Any] | None = None,\n        verbose: int = 1,\n        sample_weight: Any | None = None,\n        **kwargs: Any,\n    ) -&gt; \"SequenceEstimator\":\n        X_reshaped, validation_data_reshaped = self._reshape(X, validation_data)\n        super().fit(\n            X_reshaped,\n            y=_ensure_numpy(y),\n            epochs=epochs,\n            batch_size=batch_size,\n            validation_data=validation_data_reshaped,\n            callbacks=callbacks,\n            verbose=verbose,\n            sample_weight=sample_weight,\n            **kwargs,\n        )\n        return self\n\n    @nw.narwhalify\n    def predict(self, X, batch_size: int = 512, verbose: int = 1, **kwargs: Any) -&gt; Any:\n        if not self.model:\n            raise ValueError(\"Model not built. Call `build_model` first.\")\n\n        # Store original X for backend detection before reshaping\n        X_original = X\n        X_reshaped, _ = self._reshape(X)\n\n        predictions = self.model.predict(\n            _ensure_numpy(X_reshaped), batch_size=batch_size, verbose=verbose, **kwargs\n        )\n\n        # Inverse transform predictions back to original scale\n        if self.target_scaler:\n            predictions = self.target_scaler.inverse_transform(predictions)\n\n        # Use X_original (not X_reshaped) for backend detection\n        if isinstance(X_original, numpy.ndarray):\n            return predictions\n\n        if predictions.ndim == 1:\n            return nw.from_dict(\n                {\"prediction\": predictions}, backend=nw.get_native_namespace(X_original)\n            )\n        else:\n            cols = {\n                f\"prediction_{i}\": predictions[:, i]\n                for i in range(predictions.shape[1])\n            }\n            return nw.from_dict(cols, backend=nw.get_native_namespace(X_original))\n</code></pre>"},{"location":"api-reference/model_estimators/#centimators.model_estimators.keras_estimators.LSTMRegressor","title":"<code>LSTMRegressor</code>  <code>dataclass</code>","text":"<p>               Bases: <code>RegressorMixin</code>, <code>SequenceEstimator</code></p> <p>LSTM-based regressor for sequence prediction.</p> Source code in <code>src/centimators/model_estimators/keras_estimators/sequence.py</code> <pre><code>@dataclass(kw_only=True)\nclass LSTMRegressor(RegressorMixin, SequenceEstimator):\n    \"\"\"LSTM-based regressor for sequence prediction.\"\"\"\n\n    lstm_units: list[tuple[int, float, float]] = field(\n        default_factory=lambda: [(64, 0.01, 0.01)]\n    )\n    use_batch_norm: bool = False\n    use_layer_norm: bool = False\n    bidirectional: bool = False\n    metrics: list[str] | None = field(default_factory=lambda: [\"mse\"])\n    target_scaler: Any = field(default_factory=StandardScaler)\n\n    def build_model(self):\n        if self._n_features_in_ is None:\n            raise ValueError(\"Must call fit() before building the model\")\n\n        inputs = layers.Input(\n            shape=(self.seq_length, self.n_features_per_timestep), name=\"sequence_input\"\n        )\n        x = inputs\n\n        for layer_num, (units, dropout, recurrent_dropout) in enumerate(\n            self.lstm_units\n        ):\n            return_sequences = layer_num &lt; len(self.lstm_units) - 1\n            lstm_layer = layers.LSTM(\n                units=units,\n                activation=\"tanh\",\n                return_sequences=return_sequences,\n                dropout=dropout,\n                recurrent_dropout=recurrent_dropout,\n                name=f\"lstm_{layer_num}\",\n            )\n            if self.bidirectional:\n                x = layers.Bidirectional(lstm_layer, name=f\"bidirectional_{layer_num}\")(\n                    x\n                )\n            else:\n                x = lstm_layer(x)\n            if self.use_layer_norm:\n                x = layers.LayerNormalization(name=f\"layer_norm_{layer_num}\")(x)\n            if self.use_batch_norm:\n                x = layers.BatchNormalization(name=f\"batch_norm_{layer_num}\")(x)\n\n        outputs = layers.Dense(self.output_units, activation=\"linear\", name=\"output\")(x)\n        self.model = models.Model(inputs=inputs, outputs=outputs, name=\"lstm_regressor\")\n        self.model.compile(\n            optimizer=self.optimizer(learning_rate=self.learning_rate),\n            loss=self.loss_function,\n            metrics=self.metrics,\n        )\n        return self\n</code></pre>"},{"location":"api-reference/model_estimators/#centimators.model_estimators.keras_estimators.AttentionPooling","title":"<code>AttentionPooling</code>","text":"<p>               Bases: <code>Layer</code></p> <p>Learned weighted pooling over the sequence dimension.</p> Source code in <code>src/centimators/model_estimators/keras_estimators/transformer.py</code> <pre><code>@register_keras_serializable(package=\"centimators\")\nclass AttentionPooling(layers.Layer):\n    \"\"\"Learned weighted pooling over the sequence dimension.\"\"\"\n\n    def __init__(self, **kwargs):\n        super().__init__(**kwargs)\n        self.score = layers.Dense(1)\n\n    def call(self, inputs):\n        # inputs: (batch, seq_len, d_model)\n        logits = self.score(inputs)  # (batch, seq_len, 1)\n        weights = ops.softmax(logits, axis=1)\n        weighted = inputs * weights\n        return ops.sum(weighted, axis=1)  # (batch, d_model)\n</code></pre>"},{"location":"api-reference/model_estimators/#centimators.model_estimators.keras_estimators.CrossAttention","title":"<code>CrossAttention</code>","text":"<p>               Bases: <code>Layer</code></p> <p>Dual-axis attention: temporal attention + feature attention.</p> Source code in <code>src/centimators/model_estimators/keras_estimators/transformer.py</code> <pre><code>@register_keras_serializable(package=\"centimators\")\nclass CrossAttention(layers.Layer):\n    \"\"\"Dual-axis attention: temporal attention + feature attention.\"\"\"\n\n    def __init__(\n        self, key_dim: int = 32, num_heads: int = 4, dropout: float = 0.1, **kwargs\n    ):\n        super().__init__(**kwargs)\n        self.key_dim = int(key_dim)\n        self.num_heads = int(num_heads)\n        self.dropout = float(dropout)\n\n        self.temporal_attention = layers.MultiHeadAttention(\n            key_dim=self.key_dim,\n            num_heads=self.num_heads,\n            dropout=self.dropout,\n            attention_axes=(1,),\n        )\n        self.feature_attention = layers.MultiHeadAttention(\n            key_dim=self.key_dim,\n            num_heads=self.num_heads,\n            dropout=self.dropout,\n            attention_axes=(2,),\n        )\n\n    def call(self, inputs):\n        temporal_out = self.temporal_attention(inputs, inputs)\n        feature_out = self.feature_attention(inputs, inputs)\n        return temporal_out + feature_out\n\n    def get_config(self):\n        config = super().get_config()\n        config.update(\n            {\n                \"key_dim\": self.key_dim,\n                \"num_heads\": self.num_heads,\n                \"dropout\": self.dropout,\n            }\n        )\n        return config\n</code></pre>"},{"location":"api-reference/model_estimators/#centimators.model_estimators.keras_estimators.PositionEmbedding","title":"<code>PositionEmbedding</code>","text":"<p>               Bases: <code>Layer</code></p> <p>Learned positional embedding with fixed sequence length.</p> Source code in <code>src/centimators/model_estimators/keras_estimators/transformer.py</code> <pre><code>@register_keras_serializable(package=\"centimators\")\nclass PositionEmbedding(layers.Layer):\n    \"\"\"Learned positional embedding with fixed sequence length.\"\"\"\n\n    def __init__(\n        self, sequence_length: int, initializer: str = \"glorot_uniform\", **kwargs\n    ):\n        super().__init__(**kwargs)\n        self.sequence_length = int(sequence_length)\n        self.initializer = initializers.get(initializer)\n\n    def build(self, input_shape):\n        d_model = int(input_shape[-1])\n        self.position_embedding = self.add_weight(\n            name=\"position_embedding\",\n            shape=(self.sequence_length, d_model),\n            initializer=self.initializer,\n            trainable=True,\n        )\n        super().build(input_shape)\n\n    def call(self, inputs):\n        # (seq_len, d_model) -&gt; (1, seq_len, d_model) for broadcasting over batch\n        return ops.expand_dims(self.position_embedding, axis=0)\n\n    def get_config(self):\n        config = super().get_config()\n        config.update(\n            {\n                \"sequence_length\": self.sequence_length,\n                \"initializer\": initializers.serialize(self.initializer),\n            }\n        )\n        return config\n</code></pre>"},{"location":"api-reference/model_estimators/#centimators.model_estimators.keras_estimators.TransformerRegressor","title":"<code>TransformerRegressor</code>  <code>dataclass</code>","text":"<p>               Bases: <code>RegressorMixin</code>, <code>SequenceEstimator</code></p> <p>Transformer encoder regressor for lagged sequence inputs.</p> <p>Stacks one or more encoder blocks (multi-head attention + feed-forward) over the 3-D tensor produced by :class:<code>SequenceEstimator</code>, then collapses the sequence dimension via pooling before a final MLP prediction head.</p> <p>Three attention modes are available:</p> <ul> <li><code>\"temporal\"</code> -- standard self-attention over timesteps (default).</li> <li><code>\"feature\"</code> -- iTransformer-style attention over the feature axis.</li> <li><code>\"cross\"</code> -- dual-axis attention (temporal + feature combined).</li> </ul> <p>Two pooling strategies collapse the sequence before the MLP head:</p> <ul> <li><code>\"attention\"</code> -- learned weighted pooling (:class:<code>AttentionPooling</code>).</li> <li><code>\"average\"</code> -- global average pooling.</li> </ul>"},{"location":"api-reference/model_estimators/#centimators.model_estimators.keras_estimators.TransformerRegressor--parameters","title":"Parameters","text":"<p>d_model : int     Internal embedding dimension (default: 32). num_heads : int     Number of attention heads (default: 4). ff_dim : int     Feed-forward hidden dimension per encoder block (default: 128). num_blocks : int     Number of stacked encoder blocks (default: 1). dropout_rate : float     Dropout applied in attention and feed-forward layers (default: 0.1). attention_type : str     One of <code>\"temporal\"</code>, <code>\"feature\"</code>, or <code>\"cross\"</code> (default: <code>\"temporal\"</code>). pooling_type : str     One of <code>\"attention\"</code> or <code>\"average\"</code> (default: <code>\"attention\"</code>). use_pre_norm : bool     Apply LayerNorm before (True) or after (False) attention/FFN (default: True). mlp_units : tuple[int, ...]     Hidden layer sizes for the prediction head (default: <code>(64,)</code>).</p> Source code in <code>src/centimators/model_estimators/keras_estimators/transformer.py</code> <pre><code>@dataclass(kw_only=True)\nclass TransformerRegressor(RegressorMixin, SequenceEstimator):\n    \"\"\"Transformer encoder regressor for lagged sequence inputs.\n\n    Stacks one or more encoder blocks (multi-head attention + feed-forward)\n    over the 3-D tensor produced by :class:`SequenceEstimator`, then collapses\n    the sequence dimension via pooling before a final MLP prediction head.\n\n    Three attention modes are available:\n\n    - ``\"temporal\"`` -- standard self-attention over timesteps (default).\n    - ``\"feature\"`` -- iTransformer-style attention over the feature axis.\n    - ``\"cross\"`` -- dual-axis attention (temporal + feature combined).\n\n    Two pooling strategies collapse the sequence before the MLP head:\n\n    - ``\"attention\"`` -- learned weighted pooling (:class:`AttentionPooling`).\n    - ``\"average\"`` -- global average pooling.\n\n    Parameters\n    ----------\n    d_model : int\n        Internal embedding dimension (default: 32).\n    num_heads : int\n        Number of attention heads (default: 4).\n    ff_dim : int\n        Feed-forward hidden dimension per encoder block (default: 128).\n    num_blocks : int\n        Number of stacked encoder blocks (default: 1).\n    dropout_rate : float\n        Dropout applied in attention and feed-forward layers (default: 0.1).\n    attention_type : str\n        One of ``\"temporal\"``, ``\"feature\"``, or ``\"cross\"`` (default: ``\"temporal\"``).\n    pooling_type : str\n        One of ``\"attention\"`` or ``\"average\"`` (default: ``\"attention\"``).\n    use_pre_norm : bool\n        Apply LayerNorm before (True) or after (False) attention/FFN (default: True).\n    mlp_units : tuple[int, ...]\n        Hidden layer sizes for the prediction head (default: ``(64,)``).\n    \"\"\"\n\n    d_model: int = 32\n    num_heads: int = 4\n    ff_dim: int = 128\n    num_blocks: int = 1\n    dropout_rate: float = 0.1\n    attention_type: str = \"temporal\"\n    pooling_type: str = \"attention\"\n    use_pre_norm: bool = True\n    mlp_units: tuple[int, ...] = (64,)\n    metrics: list[str] | None = field(default_factory=lambda: [\"mse\"])\n    target_scaler: Any = field(default_factory=StandardScaler)\n\n    def _encoder_block(self, inputs):\n        x = (\n            layers.LayerNormalization(epsilon=1e-6)(inputs)\n            if self.use_pre_norm\n            else inputs\n        )\n\n        if self.attention_type == \"cross\":\n            key_dim = max(1, self.d_model // self.num_heads)\n            x = CrossAttention(\n                key_dim=key_dim, num_heads=self.num_heads, dropout=self.dropout_rate\n            )(x)\n        elif self.attention_type == \"temporal\":\n            x = layers.MultiHeadAttention(\n                key_dim=max(1, self.d_model // self.num_heads),\n                num_heads=self.num_heads,\n                dropout=self.dropout_rate,\n            )(x, x)\n        elif self.attention_type == \"feature\":\n            # iTransformer-style feature attention.\n            feature_tokens = layers.Permute((2, 1))(x)\n            feature_tokens = layers.MultiHeadAttention(\n                key_dim=max(1, self.seq_length // self.num_heads),\n                num_heads=self.num_heads,\n                dropout=self.dropout_rate,\n            )(feature_tokens, feature_tokens)\n            x = layers.Permute((2, 1))(feature_tokens)\n        else:\n            raise ValueError(\n                f\"Unknown attention_type={self.attention_type!r}. \"\n                \"Use one of {'cross', 'temporal', 'feature'}.\"\n            )\n\n        x = inputs + x\n        ffn_input = (\n            layers.LayerNormalization(epsilon=1e-6)(x) if self.use_pre_norm else x\n        )\n\n        ffn = layers.Dense(self.ff_dim, activation=\"relu\")(ffn_input)\n        ffn = layers.Dropout(self.dropout_rate)(ffn)\n        ffn = layers.Dense(self.d_model)(ffn)\n        ffn = layers.Dropout(self.dropout_rate)(ffn)\n        return x + ffn\n\n    def build_model(self):\n        if self._n_features_in_ is None:\n            raise ValueError(\"Must call fit() before building the model\")\n\n        inputs = layers.Input(\n            shape=(self.seq_length, self.n_features_per_timestep),\n            name=\"sequence_input\",\n        )\n\n        x = layers.Dense(self.d_model)(inputs)\n        x = x + PositionEmbedding(sequence_length=self.seq_length)(x)\n\n        for _ in range(self.num_blocks):\n            x = self._encoder_block(x)\n\n        if self.use_pre_norm:\n            x = layers.LayerNormalization(epsilon=1e-6)(x)\n\n        if self.pooling_type == \"attention\":\n            x = AttentionPooling()(x)\n        elif self.pooling_type == \"average\":\n            x = layers.GlobalAveragePooling1D()(x)\n        else:\n            raise ValueError(\n                f\"Unknown pooling_type={self.pooling_type!r}. Use one of {'attention', 'average'}.\"\n            )\n\n        for units in self.mlp_units:\n            x = layers.Dense(units, activation=\"relu\")(x)\n            x = layers.Dropout(self.dropout_rate)(x)\n\n        outputs = layers.Dense(self.output_units, activation=\"linear\", name=\"output\")(x)\n        self.model = models.Model(\n            inputs=inputs, outputs=outputs, name=\"transformer_regressor\"\n        )\n        self.model.compile(\n            optimizer=self.optimizer(learning_rate=self.learning_rate),\n            loss=self.loss_function,\n            metrics=self.metrics,\n        )\n        return self\n</code></pre>"},{"location":"api-reference/model_estimators/#centimators.model_estimators.keras_estimators.NeuralDecisionForestRegressor","title":"<code>NeuralDecisionForestRegressor</code>  <code>dataclass</code>","text":"<p>               Bases: <code>RegressorMixin</code>, <code>BaseKerasEstimator</code></p> <p>Neural Decision Forest regressor with differentiable tree ensembles.</p> <p>A Neural Decision Forest is an ensemble of differentiable decision trees trained end-to-end via gradient descent. Each tree uses stochastic routing where internal nodes learn probability distributions over routing decisions. The forest combines predictions by averaging over all trees.</p> <p>This architecture provides:</p> <ul> <li>Interpretable tree-like structure with learned routing</li> <li>Feature bagging via used_features_rate (like random forests)</li> <li>End-to-end differentiable training</li> <li>Ensemble averaging for improved generalization</li> <li>Temperature-controlled routing sharpness</li> <li>Input noise, per-tree noise, and tree dropout for ensemble diversity</li> </ul> <p>Parameters:</p> Name Type Description Default <code>num_trees</code> <code>int, default=25</code> <p>Number of decision trees in the forest ensemble.</p> <code>25</code> <code>depth</code> <code>int, default=4</code> <p>Depth of each tree. Each tree will have 2^depth leaf nodes. Deeper trees have more capacity but harder gradient flow.</p> <code>4</code> <code>used_features_rate</code> <code>float, default=0.5</code> <p>Fraction of features each tree randomly selects (0 to 1). Provides feature bagging. Lower values increase diversity.</p> <code>0.5</code> <code>l2_decision</code> <code>float, default=1e-4</code> <p>L2 regularization for routing decision layers. Lower values allow sharper routing decisions.</p> <code>0.0001</code> <code>l2_leaf</code> <code>float, default=1e-3</code> <p>L2 regularization for leaf output weights. Can be stronger than l2_decision since leaves are regression weights.</p> <code>0.001</code> <code>temperature</code> <code>float, default=0.5</code> <p>Temperature for sigmoid sharpness in routing. Lower values (0.3-0.5) give sharper, more tree-like routing. Higher values (1-3) give softer routing where samples flow through multiple paths.</p> <code>0.5</code> <code>input_noise_std</code> <code>float, default=0.0</code> <p>Gaussian noise std applied to inputs before trunk. Makes trunk robust to input perturbations. Try 0.02-0.05.</p> <code>0.0</code> <code>tree_noise_std</code> <code>float, default=0.0</code> <p>Gaussian noise std applied per-tree after trunk. Each tree sees a different noisy view, decorrelating the ensemble. Try 0.03-0.1.</p> <code>0.0</code> <code>tree_dropout_rate</code> <code>float, default=0.0</code> <p>Dropout rate for tree outputs during training (0 to 1). Randomly drops tree contributions to decorrelate ensemble.</p> <code>0.0</code> <code>trunk_units</code> <code>list[int] | None, default=None</code> <p>Hidden layer sizes for optional shared MLP trunk before trees. E.g. [64, 64] adds two Dense+ReLU layers. Trees then split on learned features instead of raw columns.</p> <code>None</code> <code>random_state</code> <code>int | None, default=None</code> <p>Random seed for reproducible feature mask sampling across trees.</p> <code>None</code> <code>output_units</code> <code>int, default=1</code> <p>Number of output targets to predict.</p> <code>1</code> <code>optimizer</code> <code>Type[keras.optimizers.Optimizer], default=Adam</code> <p>Keras optimizer class to use for training.</p> <code>Adam</code> <code>learning_rate</code> <code>float, default=0.001</code> <p>Learning rate for the optimizer.</p> <code>0.001</code> <code>loss_function</code> <code>str, default=\"mse\"</code> <p>Loss function for training.</p> <code>'mse'</code> <code>metrics</code> <code>list[str] | None, default=None</code> <p>List of metrics to track during training.</p> <code>(lambda: ['mse'])()</code> <code>distribution_strategy</code> <code>str | None, default=None</code> <p>Distribution strategy for multi-device training.</p> <code>None</code> <p>Attributes:</p> Name Type Description <code>model</code> <code>Model</code> <p>The compiled Keras model containing the ensemble of trees.</p> <code>trees</code> <code>list[NeuralDecisionTree]</code> <p>List of tree models in the ensemble.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from centimators.model_estimators import NeuralDecisionForestRegressor\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; X = np.random.randn(100, 10).astype('float32')\n&gt;&gt;&gt; y = np.random.randn(100, 1).astype('float32')\n&gt;&gt;&gt; ndf = NeuralDecisionForestRegressor(num_trees=5, depth=4)\n&gt;&gt;&gt; ndf.fit(X, y, epochs=10, verbose=0)\n&gt;&gt;&gt; predictions = ndf.predict(X)\n</code></pre> Note <ul> <li>Larger depth increases model capacity but may lead to overfitting</li> <li>More trees generally improve performance but increase computation</li> <li>Lower used_features_rate increases diversity but may hurt individual tree performance</li> <li>Works well on tabular data where tree-based methods traditionally excel</li> <li>Lower temperature (0.3-0.5) gives sharper, more tree-like routing</li> </ul> <p>The approach is based on Neural Decision Forests and related differentiable tree architectures that enable end-to-end learning of routing decisions.</p> Source code in <code>src/centimators/model_estimators/keras_estimators/tree.py</code> <pre><code>@dataclass(kw_only=True)\nclass NeuralDecisionForestRegressor(RegressorMixin, BaseKerasEstimator):\n    \"\"\"Neural Decision Forest regressor with differentiable tree ensembles.\n\n    A Neural Decision Forest is an ensemble of differentiable decision trees\n    trained end-to-end via gradient descent. Each tree uses stochastic routing\n    where internal nodes learn probability distributions over routing decisions.\n    The forest combines predictions by averaging over all trees.\n\n    This architecture provides:\n\n    - Interpretable tree-like structure with learned routing\n    - Feature bagging via used_features_rate (like random forests)\n    - End-to-end differentiable training\n    - Ensemble averaging for improved generalization\n    - Temperature-controlled routing sharpness\n    - Input noise, per-tree noise, and tree dropout for ensemble diversity\n\n    Args:\n        num_trees (int, default=25): Number of decision trees in the forest ensemble.\n        depth (int, default=4): Depth of each tree. Each tree will have 2^depth leaf nodes.\n            Deeper trees have more capacity but harder gradient flow.\n        used_features_rate (float, default=0.5): Fraction of features each tree randomly\n            selects (0 to 1). Provides feature bagging. Lower values increase diversity.\n        l2_decision (float, default=1e-4): L2 regularization for routing decision layers.\n            Lower values allow sharper routing decisions.\n        l2_leaf (float, default=1e-3): L2 regularization for leaf output weights.\n            Can be stronger than l2_decision since leaves are regression weights.\n        temperature (float, default=0.5): Temperature for sigmoid sharpness in routing.\n            Lower values (0.3-0.5) give sharper, more tree-like routing. Higher values\n            (1-3) give softer routing where samples flow through multiple paths.\n        input_noise_std (float, default=0.0): Gaussian noise std applied to inputs\n            before trunk. Makes trunk robust to input perturbations. Try 0.02-0.05.\n        tree_noise_std (float, default=0.0): Gaussian noise std applied per-tree after\n            trunk. Each tree sees a different noisy view, decorrelating the ensemble.\n            Try 0.03-0.1.\n        tree_dropout_rate (float, default=0.0): Dropout rate for tree outputs during\n            training (0 to 1). Randomly drops tree contributions to decorrelate ensemble.\n        trunk_units (list[int] | None, default=None): Hidden layer sizes for optional\n            shared MLP trunk before trees. E.g. [64, 64] adds two Dense+ReLU layers.\n            Trees then split on learned features instead of raw columns.\n        random_state (int | None, default=None): Random seed for reproducible feature\n            mask sampling across trees.\n        output_units (int, default=1): Number of output targets to predict.\n        optimizer (Type[keras.optimizers.Optimizer], default=Adam): Keras optimizer\n            class to use for training.\n        learning_rate (float, default=0.001): Learning rate for the optimizer.\n        loss_function (str, default=\"mse\"): Loss function for training.\n        metrics (list[str] | None, default=None): List of metrics to track during training.\n        distribution_strategy (str | None, default=None): Distribution strategy for\n            multi-device training.\n\n    Attributes:\n        model (keras.Model): The compiled Keras model containing the ensemble of trees.\n        trees (list[NeuralDecisionTree]): List of tree models in the ensemble.\n\n    Examples:\n        &gt;&gt;&gt; from centimators.model_estimators import NeuralDecisionForestRegressor\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; X = np.random.randn(100, 10).astype('float32')\n        &gt;&gt;&gt; y = np.random.randn(100, 1).astype('float32')\n        &gt;&gt;&gt; ndf = NeuralDecisionForestRegressor(num_trees=5, depth=4)\n        &gt;&gt;&gt; ndf.fit(X, y, epochs=10, verbose=0)\n        &gt;&gt;&gt; predictions = ndf.predict(X)\n\n    Note:\n        - Larger depth increases model capacity but may lead to overfitting\n        - More trees generally improve performance but increase computation\n        - Lower used_features_rate increases diversity but may hurt individual tree performance\n        - Works well on tabular data where tree-based methods traditionally excel\n        - Lower temperature (0.3-0.5) gives sharper, more tree-like routing\n\n        The approach is based on Neural Decision Forests and related differentiable\n        tree architectures that enable end-to-end learning of routing decisions.\n    \"\"\"\n\n    num_trees: int = 25\n    depth: int = 4\n    used_features_rate: float = 0.5\n    l2_decision: float = 1e-4\n    l2_leaf: float = 1e-3\n    temperature: float = 0.5\n    input_noise_std: float = 0.0\n    tree_noise_std: float = 0.0\n    tree_dropout_rate: float = 0.0\n    trunk_units: list[int] | None = None\n    random_state: int | None = None\n    metrics: list[str] | None = field(default_factory=lambda: [\"mse\"])\n    target_scaler: Any = field(default_factory=StandardScaler)\n\n    def __post_init__(self):\n        self.trees: list[NeuralDecisionTree] = []\n\n    def build_model(self):\n        \"\"\"Build the neural decision forest model.\n\n        Creates an ensemble of NeuralDecisionTree models with shared input\n        and averaged output. Each tree receives normalized input features\n        via BatchNormalization. Optionally includes input noise (before trunk\n        for robustness), per-tree noise (for diversity), tree dropout, and\n        a shared MLP trunk.\n\n        Returns:\n            self: Returns self for method chaining.\n        \"\"\"\n        if self.model is None:\n            if self.distribution_strategy:\n                self._setup_distribution_strategy()\n\n            # Set up RNG for reproducibility\n            rng = np.random.default_rng(self.random_state)\n\n            # Input layer\n            inputs = layers.Input(shape=(self._n_features_in_,))\n            x = layers.BatchNormalization()(inputs)\n\n            # Input noise before trunk (makes trunk robust to perturbations)\n            if self.input_noise_std &gt; 0:\n                x = layers.GaussianNoise(self.input_noise_std)(x)\n\n            # Optional shared trunk (MLP before trees)\n            if self.trunk_units:\n                for units in self.trunk_units:\n                    x = layers.Dense(units, activation=\"relu\")(x)\n\n            # Determine feature count for trees (trunk output or raw features)\n            tree_num_features = (\n                self.trunk_units[-1] if self.trunk_units else self._n_features_in_\n            )\n\n            # Create ensemble of trees\n            self.trees = []\n            for _ in range(self.num_trees):\n                tree = NeuralDecisionTree(\n                    depth=self.depth,\n                    num_features=tree_num_features,\n                    used_features_rate=self.used_features_rate,\n                    output_units=self.output_units,\n                    l2_decision=self.l2_decision,\n                    l2_leaf=self.l2_leaf,\n                    temperature=self.temperature,\n                    rng=rng,\n                )\n                self.trees.append(tree)\n\n            # each tree gets its own noisy view for diversity\n            tree_outputs = []\n            for tree in self.trees:\n                if self.tree_noise_std &gt; 0:\n                    noisy_x = layers.GaussianNoise(self.tree_noise_std)(x)\n                    tree_outputs.append(tree(noisy_x))\n                else:\n                    tree_outputs.append(tree(x))\n\n            if len(tree_outputs) &gt; 1:\n                stacked = K.stack(tree_outputs, axis=1)  # [batch, num_trees, out_units]\n                if self.tree_dropout_rate &gt; 0:\n                    # Drop entire trees\n                    stacked = layers.Dropout(\n                        self.tree_dropout_rate,\n                        noise_shape=(\n                            None,\n                            self.num_trees,\n                            1,\n                        ),  # broadcasts so whole tree is dropped\n                    )(stacked)\n                outputs = K.mean(stacked, axis=1)\n            else:\n                outputs = tree_outputs[0]\n\n            self.model = models.Model(inputs=inputs, outputs=outputs)\n            opt = self.optimizer(learning_rate=self.learning_rate)\n            self.model.compile(\n                optimizer=opt, loss=self.loss_function, metrics=self.metrics\n            )\n        return self\n</code></pre>"},{"location":"api-reference/model_estimators/#centimators.model_estimators.keras_estimators.NeuralDecisionForestRegressor.build_model","title":"<code>build_model()</code>","text":"<p>Build the neural decision forest model.</p> <p>Creates an ensemble of NeuralDecisionTree models with shared input and averaged output. Each tree receives normalized input features via BatchNormalization. Optionally includes input noise (before trunk for robustness), per-tree noise (for diversity), tree dropout, and a shared MLP trunk.</p> <p>Returns:</p> Name Type Description <code>self</code> <p>Returns self for method chaining.</p> Source code in <code>src/centimators/model_estimators/keras_estimators/tree.py</code> <pre><code>def build_model(self):\n    \"\"\"Build the neural decision forest model.\n\n    Creates an ensemble of NeuralDecisionTree models with shared input\n    and averaged output. Each tree receives normalized input features\n    via BatchNormalization. Optionally includes input noise (before trunk\n    for robustness), per-tree noise (for diversity), tree dropout, and\n    a shared MLP trunk.\n\n    Returns:\n        self: Returns self for method chaining.\n    \"\"\"\n    if self.model is None:\n        if self.distribution_strategy:\n            self._setup_distribution_strategy()\n\n        # Set up RNG for reproducibility\n        rng = np.random.default_rng(self.random_state)\n\n        # Input layer\n        inputs = layers.Input(shape=(self._n_features_in_,))\n        x = layers.BatchNormalization()(inputs)\n\n        # Input noise before trunk (makes trunk robust to perturbations)\n        if self.input_noise_std &gt; 0:\n            x = layers.GaussianNoise(self.input_noise_std)(x)\n\n        # Optional shared trunk (MLP before trees)\n        if self.trunk_units:\n            for units in self.trunk_units:\n                x = layers.Dense(units, activation=\"relu\")(x)\n\n        # Determine feature count for trees (trunk output or raw features)\n        tree_num_features = (\n            self.trunk_units[-1] if self.trunk_units else self._n_features_in_\n        )\n\n        # Create ensemble of trees\n        self.trees = []\n        for _ in range(self.num_trees):\n            tree = NeuralDecisionTree(\n                depth=self.depth,\n                num_features=tree_num_features,\n                used_features_rate=self.used_features_rate,\n                output_units=self.output_units,\n                l2_decision=self.l2_decision,\n                l2_leaf=self.l2_leaf,\n                temperature=self.temperature,\n                rng=rng,\n            )\n            self.trees.append(tree)\n\n        # each tree gets its own noisy view for diversity\n        tree_outputs = []\n        for tree in self.trees:\n            if self.tree_noise_std &gt; 0:\n                noisy_x = layers.GaussianNoise(self.tree_noise_std)(x)\n                tree_outputs.append(tree(noisy_x))\n            else:\n                tree_outputs.append(tree(x))\n\n        if len(tree_outputs) &gt; 1:\n            stacked = K.stack(tree_outputs, axis=1)  # [batch, num_trees, out_units]\n            if self.tree_dropout_rate &gt; 0:\n                # Drop entire trees\n                stacked = layers.Dropout(\n                    self.tree_dropout_rate,\n                    noise_shape=(\n                        None,\n                        self.num_trees,\n                        1,\n                    ),  # broadcasts so whole tree is dropped\n                )(stacked)\n            outputs = K.mean(stacked, axis=1)\n        else:\n            outputs = tree_outputs[0]\n\n        self.model = models.Model(inputs=inputs, outputs=outputs)\n        opt = self.optimizer(learning_rate=self.learning_rate)\n        self.model.compile(\n            optimizer=opt, loss=self.loss_function, metrics=self.metrics\n        )\n    return self\n</code></pre>"},{"location":"api-reference/model_estimators/#centimators.model_estimators.keras_estimators.TemperatureAnnealing","title":"<code>TemperatureAnnealing</code>","text":"<p>               Bases: <code>Callback</code></p> <p>Anneal tree routing temperature from soft to sharp over training.</p> <p>Starts with high temperature (soft routing, samples flow through many paths) and linearly decreases to low temperature (sharp routing, more tree-like). This can theoretically help training converge to better solutions.</p> <p>Parameters:</p> Name Type Description Default <code>ndf</code> <code>NeuralDecisionForestRegressor</code> <p>The forest instance whose trees will be annealed.</p> required <code>start</code> <code>float, default=2.0</code> <p>Starting temperature (soft routing).</p> <code>2.0</code> <code>end</code> <code>float, default=0.5</code> <p>Ending temperature (sharp routing).</p> <code>0.5</code> <code>epochs</code> <code>int, default=50</code> <p>Total epochs over which to anneal. Should match fit() epochs.</p> <code>50</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; ndf = NeuralDecisionForestRegressor(temperature=2.0)\n&gt;&gt;&gt; annealer = TemperatureAnnealing(ndf, start=2.0, end=0.5, epochs=50)\n&gt;&gt;&gt; ndf.fit(X, y, epochs=50, callbacks=[annealer])\n</code></pre> Source code in <code>src/centimators/model_estimators/keras_estimators/tree.py</code> <pre><code>class TemperatureAnnealing(callbacks.Callback):\n    \"\"\"Anneal tree routing temperature from soft to sharp over training.\n\n    Starts with high temperature (soft routing, samples flow through many paths)\n    and linearly decreases to low temperature (sharp routing, more tree-like).\n    This can theoretically help training converge to better solutions.\n\n    Args:\n        ndf (NeuralDecisionForestRegressor): The forest instance whose trees will be annealed.\n        start (float, default=2.0): Starting temperature (soft routing).\n        end (float, default=0.5): Ending temperature (sharp routing).\n        epochs (int, default=50): Total epochs over which to anneal. Should match fit() epochs.\n\n    Examples:\n        &gt;&gt;&gt; ndf = NeuralDecisionForestRegressor(temperature=2.0)\n        &gt;&gt;&gt; annealer = TemperatureAnnealing(ndf, start=2.0, end=0.5, epochs=50)\n        &gt;&gt;&gt; ndf.fit(X, y, epochs=50, callbacks=[annealer])\n    \"\"\"\n\n    def __init__(self, ndf, start: float = 2.0, end: float = 0.5, epochs: int = 50):\n        super().__init__()\n        self.ndf = ndf\n        self.start = start\n        self.end = end\n        self.epochs = epochs\n\n    def on_epoch_end(self, epoch, logs=None):\n        t = self.start - (self.start - self.end) * ((epoch + 1) / self.epochs)\n        for tree in self.ndf.trees:\n            tree.temperature.assign(t)\n</code></pre>"},{"location":"scripts/visualize_feature_exposure/","title":"Visualize feature exposure","text":"In\u00a0[\u00a0]: Copied! <pre>\"\"\"\nGenerate feature exposure visualizations for documentation.\n\nCreates before/after charts for:\n- FeatureNeutralizer (neutralization.png)\n- FeaturePenalizer (penalization.png)\n\nUsage:\n    uv run scripts/visualize_feature_exposure.py\n\"\"\"\n</pre> \"\"\" Generate feature exposure visualizations for documentation.  Creates before/after charts for: - FeatureNeutralizer (neutralization.png) - FeaturePenalizer (penalization.png)  Usage:     uv run scripts/visualize_feature_exposure.py \"\"\" In\u00a0[\u00a0]: Copied! <pre>import os\n</pre> import os In\u00a0[\u00a0]: Copied! <pre>import matplotlib.pyplot as plt\nimport numpy as np\nimport polars as pl\n</pre> import matplotlib.pyplot as plt import numpy as np import polars as pl In\u00a0[\u00a0]: Copied! <pre>os.environ[\"JAX_PLATFORMS\"] = \"cpu\"\n</pre> os.environ[\"JAX_PLATFORMS\"] = \"cpu\" In\u00a0[\u00a0]: Copied! <pre>from centimators.feature_transformers import FeatureNeutralizer, FeaturePenalizer\n</pre> from centimators.feature_transformers import FeatureNeutralizer, FeaturePenalizer In\u00a0[\u00a0]: Copied! <pre># Centimators theme\nCOLORS = {\n    \"before\": \"#E57373\",\n    \"after\": \"#62e4fb\",\n    \"bg\": \"#181A2A\",\n    \"text\": \"#ffffff\",\n    \"muted\": \"#8892a0\",\n}\n</pre> # Centimators theme COLORS = {     \"before\": \"#E57373\",     \"after\": \"#62e4fb\",     \"bg\": \"#181A2A\",     \"text\": \"#ffffff\",     \"muted\": \"#8892a0\", } In\u00a0[\u00a0]: Copied! <pre>FEATURES = [\n    \"momentum\",\n    \"volatility\",\n    \"value\",\n    \"size\",\n    \"quality\",\n    \"sentiment\",\n    \"liquidity\",\n    \"beta\",\n]\nOUTPUT_DIR = \"overrides/assets/images\"\n</pre> FEATURES = [     \"momentum\",     \"volatility\",     \"value\",     \"size\",     \"quality\",     \"sentiment\",     \"liquidity\",     \"beta\", ] OUTPUT_DIR = \"overrides/assets/images\" In\u00a0[\u00a0]: Copied! <pre>def compute_exposure(predictions: np.ndarray, features: np.ndarray) -&gt; np.ndarray:\n    \"\"\"Compute correlation between predictions and each feature.\"\"\"\n    pred = predictions - predictions.mean()\n    pred = pred / np.linalg.norm(pred)\n    feats = features - features.mean(axis=0)\n    feats = feats / np.linalg.norm(feats, axis=0)\n    return feats.T @ pred\n</pre> def compute_exposure(predictions: np.ndarray, features: np.ndarray) -&gt; np.ndarray:     \"\"\"Compute correlation between predictions and each feature.\"\"\"     pred = predictions - predictions.mean()     pred = pred / np.linalg.norm(pred)     feats = features - features.mean(axis=0)     feats = feats / np.linalg.norm(feats, axis=0)     return feats.T @ pred In\u00a0[\u00a0]: Copied! <pre>def generate_data(n_samples: int = 2000, seed: int = 42) -&gt; pl.DataFrame:\n    \"\"\"Generate synthetic data with known feature exposures.\"\"\"\n    np.random.seed(seed)\n    features = np.random.randn(n_samples, len(FEATURES))\n    weights = np.array([0.8, -0.6, 0.5, -0.4, 0.3, -0.2, 0.15, -0.1])\n    predictions = features @ weights + np.random.randn(n_samples) * 0.5\n    predictions = (predictions - predictions.min()) / (\n        predictions.max() - predictions.min()\n    )\n\n    return pl.DataFrame(\n        {\n            \"era\": [\"era1\"] * n_samples,\n            \"prediction\": predictions,\n            **{name: features[:, i] for i, name in enumerate(FEATURES)},\n        }\n    )\n</pre> def generate_data(n_samples: int = 2000, seed: int = 42) -&gt; pl.DataFrame:     \"\"\"Generate synthetic data with known feature exposures.\"\"\"     np.random.seed(seed)     features = np.random.randn(n_samples, len(FEATURES))     weights = np.array([0.8, -0.6, 0.5, -0.4, 0.3, -0.2, 0.15, -0.1])     predictions = features @ weights + np.random.randn(n_samples) * 0.5     predictions = (predictions - predictions.min()) / (         predictions.max() - predictions.min()     )      return pl.DataFrame(         {             \"era\": [\"era1\"] * n_samples,             \"prediction\": predictions,             **{name: features[:, i] for i, name in enumerate(FEATURES)},         }     ) In\u00a0[\u00a0]: Copied! <pre>def create_chart(\n    exp_before: np.ndarray,\n    exp_after: np.ndarray,\n    title: str,\n    after_label: str,\n    threshold: float | None,\n    save_path: str,\n):\n    \"\"\"Create a before/after bar chart.\"\"\"\n    plt.rcParams.update({\"font.family\": \"sans-serif\", \"font.size\": 11})\n\n    fig, ax = plt.subplots(figsize=(12, 5), facecolor=COLORS[\"bg\"])\n    ax.set_facecolor(COLORS[\"bg\"])\n\n    n = len(FEATURES)\n    x = np.arange(n)\n    w = 0.35\n\n    ax.bar(x - w / 2, exp_before, w, color=COLORS[\"before\"], label=\"Before\", alpha=0.9)\n    ax.bar(x + w / 2, exp_after, w, color=COLORS[\"after\"], label=after_label, alpha=0.9)\n\n    # Threshold lines (if applicable)\n    if threshold:\n        ax.axhline(threshold, color=COLORS[\"after\"], lw=1.5, ls=\"--\", alpha=0.6)\n        ax.axhline(-threshold, color=COLORS[\"after\"], lw=1.5, ls=\"--\", alpha=0.6)\n        ax.text(\n            n - 0.5,\n            threshold + 0.02,\n            f\"\u00b1{threshold} cap\",\n            ha=\"right\",\n            fontsize=9,\n            color=COLORS[\"after\"],\n            alpha=0.8,\n        )\n\n    ax.axhline(0, color=COLORS[\"muted\"], lw=0.5, alpha=0.3)\n\n    for spine in ax.spines.values():\n        spine.set_visible(False)\n\n    ax.set_yticks([-0.5, -0.25, 0, 0.25, 0.5])\n    ax.tick_params(axis=\"y\", colors=COLORS[\"muted\"], length=0)\n    ax.tick_params(axis=\"x\", colors=COLORS[\"text\"], length=0)\n    ax.set_ylabel(\"Feature Exposure\", color=COLORS[\"text\"], fontsize=11)\n    ax.set_xticks(x)\n    ax.set_xticklabels(FEATURES, color=COLORS[\"text\"])\n    ax.set_title(title, color=COLORS[\"text\"], fontsize=14, fontweight=\"bold\", pad=12)\n    ax.legend(loc=\"upper right\", frameon=False, fontsize=10, labelcolor=COLORS[\"text\"])\n    ax.set_xlim(-0.5, n - 0.5)\n    ax.set_ylim(-0.7, 0.7)\n\n    plt.tight_layout()\n    plt.savefig(save_path, dpi=150, facecolor=COLORS[\"bg\"], bbox_inches=\"tight\")\n    plt.close()\n    print(f\"Saved: {save_path}\")\n</pre> def create_chart(     exp_before: np.ndarray,     exp_after: np.ndarray,     title: str,     after_label: str,     threshold: float | None,     save_path: str, ):     \"\"\"Create a before/after bar chart.\"\"\"     plt.rcParams.update({\"font.family\": \"sans-serif\", \"font.size\": 11})      fig, ax = plt.subplots(figsize=(12, 5), facecolor=COLORS[\"bg\"])     ax.set_facecolor(COLORS[\"bg\"])      n = len(FEATURES)     x = np.arange(n)     w = 0.35      ax.bar(x - w / 2, exp_before, w, color=COLORS[\"before\"], label=\"Before\", alpha=0.9)     ax.bar(x + w / 2, exp_after, w, color=COLORS[\"after\"], label=after_label, alpha=0.9)      # Threshold lines (if applicable)     if threshold:         ax.axhline(threshold, color=COLORS[\"after\"], lw=1.5, ls=\"--\", alpha=0.6)         ax.axhline(-threshold, color=COLORS[\"after\"], lw=1.5, ls=\"--\", alpha=0.6)         ax.text(             n - 0.5,             threshold + 0.02,             f\"\u00b1{threshold} cap\",             ha=\"right\",             fontsize=9,             color=COLORS[\"after\"],             alpha=0.8,         )      ax.axhline(0, color=COLORS[\"muted\"], lw=0.5, alpha=0.3)      for spine in ax.spines.values():         spine.set_visible(False)      ax.set_yticks([-0.5, -0.25, 0, 0.25, 0.5])     ax.tick_params(axis=\"y\", colors=COLORS[\"muted\"], length=0)     ax.tick_params(axis=\"x\", colors=COLORS[\"text\"], length=0)     ax.set_ylabel(\"Feature Exposure\", color=COLORS[\"text\"], fontsize=11)     ax.set_xticks(x)     ax.set_xticklabels(FEATURES, color=COLORS[\"text\"])     ax.set_title(title, color=COLORS[\"text\"], fontsize=14, fontweight=\"bold\", pad=12)     ax.legend(loc=\"upper right\", frameon=False, fontsize=10, labelcolor=COLORS[\"text\"])     ax.set_xlim(-0.5, n - 0.5)     ax.set_ylim(-0.7, 0.7)      plt.tight_layout()     plt.savefig(save_path, dpi=150, facecolor=COLORS[\"bg\"], bbox_inches=\"tight\")     plt.close()     print(f\"Saved: {save_path}\") In\u00a0[\u00a0]: Copied! <pre>def main():\n    os.makedirs(OUTPUT_DIR, exist_ok=True)\n\n    df = generate_data()\n    predictions = df[\"prediction\"].to_numpy()\n    features = df.select(FEATURES).to_numpy()\n    era_series = df[\"era\"]\n\n    exp_before = compute_exposure(predictions, features)\n    print(f\"Original max |exposure|: {np.abs(exp_before).max():.2f}\")\n\n    # --- Neutralization chart ---\n    proportion = 0.5\n    neutralizer = FeatureNeutralizer(\n        proportion=proportion, pred_name=\"prediction\", feature_names=FEATURES\n    )\n    neut_df = neutralizer.fit_transform(\n        df[[\"prediction\"]], features=df.select(FEATURES), era_series=era_series\n    )\n    exp_neutralized = compute_exposure(neut_df.to_numpy().squeeze(), features)\n    print(\n        f\"Neutralized ({proportion}) max |exposure|: {np.abs(exp_neutralized).max():.2f}\"\n    )\n\n    create_chart(\n        exp_before,\n        exp_neutralized,\n        title=\"Feature Neutralization\",\n        after_label=f\"Neutralized ({int(proportion * 100)}%)\",\n        threshold=None,\n        save_path=f\"{OUTPUT_DIR}/neutralization.png\",\n    )\n\n    # --- Penalization chart ---\n    max_exp = 0.1\n    penalizer = FeaturePenalizer(\n        max_exposure=max_exp, pred_name=\"prediction\", feature_names=FEATURES\n    )\n    pen_df = penalizer.fit_transform(\n        df[[\"prediction\"]], features=df.select(FEATURES), era_series=era_series\n    )\n    exp_penalized = compute_exposure(pen_df.to_numpy().squeeze(), features)\n    print(f\"Penalized ({max_exp}) max |exposure|: {np.abs(exp_penalized).max():.2f}\")\n\n    create_chart(\n        exp_before,\n        exp_penalized,\n        title=\"Feature Penalization\",\n        after_label=f\"Penalized (max={max_exp})\",\n        threshold=max_exp,\n        save_path=f\"{OUTPUT_DIR}/penalization.png\",\n    )\n</pre> def main():     os.makedirs(OUTPUT_DIR, exist_ok=True)      df = generate_data()     predictions = df[\"prediction\"].to_numpy()     features = df.select(FEATURES).to_numpy()     era_series = df[\"era\"]      exp_before = compute_exposure(predictions, features)     print(f\"Original max |exposure|: {np.abs(exp_before).max():.2f}\")      # --- Neutralization chart ---     proportion = 0.5     neutralizer = FeatureNeutralizer(         proportion=proportion, pred_name=\"prediction\", feature_names=FEATURES     )     neut_df = neutralizer.fit_transform(         df[[\"prediction\"]], features=df.select(FEATURES), era_series=era_series     )     exp_neutralized = compute_exposure(neut_df.to_numpy().squeeze(), features)     print(         f\"Neutralized ({proportion}) max |exposure|: {np.abs(exp_neutralized).max():.2f}\"     )      create_chart(         exp_before,         exp_neutralized,         title=\"Feature Neutralization\",         after_label=f\"Neutralized ({int(proportion * 100)}%)\",         threshold=None,         save_path=f\"{OUTPUT_DIR}/neutralization.png\",     )      # --- Penalization chart ---     max_exp = 0.1     penalizer = FeaturePenalizer(         max_exposure=max_exp, pred_name=\"prediction\", feature_names=FEATURES     )     pen_df = penalizer.fit_transform(         df[[\"prediction\"]], features=df.select(FEATURES), era_series=era_series     )     exp_penalized = compute_exposure(pen_df.to_numpy().squeeze(), features)     print(f\"Penalized ({max_exp}) max |exposure|: {np.abs(exp_penalized).max():.2f}\")      create_chart(         exp_before,         exp_penalized,         title=\"Feature Penalization\",         after_label=f\"Penalized (max={max_exp})\",         threshold=max_exp,         save_path=f\"{OUTPUT_DIR}/penalization.png\",     ) In\u00a0[\u00a0]: Copied! <pre>if __name__ == \"__main__\":\n    main()\n</pre> if __name__ == \"__main__\":     main()"},{"location":"tutorials/dspymator/","title":"DSPyMator Tutorial","text":"In\u00a0[1]: Copied! <pre># !pip install centimators[all] datasets cluestar\n</pre> # !pip install centimators[all] datasets cluestar In\u00a0[2]: Copied! <pre># import os\n# os.environ[\"OPENAI_API_KEY\"] = \"sk-proj-...\"\nimport polars as pl\nfrom datasets import load_dataset\n\n# Load the Rotten Tomatoes movie review dataset from Hugging Face\nprint(\"Loading Rotten Tomatoes dataset...\")\ndataset = load_dataset(\"rotten_tomatoes\")\n\n# Convert to polars and take a subset for faster execution\n# Using 300 samples for training and 50 for testing\ntrain_data = dataset[\"train\"].shuffle(seed=42).select(range(300))\ntest_data = dataset[\"test\"].shuffle(seed=42).select(range(100))\n\n# Convert to polars DataFrames\ntrain_df = pl.DataFrame(\n    {\n        \"review\": train_data[\"text\"],\n        \"sentiment\": [\n            \"positive\" if label == 1 else \"negative\" for label in train_data[\"label\"]\n        ],\n    }\n)\n\ntest_df = pl.DataFrame(\n    {\n        \"review\": test_data[\"text\"],\n        \"sentiment\": [\n            \"positive\" if label == 1 else \"negative\" for label in test_data[\"label\"]\n        ],\n    }\n)\n\n# Prepare X and y\nX_train = train_df[[\"review\"]]\ny_train = train_df[\"sentiment\"]\n\nX_test = test_df[[\"review\"]]\ny_test = test_df[\"sentiment\"]\n\nprint(\"\\n\u2713 Dataset loaded successfully!\")\nprint(f\"Training samples: {len(X_train)}\")\nprint(f\"Test samples: {len(X_test)}\")\nprint(\"\\nSentiment distribution (train):\")\ndisplay(y_train.value_counts())\nprint(\"\\nSample reviews:\")\ntrain_df.head(3)\n</pre> # import os # os.environ[\"OPENAI_API_KEY\"] = \"sk-proj-...\" import polars as pl from datasets import load_dataset  # Load the Rotten Tomatoes movie review dataset from Hugging Face print(\"Loading Rotten Tomatoes dataset...\") dataset = load_dataset(\"rotten_tomatoes\")  # Convert to polars and take a subset for faster execution # Using 300 samples for training and 50 for testing train_data = dataset[\"train\"].shuffle(seed=42).select(range(300)) test_data = dataset[\"test\"].shuffle(seed=42).select(range(100))  # Convert to polars DataFrames train_df = pl.DataFrame(     {         \"review\": train_data[\"text\"],         \"sentiment\": [             \"positive\" if label == 1 else \"negative\" for label in train_data[\"label\"]         ],     } )  test_df = pl.DataFrame(     {         \"review\": test_data[\"text\"],         \"sentiment\": [             \"positive\" if label == 1 else \"negative\" for label in test_data[\"label\"]         ],     } )  # Prepare X and y X_train = train_df[[\"review\"]] y_train = train_df[\"sentiment\"]  X_test = test_df[[\"review\"]] y_test = test_df[\"sentiment\"]  print(\"\\n\u2713 Dataset loaded successfully!\") print(f\"Training samples: {len(X_train)}\") print(f\"Test samples: {len(X_test)}\") print(\"\\nSentiment distribution (train):\") display(y_train.value_counts()) print(\"\\nSample reviews:\") train_df.head(3) <pre>Loading Rotten Tomatoes dataset...\n\n\u2713 Dataset loaded successfully!\nTraining samples: 300\nTest samples: 100\n\nSentiment distribution (train):\n</pre> shape: (2, 2)sentimentcountstru32\"negative\"146\"positive\"154 <pre>\nSample reviews:\n</pre> Out[2]: shape: (3, 2)reviewsentimentstrstr\". . . plays like somebody spli\u2026\"negative\"\"michael moore has perfected th\u2026\"positive\"\". . . too gory to be a comedy \u2026\"negative\" In\u00a0[3]: Copied! <pre>import dspy\nfrom centimators.model_estimators import DSPyMator\n\n# Define a DSPy program with input -&gt; output signature\n# Format: \"input_field: type -&gt; output_field: type\"\nsentiment_program = dspy.Predict(\"review: str -&gt; sentiment: str\")\n\n# Create the DSPyMator estimator\nclassifier = DSPyMator(\n    program=sentiment_program,\n    target_names=\"sentiment\",  # Which output field to use as prediction\n)\n\nclassifier.fit(X_train, y_train)  # establishes LM configuration\n\n# Predict sentiments\npredictions = classifier.predict(X_test)\n\n# Create a results dataframe\nresults = pl.DataFrame(\n    {\"review\": X_test[\"review\"], \"predicted\": predictions, \"actual\": y_test}\n)\n\nprint(\"\\nPrediction Results:\")\ndisplay(results)\n\n# Calculate accuracy\naccuracy = (results[\"actual\"] == results[\"predicted\"]).sum() / len(results)\nprint(f\"\\nAccuracy: {accuracy:.2%}\")\n</pre> import dspy from centimators.model_estimators import DSPyMator  # Define a DSPy program with input -&gt; output signature # Format: \"input_field: type -&gt; output_field: type\" sentiment_program = dspy.Predict(\"review: str -&gt; sentiment: str\")  # Create the DSPyMator estimator classifier = DSPyMator(     program=sentiment_program,     target_names=\"sentiment\",  # Which output field to use as prediction )  classifier.fit(X_train, y_train)  # establishes LM configuration  # Predict sentiments predictions = classifier.predict(X_test)  # Create a results dataframe results = pl.DataFrame(     {\"review\": X_test[\"review\"], \"predicted\": predictions, \"actual\": y_test} )  print(\"\\nPrediction Results:\") display(results)  # Calculate accuracy accuracy = (results[\"actual\"] == results[\"predicted\"]).sum() / len(results) print(f\"\\nAccuracy: {accuracy:.2%}\") <pre>DSPyMator predicting: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 100/100 [00:00&lt;00:00, 379.11it/s]</pre> <pre>\nPrediction Results:\n</pre> <pre>\n</pre> shape: (100, 3)reviewpredictedactualstrstrstr\"unpretentious , charming , qui\u2026\"positive\"\"positive\"\"a film really has to be except\u2026\"negative\"\"negative\"\"working from a surprisingly se\u2026\"positive\"\"positive\"\"it may not be particularly inn\u2026\"positive\"\"positive\"\"such a premise is ripe for all\u2026\"negative\"\"negative\"\u2026\u2026\u2026\"ice age is the first computer-\u2026\"negative\"\"negative\"\"there's no denying that burns \u2026\"Positive\"\"positive\"\"it collapses when mr . taylor \u2026\"negative\"\"negative\"\"there's a great deal of corny \u2026\"positive\"\"positive\"\"ah , the travails of metropoli\u2026\"negative\"\"negative\" <pre>\nAccuracy: 79.00%\n</pre> In\u00a0[4]: Copied! <pre># Wrap the same signature as before but with a ChainOfThought (adds reasoning step)\ncot_program = dspy.ChainOfThought(\"review: str -&gt; sentiment: str\")\n\ncot_classifier = DSPyMator(program=cot_program, target_names=\"sentiment\")\n\n# Fit and transform to get reasoning\noutputs_with_reasoning = cot_classifier.fit_transform(X_test)\n\n# Create a results dataframe\nresults = pl.DataFrame(\n    {\n        \"review\": X_test[\"review\"],\n        \"reasoning\": outputs_with_reasoning[\"reasoning\"],\n        \"predicted\": outputs_with_reasoning[\"sentiment\"],\n        \"actual\": y_test,\n    }\n)\n\nprint(\"\\nPrediction Results:\")\ndisplay(results)\n\n# Calculate accuracy\naccuracy = (results[\"actual\"] == results[\"predicted\"]).sum() / len(results)\nprint(f\"\\nAccuracy: {accuracy:.2%}\")\n</pre> # Wrap the same signature as before but with a ChainOfThought (adds reasoning step) cot_program = dspy.ChainOfThought(\"review: str -&gt; sentiment: str\")  cot_classifier = DSPyMator(program=cot_program, target_names=\"sentiment\")  # Fit and transform to get reasoning outputs_with_reasoning = cot_classifier.fit_transform(X_test)  # Create a results dataframe results = pl.DataFrame(     {         \"review\": X_test[\"review\"],         \"reasoning\": outputs_with_reasoning[\"reasoning\"],         \"predicted\": outputs_with_reasoning[\"sentiment\"],         \"actual\": y_test,     } )  print(\"\\nPrediction Results:\") display(results)  # Calculate accuracy accuracy = (results[\"actual\"] == results[\"predicted\"]).sum() / len(results) print(f\"\\nAccuracy: {accuracy:.2%}\") <pre>DSPyMator predicting: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 100/100 [00:00&lt;00:00, 1715.87it/s]</pre> <pre>\nPrediction Results:\n</pre> <pre>\n</pre> shape: (100, 4)reviewreasoningpredictedactualstrstrstrstr\"unpretentious , charming , qui\u2026\"This short review uses multipl\u2026\"positive\"\"positive\"\"a film really has to be except\u2026\"This review claims the film is\u2026\"negative\"\"negative\"\"working from a surprisingly se\u2026\"The reviewer describes the scr\u2026\"positive\"\"positive\"\"it may not be particularly inn\u2026\"The reviewer notes a lack of i\u2026\"positive\"\"positive\"\"such a premise is ripe for all\u2026\"The review expresses a negativ\u2026\"negative\"\"negative\"\u2026\u2026\u2026\u2026\"ice age is the first computer-\u2026\"The review criticizes the paci\u2026\"negative\"\"negative\"\"there's no denying that burns \u2026\"The review expresses praise an\u2026\"positive\"\"positive\"\"it collapses when mr . taylor \u2026\"The reviewer criticizes the sh\u2026\"negative\"\"negative\"\"there's a great deal of corny \u2026\"The reviewer acknowledges flaw\u2026\"Positive\"\"positive\"\"ah , the travails of metropoli\u2026\"The reviewer expresses dissati\u2026\"negative\"\"negative\" <pre>\nAccuracy: 77.00%\n</pre> In\u00a0[\u00a0]: hide-output Copied! <pre># Define a metric function for optimization\ndef sentiment_metric(gold, pred, trace=None, pred_name=None, pred_trace=None):\n    \"\"\"\n    GEPA-compatible metric that returns score and textual feedback.\n\n    Args:\n        gold: The ground truth example\n        pred: The predicted output\n        trace: Optional full program trace\n        pred_name: Optional name of predictor being optimized\n        pred_trace: Optional trace of specific predictor\n\n    Returns:\n        float score or dspy.Prediction(score=float, feedback=str)\n    \"\"\"\n    y_pred = pred.sentiment\n    y_true = gold.sentiment\n    is_correct = y_pred == y_true\n    score = 1.0 if is_correct else 0.0\n\n    # If GEPA is requesting predictor-level feedback, provide rich guidance\n    if pred_name:\n        if is_correct:\n            feedback = f\"Correctly classified as {y_pred}.\"\n        else:\n            feedback = (\n                f\"Incorrect prediction. Predicted '{y_pred}' but actual was '{y_true}'. \"\n                f\"Review text: '{gold.review}'\"\n            )\n\n        # Add reasoning context if available\n        if hasattr(pred, \"reasoning\"):\n            feedback += f\" Reasoning: {pred.reasoning}\"\n\n        return dspy.Prediction(score=score, feedback=feedback)\n\n    return score\n\n\n# Create a light/constrained GEPA optimizer for faster results and demo\ngepa_optimizer = dspy.teleprompt.GEPA(\n    metric=sentiment_metric,\n    auto=\"light\",\n    reflection_minibatch_size=10,\n    reflection_lm=dspy.LM(model=\"openai/gpt-5-nano\", temperature=1.0, max_tokens=16000),\n)\n\n# Fit with optimization\nprint(\n    \"Starting GEPA Optimization (this may take a long time and cost a lot of money)...\"\n)\npreoptimized_instructions = cot_classifier.signature_.instructions\ncot_classifier.fit(\n    X_train,\n    y_train,\n    optimizer=gepa_optimizer,\n    validation_data=0.3,  # Use 30% of training data for validation\n)\n\nprint(\"\\n\u2713 Optimization complete!\")\n</pre> # Define a metric function for optimization def sentiment_metric(gold, pred, trace=None, pred_name=None, pred_trace=None):     \"\"\"     GEPA-compatible metric that returns score and textual feedback.      Args:         gold: The ground truth example         pred: The predicted output         trace: Optional full program trace         pred_name: Optional name of predictor being optimized         pred_trace: Optional trace of specific predictor      Returns:         float score or dspy.Prediction(score=float, feedback=str)     \"\"\"     y_pred = pred.sentiment     y_true = gold.sentiment     is_correct = y_pred == y_true     score = 1.0 if is_correct else 0.0      # If GEPA is requesting predictor-level feedback, provide rich guidance     if pred_name:         if is_correct:             feedback = f\"Correctly classified as {y_pred}.\"         else:             feedback = (                 f\"Incorrect prediction. Predicted '{y_pred}' but actual was '{y_true}'. \"                 f\"Review text: '{gold.review}'\"             )          # Add reasoning context if available         if hasattr(pred, \"reasoning\"):             feedback += f\" Reasoning: {pred.reasoning}\"          return dspy.Prediction(score=score, feedback=feedback)      return score   # Create a light/constrained GEPA optimizer for faster results and demo gepa_optimizer = dspy.teleprompt.GEPA(     metric=sentiment_metric,     auto=\"light\",     reflection_minibatch_size=10,     reflection_lm=dspy.LM(model=\"openai/gpt-5-nano\", temperature=1.0, max_tokens=16000), )  # Fit with optimization print(     \"Starting GEPA Optimization (this may take a long time and cost a lot of money)...\" ) preoptimized_instructions = cot_classifier.signature_.instructions cot_classifier.fit(     X_train,     y_train,     optimizer=gepa_optimizer,     validation_data=0.3,  # Use 30% of training data for validation )  print(\"\\n\u2713 Optimization complete!\") In\u00a0[6]: Copied! <pre>optimized_predictions = cot_classifier.predict(X_test)\noptimized_results = pl.DataFrame(\n    {\"review\": X_test[\"review\"], \"actual\": y_test, \"predicted\": optimized_predictions}\n)\n\noptimized_accuracy = (\n    optimized_results[\"actual\"] == optimized_results[\"predicted\"]\n).sum() / len(optimized_results)\n\nprint(f\"Preoptimized accuracy: {accuracy:.2%}\")\nprint(f\"Preoptimized instructions: {preoptimized_instructions} \\n\")\nprint(f\"Optimized accuracy: {optimized_accuracy:.2%}\")\nprint(\n    f\"Optimized instructions (first 750 characters): {cot_classifier.signature_.instructions[:750]}\"\n)\n</pre> optimized_predictions = cot_classifier.predict(X_test) optimized_results = pl.DataFrame(     {\"review\": X_test[\"review\"], \"actual\": y_test, \"predicted\": optimized_predictions} )  optimized_accuracy = (     optimized_results[\"actual\"] == optimized_results[\"predicted\"] ).sum() / len(optimized_results)  print(f\"Preoptimized accuracy: {accuracy:.2%}\") print(f\"Preoptimized instructions: {preoptimized_instructions} \\n\") print(f\"Optimized accuracy: {optimized_accuracy:.2%}\") print(     f\"Optimized instructions (first 750 characters): {cot_classifier.signature_.instructions[:750]}\" ) <pre>DSPyMator predicting: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 100/100 [00:00&lt;00:00, 551.67it/s]\n</pre> <pre>Preoptimized accuracy: 77.00%\nPreoptimized instructions: Given the fields `review`, produce the fields `sentiment`. \n\nOptimized accuracy: 89.00%\nOptimized instructions (first 750 characters): New instruction for binary sentiment classification of film reviews\n\nTask\n- Determine the overall sentiment toward the film described in a single English review.\n- Output exactly one field:\n  sentiment: \"positive\" or \"negative\" (all lowercase, no quotes beyond the field value).\n- Do not include any other fields, text, or explanations.\n\nInput\n- review: A single English text review of a film. It may contain punctuation, quotes, references to acting, directing, plot, cinematography, etc.\n\nOutput format\n- Only the line: sentiment: positive\n  or: sentiment: negative\n- Do not prepend, append, or include any reasoning, justification, or extraneous text.\n\nDecision rules\n1) Overall sentiment\n   - If the review expresses clear praise or a positive ov\n</pre> In\u00a0[7]: Copied! <pre>from sklearn.pipeline import make_pipeline\nfrom centimators.feature_transformers import EmbeddingTransformer, DimReducer\n\ndspymator = DSPyMator(\n    program=dspy.ChainOfThought(\"review: str -&gt; sentiment: str\"),\n    target_names=\"sentiment\",\n)\n\n# Create an embedder that embeds the reasoning text\nembedder = EmbeddingTransformer(\n    model=\"openai/text-embedding-3-small\",\n    feature_names=[\"reasoning\"],  # Embed the reasoning field\n)\n\n# Create a dimensionality reducer\ndim_reducer = DimReducer(\n    method=\"umap\",\n    n_components=2,  # Reduce embeddings to 2D for visualization\n)\n\n# Build the pipeline\npipeline = make_pipeline(dspymator, embedder, dim_reducer)\n\nprint(\"Pipeline created:\")\ndisplay(pipeline)\n</pre> from sklearn.pipeline import make_pipeline from centimators.feature_transformers import EmbeddingTransformer, DimReducer  dspymator = DSPyMator(     program=dspy.ChainOfThought(\"review: str -&gt; sentiment: str\"),     target_names=\"sentiment\", )  # Create an embedder that embeds the reasoning text embedder = EmbeddingTransformer(     model=\"openai/text-embedding-3-small\",     feature_names=[\"reasoning\"],  # Embed the reasoning field )  # Create a dimensionality reducer dim_reducer = DimReducer(     method=\"umap\",     n_components=2,  # Reduce embeddings to 2D for visualization )  # Build the pipeline pipeline = make_pipeline(dspymator, embedder, dim_reducer)  print(\"Pipeline created:\") display(pipeline) <pre>Pipeline created:\n</pre> <pre>Pipeline(steps=[('dspymator',\n                 DSPyMator(program=predict = Predict(StringSignature(review -&gt; reasoning, sentiment\n    instructions='Given the fields `review`, produce the fields `sentiment`.'\n    review = Field(annotation=str required=True json_schema_extra={'__dspy_field_type': 'input', 'prefix': 'Review:', 'desc': '${review}'})\n    reasoning = Field(annotation=str required=True json_sche...\n    sentiment = Field(annotation=str required=True json_schema_extra={'__dspy_field_type': 'output', 'prefix': 'Sentiment:', 'desc': '${sentiment}'})\n)),\n                           target_names='sentiment',\n                           feature_names=None,\n                           lm='openai/gpt-5-nano',\n                           temperature=1.0,\n                           max_tokens=16000,\n                           use_async=True,\n                           max_concurrent=50,\n                           verbose=True)),\n                ('embeddingtransformer',\n                 EmbeddingTransformer(categorical_mapping={},\n                                      feature_names=['reasoning'],\n                                      model='openai/text-embedding-3-small')),\n                ('dimreducer', DimReducer(method='umap'))])</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.Pipeline?Documentation for PipelineiNot fitted Parameters steps\u00a0 [('dspymator', ...), ('embeddingtransformer', ...), ...] transform_input\u00a0 None memory\u00a0 None verbose\u00a0 False DSPyMator Parameters program\u00a0 predict = Pre...ntiment}'}) )) target_names\u00a0 'sentiment' feature_names\u00a0 None lm\u00a0 'openai/gpt-5-nano' temperature\u00a0 1.0 max_tokens\u00a0 16000 use_async\u00a0 True max_concurrent\u00a0 50 verbose\u00a0 True EmbeddingTransformer Parameters model\u00a0 'openai/text-embedding-3-small' feature_names\u00a0 ['reasoning'] categorical_mapping\u00a0 {} batch_size\u00a0 200 caching\u00a0 True DimReducer Parameters method\u00a0 'umap' n_components\u00a0 2 feature_names\u00a0 None In\u00a0[8]: Copied! <pre># Run the full pipeline on a small subset (to save time and API costs)\nprint(\"\\nRunning pipeline: DSPyMator \u2192 Embeddings \u2192 UMAP...\\n\")\n\n# Fit and transform\nreduced_features = pipeline.fit_transform(X_train, y_train)\nprint(\"\\nFirst few rows:\")\ndisplay(reduced_features.head())\n</pre> # Run the full pipeline on a small subset (to save time and API costs) print(\"\\nRunning pipeline: DSPyMator \u2192 Embeddings \u2192 UMAP...\\n\")  # Fit and transform reduced_features = pipeline.fit_transform(X_train, y_train) print(\"\\nFirst few rows:\") display(reduced_features.head()) <pre>\nRunning pipeline: DSPyMator \u2192 Embeddings \u2192 UMAP...\n\n</pre> <pre>DSPyMator predicting: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 300/300 [00:00&lt;00:00, 618.77it/s]\n</pre> <pre>\nFirst few rows:\n</pre> shape: (5, 2)dim_0dim_1f32f320.2869035.5891237.8631944.5940910.0605585.4370398.0574855.6435341.1493644.60095 In\u00a0[9]: Copied! <pre>import cluestar\n\n# Create an interactive visualization with cluestar\ncluestar.plot_text(\n    X=reduced_features,\n    texts=X_train[\"review\"].to_list(),\n    color_array=y_train.to_list(),\n)\n</pre> import cluestar  # Create an interactive visualization with cluestar cluestar.plot_text(     X=reduced_features,     texts=X_train[\"review\"].to_list(),     color_array=y_train.to_list(), ) Out[9]:"},{"location":"tutorials/dspymator/#dspymator-tutorial","title":"DSPyMator Tutorial\u00b6","text":"<p>This tutorial demonstrates how to use <code>DSPyMator</code>, a scikit-learn compatible wrapper that brings the power of Large Language Models (LLMs) to tabular prediction tasks through DSPy. You'll learn to build a basic text classifier with DSPyMator, use <code>predict()</code> vs <code>transform()</code> to get different outputs, extract reasoning with <code>ChainOfThought</code>, optimize prompts automatically with GEPA and <code>fit()</code>, and build advanced pipelines with embeddings and dimensionality reduction</p>"},{"location":"tutorials/dspymator/#overview","title":"Overview\u00b6","text":"<p><code>DSPyMator</code> wraps any DSPy module (e.g., <code>dspy.Predict</code>, <code>dspy.ChainOfThought</code>) and exposes it through the familiar scikit-learn API. It enables LLM-based predictions that work seamlessly with sklearn pipelines, cross-validation, and other ML tooling. Unlike traditional ML models that learn patterns through gradient descent, DSPyMator leverages pre-trained LLMs and natural language reasoning\u2014making it ideal for tasks where domain knowledge, explainability, and few-shot learning are critical.</p>"},{"location":"tutorials/dspymator/#prerequisites","title":"Prerequisites\u00b6","text":"<p>To run this tutorial, you'll need:</p> <ul> <li>An OpenAI API key Warning: This tutorial uses an LLM via an API (like OpenAI's). Running the code, especially the prompt optimization part, will make calls to this API and may incur costs.</li> <li>The <code>dspy</code> library for LLM orchestration</li> <li>The <code>datasets</code> library from Hugging Face for loading the Rotten Tomatoes dataset</li> <li><code>cluestar</code> for interactive text visualization</li> </ul>"},{"location":"tutorials/dspymator/#1-load-the-dataset","title":"1. Load the Dataset\u00b6","text":"<p>We'll use the Rotten Tomatoes dataset from Hugging Face - a popular movie review dataset for sentiment analysis. We'll load a subset for faster execution in this tutorial.</p>"},{"location":"tutorials/dspymator/#2-basic-usage-sentiment-classification-predictions","title":"2. Basic Usage: Sentiment Classification Predictions\u00b6","text":"<p>Let's start with a simple sentiment classifier using <code>dspy.Predict</code>. We'll define a signature that maps review text to sentiment.</p>"},{"location":"tutorials/dspymator/#3-adding-reasoning-with-chainofthought","title":"3. Adding Reasoning with ChainOfThought\u00b6","text":"<p>One of the most powerful features of LLMs is their ability to explain their reasoning. Let's use <code>dspy.ChainOfThought</code> to get not just predictions, but also the reasoning behind them.</p>"},{"location":"tutorials/dspymator/#4-prompt-optimization-with-gepa","title":"4. Prompt Optimization with GEPA\u00b6","text":"<p>DSPyMator supports automatic prompt optimization using DSPy optimizers. Let's use GEPA (Generalized Expectation-driven Prompt Adaptation) to automatically improve our prompts based on training data.</p> <p>GEPA iteratively refines prompts by analyzing errors and generating better instructions.</p> <p>WARNING!! Running a full GEPA optimization can require a significant number of API calls and credits</p>"},{"location":"tutorials/dspymator/#5-compose-dspymator-with-other-feature-transformers-in-scikit-learn-pipelines","title":"5. Compose DSPyMator with other feature transformers in scikit-learn pipelines\u00b6","text":"<p>While <code>predict()</code> returns only the pre-specified <code>target_names</code>, <code>transform()</code> returns all output fields from the DSPy program. This is useful when you want access to intermediate outputs or additional fields, like reasoning traces.</p>"},{"location":"tutorials/dspymator/#why-a-pipeline-integration-example-from-text-to-embeddings-to-visualization","title":"Why? A Pipeline Integration Example: From Text to Embeddings to Visualization\u00b6","text":"<p>One of DSPyMator's strengths is its compatibility with scikit-learn pipelines. Let's build a pipeline that:</p> <ol> <li>Uses DSPyMator to generate reasoning about each review</li> <li>Embeds the reasoning using <code>EmbeddingTransformer</code></li> <li>Reduces dimensionality with <code>DimReducer</code></li> </ol> <p>This creates a feature extraction pipeline where LLM reasoning becomes structured numerical features.</p>"},{"location":"tutorials/dspymator/#visualize-the-reasoning-embeddings","title":"Visualize the Reasoning Embeddings\u00b6","text":"<p>Let's visualize how the LLM's reasoning clusters different sentiments in 2D space. By embedding and visualizing the reasoning of the LLM-classifier, we can actually see the decision boundary that has been created and inspect why certain examples have been classified incorrectly in the two clusters.</p>"},{"location":"tutorials/dspymator/#6-key-takeaways","title":"6. Key Takeaways\u00b6","text":"<p>In this tutorial, you learned:</p> <p>\u2705 Basic Usage: How to wrap DSPy programs with <code>DSPyMator</code> for sklearn compatibility</p> <p>\u2705 Prediction Methods:</p> <ul> <li><code>predict()</code> returns only target field(s)</li> <li><code>transform()</code> returns all output fields (including reasoning)</li> </ul> <p>\u2705 Chain of Thought: Using <code>dspy.ChainOfThought</code> to get explainable predictions</p> <p>\u2705 Optimization: Leveraging GEPA to automatically improve prompts based on training data</p> <p>\u2705 Pipeline Integration: Building end-to-end pipelines combining LLM reasoning, embeddings, and dimensionality reduction</p>"},{"location":"tutorials/dspymator/#next-steps","title":"Next Steps\u00b6","text":"<ul> <li>Experiment with other DSPy optimizers like <code>MIPROv2</code> or <code>BootstrapFewShot</code></li> <li>Try different LLM providers (Anthropic, local models, etc.)</li> <li>Combine DSPyMator with traditional ML models in ensemble pipelines</li> <li>Explore multi-output predictions for richer feature extraction</li> </ul>"},{"location":"tutorials/dspymator/#learn-more","title":"Learn More\u00b6","text":"<ul> <li>DSPyMator Documentation</li> <li>DSPy Official Docs</li> <li>Centimators GitHub</li> <li>Hugging Face Datasets</li> </ul>"},{"location":"tutorials/keras-cortex/","title":"Keras Cortex Tutorial","text":"In\u00a0[1]: Copied! <pre>%load_ext dotenv\n%dotenv # ensure OPENAI_API_KEY is set in .env\n</pre> %load_ext dotenv %dotenv # ensure OPENAI_API_KEY is set in .env In\u00a0[\u00a0]: Copied! <pre>import os\n\nos.environ[\"KERAS_BACKEND\"] = \"jax\"\nos.environ[\"OPENAI_API_KEY\"] = \"sk-proj-...\"\n\nimport polars as pl\nfrom sklearn.datasets import make_regression\nfrom sklearn.model_selection import train_test_split\n\nfrom centimators.model_estimators import MLPRegressor, KerasCortex\n</pre> import os  os.environ[\"KERAS_BACKEND\"] = \"jax\" os.environ[\"OPENAI_API_KEY\"] = \"sk-proj-...\"  import polars as pl from sklearn.datasets import make_regression from sklearn.model_selection import train_test_split  from centimators.model_estimators import MLPRegressor, KerasCortex In\u00a0[3]: Copied! <pre>X, y = make_regression(\n    n_samples=10000,\n    n_features=100,\n    noise=0.1,\n    random_state=42,\n)\n\nX = pl.DataFrame(X)\ny = pl.Series(y)\n\n# train / val / test split  (60 / 20 / 20)\nX_train, X_tmp, y_train, y_tmp = train_test_split(X, y, test_size=0.4, random_state=42)\nX_val, X_test, y_val, y_test = train_test_split(\n    X_tmp, y_tmp, test_size=0.5, random_state=42\n)\n\nprint(X_train.shape, X_val.shape, X_test.shape)\n</pre> X, y = make_regression(     n_samples=10000,     n_features=100,     noise=0.1,     random_state=42, )  X = pl.DataFrame(X) y = pl.Series(y)  # train / val / test split  (60 / 20 / 20) X_train, X_tmp, y_train, y_tmp = train_test_split(X, y, test_size=0.4, random_state=42) X_val, X_test, y_val, y_test = train_test_split(     X_tmp, y_tmp, test_size=0.5, random_state=42 )  print(X_train.shape, X_val.shape, X_test.shape) <pre>(6000, 100) (2000, 100) (2000, 100)\n</pre> In\u00a0[4]: Copied! <pre>base_mlp = MLPRegressor(\n    hidden_units=(64, 32),\n    dropout_rate=0.1,\n)\n\ncortex = KerasCortex(\n    base_estimator=base_mlp, n_iterations=6, lm=\"openai/gpt-4o-mini\", verbose=True\n)\n</pre> base_mlp = MLPRegressor(     hidden_units=(64, 32),     dropout_rate=0.1, )  cortex = KerasCortex(     base_estimator=base_mlp, n_iterations=6, lm=\"openai/gpt-4o-mini\", verbose=True ) In\u00a0[5]: Copied! <pre>cortex.fit(\n    X_train,\n    y_train,\n    validation_data=(X_val, y_val),\n    epochs=10,\n    batch_size=516,\n)\n</pre> cortex.fit(     X_train,     y_train,     validation_data=(X_val, y_val),     epochs=10,     batch_size=516, ) <pre>WARNING:2025-05-23 02:23:28,521:jax._src.xla_bridge:909: An NVIDIA GPU may be present on this machine, but a CUDA-enabled jaxlib is not installed. Falling back to cpu.\n</pre> <pre>Epoch 1/10\n12/12 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 1s 64ms/step - loss: 38910.3633 - mse: 38910.3633\nEpoch 2/10\n12/12 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - loss: 37584.1914 - mse: 37584.1914  \nEpoch 3/10\n12/12 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - loss: 38222.5898 - mse: 38222.5898\nEpoch 4/10\n12/12 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 4ms/step - loss: 38282.4961 - mse: 38282.4961\nEpoch 5/10\n12/12 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 4ms/step - loss: 37073.2383 - mse: 37073.2383\nEpoch 6/10\n12/12 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 4ms/step - loss: 36388.0781 - mse: 36388.0781\nEpoch 7/10\n12/12 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 4ms/step - loss: 36612.1328 - mse: 36612.1328\nEpoch 8/10\n12/12 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 4ms/step - loss: 34292.5352 - mse: 34292.5352\nEpoch 9/10\n12/12 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 4ms/step - loss: 33463.0625 - mse: 33463.0625\nEpoch 10/10\n12/12 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 4ms/step - loss: 31842.3281 - mse: 31842.3281\n4/4 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 26ms/step\n\n--- Iteration 1 ---\nReasoning: \nTo improve the validation metrics, particularly the R2 score, we can consider several modifications to the model architecture and training process. First, we can experiment with adding more layers or increasing the number of units in the existing layers to allow the model to learn more complex patterns. Additionally, we can adjust the activation function to a non-linear one like 'relu' or 'swish' which often performs better in practice. Furthermore, we can implement batch normalization after each dense layer to stabilize and accelerate training. Lastly, we can consider using a different optimizer like 'Adam' with a learning rate schedule to improve convergence.\nSuggested code: \ndef build_model(self):\n    \"\"\"Construct a simple MLP with the configured hyper-parameters.\"\"\"\n    inputs = layers.Input(shape=(self._n_features_in_,), name=\"features\")\n    x = inputs\n    for units in self.hidden_units:\n        x = layers.Dense(units, activation='relu')(x)\n        if self.dropout_rate &gt; 0:\n            x = layers.Dropout(self.dropout_rate)(x)\n        x = layers.BatchNormalization()(x)  # Adding batch normalization\n    outputs = layers.Dense(self.output_units, activation=\"linear\")(x)\n    self.model = models.Model(inputs=inputs, outputs=outputs, name=\"mlp_regressor\")\n\n    self.model.compile(\n        optimizer='adam',  # Using Adam optimizer\n        loss=self.loss_function,\n        metrics=self.metrics,\n    )\n\n    return self\nEpoch 1/10\n12/12 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 1s 78ms/step - loss: 37700.0586 - mse: 37700.0586\nEpoch 2/10\n12/12 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 3ms/step - loss: 37608.2930 - mse: 37608.2930  \nEpoch 3/10\n12/12 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 4ms/step - loss: 37138.8477 - mse: 37138.8477\nEpoch 4/10\n12/12 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 4ms/step - loss: 37910.6211 - mse: 37910.6211\nEpoch 5/10\n12/12 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 5ms/step - loss: 37415.6562 - mse: 37415.6562\nEpoch 6/10\n12/12 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 5ms/step - loss: 36536.8359 - mse: 36536.8359\nEpoch 7/10\n12/12 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 5ms/step - loss: 35623.8125 - mse: 35623.8125\nEpoch 8/10\n12/12 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 5ms/step - loss: 35256.6914 - mse: 35256.6914\nEpoch 9/10\n12/12 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 5ms/step - loss: 34580.7734 - mse: 34580.7734\nEpoch 10/10\n12/12 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 5ms/step - loss: 34898.6445 - mse: 34898.6445\n4/4 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 31ms/step\nNo improvement (0.1016 &lt;= 0.1919), keeping best code.\n\n--- Iteration 2 ---\nReasoning: \nTo improve the validation metrics, particularly the R2 score, we can consider several modifications to the model architecture and training process. The current model uses a simple MLP structure with ReLU activations and dropout, which may not be sufficient for capturing complex patterns in the data. \n\n1. **Activation Function**: Experimenting with different activation functions, such as Leaky ReLU or ELU, can help mitigate issues with dying neurons and improve learning.\n2. **Layer Configuration**: Adding more layers or increasing the number of units in existing layers can enhance the model's capacity to learn complex relationships.\n3. **Regularization**: Adjusting dropout rates or adding L2 regularization can help prevent overfitting, which is crucial for improving validation metrics.\n4. **Learning Rate**: Fine-tuning the learning rate of the optimizer can lead to better convergence during training.\n\nGiven these considerations, I suggest modifying the activation function to Leaky ReLU, increasing the number of units in the hidden layers, and adjusting the dropout rate if necessary.\nSuggested code: \ndef build_model(self):\n    \"\"\"Construct a simple MLP with the configured hyper-parameters.\"\"\"\n    inputs = layers.Input(shape=(self._n_features_in_,), name=\"features\")\n    x = inputs\n    for units in self.hidden_units:\n        x = layers.Dense(units, activation='leaky_relu')(x)  # Changed to Leaky ReLU\n        if self.dropout_rate &gt; 0:\n            x = layers.Dropout(self.dropout_rate)(x)\n        x = layers.BatchNormalization()(x)  # Adding batch normalization\n    outputs = layers.Dense(self.output_units, activation=\"linear\")(x)\n    self.model = models.Model(inputs=inputs, outputs=outputs, name=\"mlp_regressor\")\n\n    self.model.compile(\n        optimizer='adam',  # Using Adam optimizer\n        loss=self.loss_function,\n        metrics=self.metrics,\n    )\n\n    return self\nEpoch 1/10\n12/12 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 1s 80ms/step - loss: 38861.8711 - mse: 38861.8711\nEpoch 2/10\n12/12 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 4ms/step - loss: 37264.9961 - mse: 37264.9961  \nEpoch 3/10\n12/12 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 4ms/step - loss: 37546.3281 - mse: 37546.3281\nEpoch 4/10\n12/12 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 4ms/step - loss: 37600.1172 - mse: 37600.1172\nEpoch 5/10\n12/12 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 5ms/step - loss: 36725.0469 - mse: 36725.0469\nEpoch 6/10\n12/12 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 5ms/step - loss: 36916.5859 - mse: 36916.5859\nEpoch 7/10\n12/12 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 5ms/step - loss: 35638.0977 - mse: 35638.0977\nEpoch 8/10\n12/12 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 5ms/step - loss: 35082.1836 - mse: 35082.1836\nEpoch 9/10\n12/12 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 5ms/step - loss: 34401.2461 - mse: 34401.2461\nEpoch 10/10\n12/12 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 5ms/step - loss: 33381.0312 - mse: 33381.0312\n4/4 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 25ms/step\nNo improvement (0.1285 &lt;= 0.1919), keeping best code.\n\n--- Iteration 3 ---\nReasoning: \nTo improve the validation metrics (R2), we can consider several modifications to the model architecture and training process. The current model uses Leaky ReLU activation, which is a good choice, but we can experiment with adding more complexity to the model by increasing the number of layers or units. Additionally, we can adjust the dropout rate to prevent overfitting, especially if the model is too complex for the dataset. We can also consider using a different optimizer or adjusting the learning rate for better convergence. Finally, we can add more batch normalization layers to stabilize the learning process.\nSuggested code: \ndef build_model(self):\n    \"\"\"Construct a simple MLP with the configured hyper-parameters.\"\"\"\n    inputs = layers.Input(shape=(self._n_features_in_,), name=\"features\")\n    x = inputs\n    for units in self.hidden_units:\n        x = layers.Dense(units, activation='leaky_relu')(x)  # Keeping Leaky ReLU\n        if self.dropout_rate &gt; 0:\n            x = layers.Dropout(self.dropout_rate)(x)\n        x = layers.BatchNormalization()(x)  # Adding batch normalization\n    # Adding an additional Dense layer for more complexity\n    x = layers.Dense(64, activation='leaky_relu')(x)  # Additional layer\n    if self.dropout_rate &gt; 0:\n        x = layers.Dropout(self.dropout_rate)(x)\n    outputs = layers.Dense(self.output_units, activation=\"linear\")(x)\n    self.model = models.Model(inputs=inputs, outputs=outputs, name=\"mlp_regressor\")\n\n    self.model.compile(\n        optimizer='adam',  # Using Adam optimizer\n        loss=self.loss_function,\n        metrics=self.metrics,\n    )\n\n    return self\nEpoch 1/10\n12/12 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 1s 88ms/step - loss: 38137.1836 - mse: 38137.1836\nEpoch 2/10\n12/12 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 5ms/step - loss: 38227.8203 - mse: 38227.8203  \nEpoch 3/10\n12/12 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 5ms/step - loss: 38262.1445 - mse: 38262.1445\nEpoch 4/10\n12/12 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 6ms/step - loss: 37551.5586 - mse: 37551.5586\nEpoch 5/10\n12/12 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 6ms/step - loss: 35559.9570 - mse: 35559.9570\nEpoch 6/10\n12/12 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 7ms/step - loss: 33512.2930 - mse: 33512.2930\nEpoch 7/10\n12/12 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 7ms/step - loss: 30176.9766 - mse: 30176.9766\nEpoch 8/10\n12/12 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 7ms/step - loss: 26692.1738 - mse: 26692.1738\nEpoch 9/10\n12/12 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 6ms/step - loss: 23776.6172 - mse: 23776.6172\nEpoch 10/10\n12/12 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 7ms/step - loss: 20573.6172 - mse: 20573.6172\n4/4 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 30ms/step\nImprovement! New validation score: 0.6164 &gt; 0.1919\n\n--- Iteration 4 ---\nReasoning: \nTo improve the validation metrics (R2), we can consider several modifications to the model architecture and training process. The current model uses Leaky ReLU activations, which is good, but we can experiment with adding more complexity and regularization. The addition of more layers or units can help the model learn better representations. Additionally, we can adjust the learning rate of the Adam optimizer to ensure that the model converges more effectively. Implementing early stopping during training can also help prevent overfitting, which is crucial for improving validation metrics.\nSuggested code: \ndef build_model(self):\n    \"\"\"Construct a simple MLP with the configured hyper-parameters.\"\"\"\n    inputs = layers.Input(shape=(self._n_features_in_,), name=\"features\")\n    x = inputs\n    for units in self.hidden_units:\n        x = layers.Dense(units, activation='leaky_relu')(x)  # Keeping Leaky ReLU\n        if self.dropout_rate &gt; 0:\n            x = layers.Dropout(self.dropout_rate)(x)\n        x = layers.BatchNormalization()(x)  # Adding batch normalization\n    # Adding an additional Dense layer for more complexity\n    x = layers.Dense(128, activation='leaky_relu')(x)  # Increased units for additional layer\n    if self.dropout_rate &gt; 0:\n        x = layers.Dropout(self.dropout_rate)(x)\n    outputs = layers.Dense(self.output_units, activation=\"linear\")(x)\n    self.model = models.Model(inputs=inputs, outputs=outputs, name=\"mlp_regressor\")\n\n    self.model.compile(\n        optimizer='adam',  # Using Adam optimizer\n        loss=self.loss_function,\n        metrics=self.metrics,\n    )\n\n    return self\nEpoch 1/10\n12/12 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 1s 87ms/step - loss: 40391.3164 - mse: 40391.3164\nEpoch 2/10\n12/12 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 5ms/step - loss: 38161.0781 - mse: 38161.0781 \nEpoch 3/10\n12/12 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 6ms/step - loss: 37445.9609 - mse: 37445.9609\nEpoch 4/10\n12/12 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 6ms/step - loss: 36245.1328 - mse: 36245.1328\nEpoch 5/10\n12/12 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 7ms/step - loss: 34253.0195 - mse: 34253.0195\nEpoch 6/10\n12/12 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 7ms/step - loss: 30194.2812 - mse: 30194.2812\nEpoch 7/10\n12/12 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 7ms/step - loss: 26387.5801 - mse: 26387.5801\nEpoch 8/10\n12/12 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 7ms/step - loss: 22368.8633 - mse: 22368.8633\nEpoch 9/10\n12/12 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 7ms/step - loss: 16535.9961 - mse: 16535.9961\nEpoch 10/10\n12/12 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 7ms/step - loss: 12131.1436 - mse: 12131.1436\n4/4 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 29ms/step\nImprovement! New validation score: 0.8433 &gt; 0.6164\n\n--- Iteration 5 ---\nReasoning: \nTo improve validation metrics, particularly R2, we can consider several modifications to the model architecture and training process. The current model has a relatively simple structure with a few dense layers and dropout for regularization. However, the performance log indicates that the model's complexity may not be sufficient to capture the underlying patterns in the data. \n\n1. **Increase Model Complexity**: Adding more layers or increasing the number of units in existing layers can help the model learn more complex representations.\n2. **Adjust Activation Functions**: While Leaky ReLU is a good choice, experimenting with other activation functions like 'relu' or 'swish' might yield better results.\n3. **Learning Rate Adjustment**: Fine-tuning the learning rate of the Adam optimizer can significantly impact convergence and performance.\n4. **Regularization Techniques**: Besides dropout, we could also consider L2 regularization on the dense layers to prevent overfitting.\n\nGiven these considerations, I suggest increasing the number of units in the additional dense layer and experimenting with a different activation function.\nSuggested code: \ndef build_model(self):\n    \"\"\"Construct a simple MLP with the configured hyper-parameters.\"\"\"\n    inputs = layers.Input(shape=(self._n_features_in_,), name=\"features\")\n    x = inputs\n    for units in self.hidden_units:\n        x = layers.Dense(units, activation='relu')(x)  # Changed to ReLU for potential better performance\n        if self.dropout_rate &gt; 0:\n            x = layers.Dropout(self.dropout_rate)(x)\n        x = layers.BatchNormalization()(x)  # Adding batch normalization\n    # Adding an additional Dense layer with increased units for more complexity\n    x = layers.Dense(256, activation='relu')(x)  # Increased units for additional layer\n    if self.dropout_rate &gt; 0:\n        x = layers.Dropout(self.dropout_rate)(x)\n    outputs = layers.Dense(self.output_units, activation=\"linear\")(x)\n    self.model = models.Model(inputs=inputs, outputs=outputs, name=\"mlp_regressor\")\n\n    self.model.compile(\n        optimizer='adam',  # Using Adam optimizer\n        loss=self.loss_function,\n        metrics=self.metrics,\n    )\n\n    return self\nEpoch 1/10\n12/12 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 2s 89ms/step - loss: 38065.1797 - mse: 38065.1797\nEpoch 2/10\n12/12 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 6ms/step - loss: 37779.0117 - mse: 37779.0117 \nEpoch 3/10\n12/12 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 7ms/step - loss: 37103.6758 - mse: 37103.6758\nEpoch 4/10\n12/12 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 7ms/step - loss: 36028.3477 - mse: 36028.3477\nEpoch 5/10\n12/12 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 7ms/step - loss: 33991.7109 - mse: 33991.7109\nEpoch 6/10\n12/12 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 6ms/step - loss: 30745.7578 - mse: 30745.7578\nEpoch 7/10\n12/12 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 7ms/step - loss: 24856.4180 - mse: 24856.4180\nEpoch 8/10\n12/12 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 7ms/step - loss: 18043.0879 - mse: 18043.0879\nEpoch 9/10\n12/12 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 7ms/step - loss: 12032.8086 - mse: 12032.8086\nEpoch 10/10\n12/12 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 7ms/step - loss: 7566.1025 - mse: 7566.1025\n4/4 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 32ms/step\nImprovement! New validation score: 0.8788 &gt; 0.8433\n\n--- Iteration 6 ---\nReasoning: \nTo improve the validation metrics (R2), we can consider several modifications to the model architecture. The current model has a relatively high number of units in the last dense layer (256), which may lead to overfitting, especially if the dataset is not large enough. Reducing the complexity of the model by decreasing the number of units in the last layer can help generalize better. Additionally, we can introduce a more advanced activation function like 'swish' or 'gelu' which may provide better performance than 'leaky_relu' or 'relu'. We can also consider adding more dropout layers or increasing the dropout rate to further prevent overfitting.\nSuggested code: \ndef build_model(self):\n    \"\"\"Construct a simple MLP with the configured hyper-parameters.\"\"\"\n    inputs = layers.Input(shape=(self._n_features_in_,), name=\"features\")\n    x = inputs\n    for units in self.hidden_units:\n        x = layers.Dense(units, activation='swish')(x)  # Changed to Swish activation\n        if self.dropout_rate &gt; 0:\n            x = layers.Dropout(self.dropout_rate)(x)\n        x = layers.BatchNormalization()(x)  # Adding batch normalization\n    # Reducing the number of units in the additional Dense layer\n    x = layers.Dense(128, activation='swish')(x)  # Decreased units for additional layer\n    if self.dropout_rate &gt; 0:\n        x = layers.Dropout(self.dropout_rate)(x)\n    outputs = layers.Dense(self.output_units, activation=\"linear\")(x)\n    self.model = models.Model(inputs=inputs, outputs=outputs, name=\"mlp_regressor\")\n\n    self.model.compile(\n        optimizer='adam',  # Using Adam optimizer\n        loss=self.loss_function,\n        metrics=self.metrics,\n    )\n\n    return self\nEpoch 1/10\n12/12 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 1s 89ms/step - loss: 38398.5625 - mse: 38398.5625\nEpoch 2/10\n12/12 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 6ms/step - loss: 38273.7734 - mse: 38273.7734 \nEpoch 3/10\n12/12 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 6ms/step - loss: 37329.9453 - mse: 37329.9453\nEpoch 4/10\n12/12 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 7ms/step - loss: 37087.1641 - mse: 37087.1641\nEpoch 5/10\n12/12 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 7ms/step - loss: 34895.7227 - mse: 34895.7227\nEpoch 6/10\n12/12 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 7ms/step - loss: 31744.3477 - mse: 31744.3477\nEpoch 7/10\n12/12 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 7ms/step - loss: 28108.6406 - mse: 28108.6406\nEpoch 8/10\n12/12 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 7ms/step - loss: 24307.8398 - mse: 24307.8398\nEpoch 9/10\n12/12 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 7ms/step - loss: 20269.2207 - mse: 20269.2207\nEpoch 10/10\n12/12 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 7ms/step - loss: 16637.2168 - mse: 16637.2168\n4/4 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 36ms/step\nNo improvement (0.6662 &lt;= 0.8788), keeping best code.\n</pre> Out[5]: <pre>KerasCortex(base_estimator=MLPRegressor(output_units=1,\n                                        optimizer=&lt;class 'keras.src.optimizers.adam.Adam'&gt;,\n                                        learning_rate=0.001,\n                                        loss_function='mse',\n                                        metrics=['mse'],\n                                        model=None,\n                                        distribution_strategy=None,\n                                        hidden_units=(64, 32),\n                                        activation='relu',\n                                        dropout_rate=0.1),\n            lm=&lt;dspy.clients.lm.LM object at 0x7f9c4414daf0&gt;, n_iterations=6,\n            verbose=True)</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.KerasCortexiFitted<pre>KerasCortex(base_estimator=MLPRegressor(output_units=1,\n                                        optimizer=&lt;class 'keras.src.optimizers.adam.Adam'&gt;,\n                                        learning_rate=0.001,\n                                        loss_function='mse',\n                                        metrics=['mse'],\n                                        model=None,\n                                        distribution_strategy=None,\n                                        hidden_units=(64, 32),\n                                        activation='relu',\n                                        dropout_rate=0.1),\n            lm=&lt;dspy.clients.lm.LM object at 0x7f9c4414daf0&gt;, n_iterations=6,\n            verbose=True)</pre> base_estimator: MLPRegressor<pre>MLPRegressor(output_units=1, optimizer=&lt;class 'keras.src.optimizers.adam.Adam'&gt;, learning_rate=0.001, loss_function='mse', metrics=['mse'], model=None, distribution_strategy=None, hidden_units=(64, 32), activation='relu', dropout_rate=0.1)</pre> MLPRegressor<pre>MLPRegressor(output_units=1, optimizer=&lt;class 'keras.src.optimizers.adam.Adam'&gt;, learning_rate=0.001, loss_function='mse', metrics=['mse'], model=None, distribution_strategy=None, hidden_units=(64, 32), activation='relu', dropout_rate=0.1)</pre>"},{"location":"tutorials/keras-cortex/#keras-cortex-tutorial","title":"Keras Cortex Tutorial\u00b6","text":"<p>This tutorial demonstrates how to use <code>KerasCortex</code>, a meta-estimator that uses Large Language Models (LLMs) to iteratively improve neural network architectures through self-reflection and iterative improvement.</p>"},{"location":"tutorials/keras-cortex/#overview","title":"Overview\u00b6","text":"<p><code>KerasCortex</code> wraps a base Keras estimator and uses an LLM via DSPy to suggest modifications to the <code>build_model</code> method. It iteratively attempts these suggestions, evaluates their performance on validation data, and keeps the best-performing model architecture. Each step, it reasons about changes it should make to the model before re-fitting it.</p>"},{"location":"tutorials/keras-cortex/#prerequisites","title":"Prerequisites\u00b6","text":"<p>To run the full tutorial, you'll need:</p> <ul> <li>An OpenAI API key (or access to another LLM via DSPy)</li> <li>JAX, TensorFlow, or PyTorch backend for Keras</li> <li>The <code>dspy</code> library for LLM orchestration</li> </ul> <p>!!! Warning</p> <pre><code>This module is a work in progress. It is not yet ready for production use.\nThis is highly experimental and likely to overfit.</code></pre>"},{"location":"user-guide/advanced-pipelines/","title":"Advanced Pipelines","text":"<p><code>centimators</code> transformers are designed to work seamlessly within scikit-learn Pipelines, leveraging its metadata routing capabilities. This allows you to pass data like date or ticker series through the pipeline to the specific transformers that need them, while also chaining together multiple transformers. This is useful for building more complex feature pipelines, but also allows for better cross-validation, hyperparameter tuning, and model selection. For example, if you add a Regressor at the end of the pipeline, you can imagine searching over various combinations of lags, moving average windows, and model hyperparameters during the training process.</p>"},{"location":"user-guide/advanced-pipelines/#building-feature-pipelines","title":"Building Feature Pipelines","text":"<pre><code>from sklearn import set_config\nfrom sklearn.pipeline import make_pipeline\nfrom centimators.feature_transformers import (\n    LogReturnTransformer,\n    RankTransformer,\n    LagTransformer,\n    MovingAverageTransformer,\n)\n\n# Enable metadata routing globally\nset_config(enable_metadata_routing=True)\n\n# Define individual transformers with their parameters\nlog_return_transformer = LogReturnTransformer().set_transform_request(\n    ticker_series=True\n)\nranker = RankTransformer().set_transform_request(date_series=True)\nlag_windows = [0, 5, 10, 15]\nlagger = LagTransformer(windows=lag_windows).set_transform_request(\n    ticker_series=True\n)\nma_windows = [5, 10, 20, 40]\nma_transformer = MovingAverageTransformer(\n    windows=ma_windows\n).set_transform_request(ticker_series=True)\n\n# Create the pipeline\nfeature_pipeline = make_pipeline(\n    log_return_transformer, ranker, lagger, ma_transformer\n)\n</code></pre> <p>Explanation:</p> <ul> <li><code>set_config(enable_metadata_routing=True)</code> turns on scikit-learn's metadata routing.</li> <li><code>set_transform_request(metadata_name=True)</code> on each transformer tells the pipeline that this transformer expects <code>metadata_name</code> (e.g., <code>date_series</code>).</li> <li>When <code>pipeline.fit_transform(X, date_series=dates, ticker_series=tickers)</code> is called:<ul> <li>The <code>date_series</code> is automatically passed to <code>RankTransformer</code>.</li> <li>The <code>ticker_series</code> is automatically passed to <code>LagTransformer</code>, <code>MovingAverageTransformer</code>, and <code>LogReturnTransformer</code>.</li> <li>The output of <code>LogReturnTransformer</code> is passed to <code>RankTransformer</code>.</li> <li>The output of <code>RankTransformer</code> is passed to <code>LagTransformer</code>.</li> <li>The output of <code>LagTransformer</code> is passed to <code>MovingAverageTransformer</code>.</li> </ul> </li> </ul> <p>This allows for complex data transformations where different steps require different auxiliary information, all managed cleanly by the pipeline.</p> <pre><code># Now you can use this pipeline with your data\nfeature_names = [\"open\", \"high\", \"low\", \"close\"]\ntransformed_df = feature_pipeline.fit_transform(\n    df_pl[feature_names],\n    date_series=df_pl[\"date\"],\n    ticker_series=df_pl[\"ticker\"],\n)\n</code></pre> <p>We can take a closer look at a sample output for a single ticker and for a single initial feature. This clearly shows how the close price for a cross-sectional dataset is transformed into a log return, ranked (between 0 and 1) by date, and smoothed (moving average windows) by ticker:</p>"},{"location":"user-guide/advanced-pipelines/#end-to-end-pipeline-with-an-estimator","title":"End-to-End Pipeline with an Estimator","text":"<p>The previous section constructed only the feature engineering part of a workflow. Thanks to Centimators' Keras-backed estimators you can seamlessly append a model as the final step and train everything through a single <code>fit</code> call.</p> <pre><code>from sklearn.impute import SimpleImputer\nfrom centimators.model_estimators import MLPRegressor\n\nlag_windows = [0, 5, 10, 15]\nma_windows = [5, 10, 20, 40]\n\nmlp_pipeline = make_pipeline(\n    feature_pipeline,\n    # Replace NaNs created by lagging with a constant value\n    SimpleImputer(strategy=\"constant\", fill_value=0.5).set_output(transform=\"pandas\"),\n    # Train a neural network in-place\n    MLPRegressor().set_fit_request(epochs=True),\n)\n\nfeature_names = [\"open\", \"high\", \"low\", \"close\"]\n\nmlp_pipeline.fit(\n    df_pl[feature_names],\n    df_pl[\"close\"],\n    date_series=df_pl[\"date\"],\n    ticker_series=df_pl[\"ticker\"],\n    epochs=5,\n)\n</code></pre> <p></p> <p>Just as before, scikit-learn's metadata routing ensures that auxiliary inputs (<code>date_series</code>, <code>ticker_series</code>, <code>epochs</code>) are forwarded only to the steps that explicitly requested them.</p>"},{"location":"user-guide/advanced-pipelines/#cross-validation-and-hyperparameter-tuning","title":"Cross-Validation and Hyperparameter Tuning","text":"<p>Because everything follows the scikit-learn API, you can use standard tools for model validation and optimization:</p> <pre><code>from sklearn.model_selection import GridSearchCV, TimeSeriesSplit\n\n# Define parameter grid for the pipeline\nparam_grid = {\n    'lagTransformer__windows': [[1, 5], [1, 5, 10], [1, 5, 10, 20]],\n    'movingaverageTransformer__windows': [[5, 10], [5, 10, 20], [5, 10, 20, 50]],\n    'mlpregressor__hidden_units': [(64,), (64, 32), (128, 64)],\n    'mlpregressor__dropout_rate': [0.0, 0.1, 0.2],\n}\n\n# Use time series cross-validation\ntscv = TimeSeriesSplit(n_splits=5)\n\n# Grid search with the pipeline\ngrid_search = GridSearchCV(\n    mlp_pipeline,\n    param_grid,\n    cv=tscv,\n    scoring='neg_mean_squared_error',\n    n_jobs=-1\n)\n\n# Fit with metadata routing\ngrid_search.fit(\n    df_pl[feature_names],\n    df_pl[\"target\"],\n    date_series=df_pl[\"date\"],\n    ticker_series=df_pl[\"ticker\"],\n)\n\nprint(f\"Best parameters: {grid_search.best_params_}\")\nprint(f\"Best score: {grid_search.best_score_}\")\n</code></pre> <p>This allows you to search over various combinations of lag windows, moving average windows, model architectures, and other hyperparameters while maintaining proper time series validation and metadata routing. </p>"},{"location":"user-guide/dspymator/","title":"DSPyMator","text":"<p>Requires DSPy</p> <p>This estimator requires the <code>dspy</code> optional dependency. Install with: <pre><code>uv add centimators[dspy]\n</code></pre></p> <p><code>centimators.model_estimators.DSPyMator</code> brings the power of Large Language Models (LLMs) to feature engineering and tabular prediction tasks through DSPy. Unlike traditional neural networks that learn patterns from data through gradient descent, DSPyMator leverages pre-trained LLMs and natural language reasoning to make predictions, making it uniquely suited for tasks where domain knowledge, explainability, and few-shot learning are critical.</p>"},{"location":"user-guide/dspymator/#why-use-dspymator","title":"Why Use DSPyMator?","text":"<p>DSPyMator excels in scenarios where traditional machine learning falls short:</p> <ul> <li>Few-Shot Learning: Achieve strong performance with limited training data by leveraging the LLM's pre-existing knowledge</li> <li>Domain Knowledge Integration: Incorporate reasoning and expert knowledge naturally through task descriptions</li> <li>Explainable Predictions: Access the model's reasoning process (when using chain-of-thought)</li> <li>Mixed Data Types: Seamlessly handle numerical, categorical, and text features without complex preprocessing</li> <li>Rapid Prototyping: Get baseline predictions quickly before investing in traditional model training</li> <li>Scikit-learn Compatible: Stack and compose DSPyMator in scikit-learn pipelines, column transformers, and with other compatible workflows</li> </ul>"},{"location":"user-guide/dspymator/#how-it-works","title":"How It Works","text":"<p>DSPyMator wraps any DSPy <code>Module</code> (like <code>dspy.Predict</code> or <code>dspy.ChainOfThought</code>) and exposes it through the familiar scikit-learn API. Under the hood:</p> <ol> <li>Signature Definition: You define input and output fields via DSPy signatures (e.g., <code>\"review_text -&gt; sentiment\"</code>)</li> <li>Feature Mapping: DSPyMator automatically maps your dataframe columns to the signature's input fields</li> <li>LLM Execution: During prediction, each row is converted into a prompt and sent to the LLM</li> <li>Output Extraction: Results are extracted and returned as numpy arrays (for <code>predict</code>) or dataframes (for <code>transform</code>)</li> </ol> <p>The real power comes from DSPy's optimization capabilities. You can use optimizers like <code>GEPA</code>, <code>MIPROv2</code>, or <code>BootstrapFewShot</code> to automatically improve prompts, select better demonstrations, or even finetune the model\u2014all through the standard <code>fit()</code> method.</p>"},{"location":"user-guide/dspymator/#usage","title":"Usage","text":""},{"location":"user-guide/dspymator/#basic-classification","title":"Basic Classification","text":"<p>Let's start with a simple sentiment classification task using movie reviews:</p> <pre><code>import polars as pl\nimport dspy\nfrom centimators.model_estimators import DSPyMator\n\n# Sample movie reviews\nreviews = pl.DataFrame({\n    \"review_text\": [\n        \"This movie was absolutely fantastic! A masterpiece.\",\n        \"Terrible waste of time. Boring and predictable.\",\n        \"Pretty good, though it had some slow moments.\",\n        \"One of the worst films I've ever seen.\",\n        \"Loved every minute of it! Highly recommended.\"\n    ],\n    \"sentiment\": [\"positive\", \"negative\", \"neutral\", \"negative\", \"positive\"]\n})\n\n# Define a DSPy program with input and output signature\n# You can use dspy.Predict for simple predictions or dspy.ChainOfThought for reasoning\nclassifier_program = dspy.Predict(\"review_text: str -&gt; sentiment: str\")\n\n# Create the DSPyMator estimator\nsentiment_classifier = DSPyMator(\n    program=classifier_program,\n    target_names=\"sentiment\",  # Which output field to use as predictions\n    lm=\"openai/gpt-4o-mini\",   # Language model to use\n    temperature=0.0,            # Low temperature for consistent outputs\n)\n\n# Fit the classifier (establishes the LM configuration)\nX = reviews[[\"review_text\"]]\ny = reviews[\"sentiment\"]\nsentiment_classifier.fit(X, y)\n\n# Make predictions\ntest_reviews = pl.DataFrame({\n    \"review_text\": [\n        \"An incredible journey with stunning visuals.\",\n        \"Could barely stay awake through this one.\"\n    ]\n})\n\npredictions = sentiment_classifier.predict(test_reviews[[\"review_text\"]])\nprint(predictions)  # ['positive', 'negative']\n</code></pre>"},{"location":"user-guide/dspymator/#getting-full-outputs-with-transform","title":"Getting Full Outputs with Transform","text":"<p>Unlike <code>predict()</code> which returns only the target field, <code>transform()</code> returns all output fields from the DSPy program:</p> <pre><code># Get all outputs (useful for accessing reasoning, confidence, etc.)\nfull_outputs = sentiment_classifier.transform(test_reviews[[\"review_text\"]])\nprint(full_outputs)\n# Polars DataFrame with column: sentiment\n</code></pre> <p>If you're using <code>dspy.ChainOfThought</code> instead of <code>dspy.Predict</code>, you'll also get reasoning:</p> <pre><code># Using ChainOfThought to get reasoning\ncot_program = dspy.ChainOfThought(\"review_text: str -&gt; sentiment: str\")\n\ncot_classifier = DSPyMator(\n    program=cot_program,\n    target_names=\"sentiment\",\n)\n\ncot_classifier.fit(X, y)\noutputs = cot_classifier.transform(test_reviews[[\"review_text\"]])\nprint(outputs)\n# Polars DataFrame with columns: rationale, sentiment\n</code></pre>"},{"location":"user-guide/dspymator/#multi-output-predictions","title":"Multi-Output Predictions","text":"<p>DSPyMator supports multiple output fields for richer predictions:</p> <pre><code># Define a program with multiple outputs\nmulti_output_program = dspy.Predict(\n    \"review_text: str -&gt; sentiment: str, confidence: float\"\n)\n\nmulti_classifier = DSPyMator(\n    program=multi_output_program,\n    target_names=[\"sentiment\", \"confidence\"],  # Multiple targets\n)\n\nmulti_classifier.fit(X, y)\npredictions = multi_classifier.predict(test_reviews[[\"review_text\"]])\nprint(predictions.shape)  # (2, 2) - rows \u00d7 outputs\n</code></pre>"},{"location":"user-guide/dspymator/#prompt-optimization-with-gepa","title":"Prompt Optimization with GEPA","text":"<p>The real magic happens when you use DSPy optimizers to automatically improve your prompts:</p> <pre><code># Define a metric function for optimization\ndef sentiment_metric(gold, pred, trace=None, pred_name=None, pred_trace=None):\n    \"\"\"\n    Metric that returns score and optional feedback for GEPA.\n\n    Args:\n        gold: The ground truth example\n        pred: The predicted output\n        trace: Optional full program trace\n        pred_name: Optional name of predictor being optimized\n        pred_trace: Optional trace of specific predictor\n\n    Returns:\n        float score or dspy.Prediction(score=float, feedback=str)\n    \"\"\"\n    y_pred = pred.sentiment\n    y_true = gold.sentiment\n    is_correct = (y_pred == y_true)\n    score = 1.0 if is_correct else 0.0\n\n    # If GEPA is requesting feedback, provide rich textual guidance\n    if pred_name:\n        if is_correct:\n            feedback = f\"Correctly classified as {y_pred}.\"\n        else:\n            feedback = f\"Incorrect. Predicted {y_pred} but should be {y_true}.\"\n\n        return dspy.Prediction(score=score, feedback=feedback)\n\n    return score\n\n# Create a GEPA optimizer\ngepa_optimizer = dspy.GEPA(\n    metric=sentiment_metric,\n    auto=\"light\",  # or \"medium\", \"heavy\" for more thoroughness\n    reflection_minibatch_size=20,\n    reflection_lm=dspy.LM(model=\"openai/gpt-4o-mini\", temperature=1.0)\n)\n\n# Create a fresh classifier\noptimized_classifier = DSPyMator(\n    program=dspy.Predict(\"review_text: str -&gt; sentiment: str\"),\n    target_names=\"sentiment\",\n)\n\n# Fit with optimization (GEPA will improve the prompts)\noptimized_classifier.fit(\n    X, \n    y, \n    optimizer=gepa_optimizer,\n    validation_data=0.3  # Use 30% of data for validation\n)\n\n# The optimized program is now ready to use\npredictions = optimized_classifier.predict(test_reviews[[\"review_text\"]])\n</code></pre>"},{"location":"user-guide/dspymator/#few-shot-learning-with-bootstrap","title":"Few-Shot Learning with Bootstrap","text":"<p>For few-shot learning, use <code>BootstrapFewShot</code> to automatically select good demonstrations:</p> <pre><code># Few-shot optimizer doesn't need validation data\nbootstrap_optimizer = dspy.BootstrapFewShot(\n    metric=sentiment_metric,\n    max_bootstrapped_demos=3,  # Number of examples to use\n)\n\nfew_shot_classifier = DSPyMator(\n    program=dspy.ChainOfThought(\"review_text: str -&gt; sentiment: str\"),\n    target_names=\"sentiment\",\n)\n\n# Fit with bootstrap (no validation_data needed)\nfew_shot_classifier.fit(\n    X,\n    y,\n    optimizer=bootstrap_optimizer,\n    validation_data=None  # Few-shot optimizers only need trainset\n)\n\npredictions = few_shot_classifier.predict(test_reviews[[\"review_text\"]])\n</code></pre>"},{"location":"user-guide/dspymator/#advanced-custom-multi-input-features","title":"Advanced: Custom Multi-Input Features","text":"<p>DSPyMator automatically maps multiple dataframe columns to signature fields:</p> <pre><code># Multi-feature example\nmovie_data = pl.DataFrame({\n    \"title\": [\"The Matrix\", \"Cats\"],\n    \"review_text\": [\"Mind-bending sci-fi classic\", \"A catastrophic mistake\"],\n    \"rating\": [5, 1],\n})\n\n# Signature with multiple inputs\nmulti_input_program = dspy.Predict(\n    \"title: str, review_text: str, rating: int -&gt; sentiment: str\"\n)\n\nmulti_input_classifier = DSPyMator(\n    program=multi_input_program,\n    target_names=\"sentiment\",\n    feature_names=[\"title\", \"review_text\", \"rating\"],  # Map columns to signature\n)\n\n# Fit and predict\nmulti_input_classifier.fit(movie_data[[\"title\", \"review_text\", \"rating\"]], None)\npredictions = multi_input_classifier.predict(movie_data[[\"title\", \"review_text\", \"rating\"]])\n</code></pre>"},{"location":"user-guide/dspymator/#configuring-language-models","title":"Configuring Language Models","text":"<p>DSPyMator accepts either a model string or a pre-configured <code>dspy.LM</code> object for the <code>lm</code> parameter.</p> <p>Simple usage with model string:</p> <pre><code># Uses default OpenAI API (requires OPENAI_API_KEY env var)\nclassifier = DSPyMator(\n    program=dspy.Predict(\"text -&gt; label\"),\n    target_names=\"label\",\n    lm=\"openai/gpt-4o-mini\",\n    temperature=0.0,\n    max_tokens=1000,\n)\n</code></pre> <p>Using custom providers:</p> <p>For custom API configuration, pass a pre-configured <code>dspy.LM</code> object. DSPy uses LiteLLM under the hood, so any LiteLLM-supported provider works. Extra kwargs are passed through to LiteLLM:</p> <pre><code>import dspy\n\n# Pass a pre-configured LM object\nclassifier = DSPyMator(\n    program=dspy.Predict(\"text -&gt; label\"),\n    target_names=\"label\",\n    lm=dspy.LM(\n        \"openrouter/anthropic/claude-3-haiku\",\n        temperature=0.1,\n        max_tokens=1000,\n        # Additional kwargs are passed to LiteLLM\n    ),\n)\n</code></pre> <p>Temperature and max_tokens</p> <p>When passing a <code>dspy.LM</code> object, configure <code>temperature</code> and <code>max_tokens</code> on the LM directly. The DSPyMator parameters are ignored when using a pre-configured LM.</p> <p>Environment variables:</p> <p>Most providers are configured via environment variables. Set them before calling <code>fit()</code>:</p> <pre><code>import os\n\n# OpenAI\nos.environ[\"OPENAI_API_KEY\"] = \"sk-...\"\n\n# OpenRouter\nos.environ[\"OPENROUTER_API_KEY\"] = \"sk-or-...\"\n\n# Anthropic\nos.environ[\"ANTHROPIC_API_KEY\"] = \"sk-ant-...\"\n</code></pre> <p>See the DSPy LM documentation and LiteLLM provider docs for supported providers and configuration.</p>"},{"location":"user-guide/dspymator/#async-execution-for-speed","title":"Async Execution for Speed","text":"<p>By default, DSPyMator uses async execution for faster batch predictions:</p> <pre><code># Async is on by default\nfast_classifier = DSPyMator(\n    program=dspy.Predict(\"review_text: str -&gt; sentiment: str\"),\n    target_names=\"sentiment\",\n    use_async=True,        # Default behavior\n    max_concurrent=50,     # Max concurrent API requests\n    verbose=True,          # Show progress bar\n)\n\n# For synchronous execution (useful for debugging)\nsync_classifier = DSPyMator(\n    program=dspy.Predict(\"review_text: str -&gt; sentiment: str\"),\n    target_names=\"sentiment\",\n    use_async=False,\n    verbose=True,\n)\n</code></pre> <p>AsyncIO and Local Model Limitations</p> <p>DSPyMator's async execution mode is ideal for rapid API-backed LLM calls (e.g., OpenAI, Anthropic) because it uses asyncio. For best results with local LLMs, use <code>use_async=False</code> unless you know your backend supports true parallelism.</p>"},{"location":"user-guide/dspymator/#pipeline-integration","title":"Pipeline Integration","text":"<p>DSPyMator works seamlessly in scikit-learn pipelines:</p> <pre><code>from sklearn.pipeline import make_pipeline\n\nllm_pipeline = make_pipeline(\n    # ... preprocessing steps ...\n    DSPyMator(\n        program=dspy.ChainOfThought(\"review_text: str -&gt; sentiment: str\"),\n        target_names=\"sentiment\"\n    ),\n    EmbeddingTransformer(\n        model=\"openai/text-embedding-3-small\",      # or your preferred embedding model\n        feature_names=[\"reasoning\"],                # specify which columns to embed\n    )\n</code></pre>"},{"location":"user-guide/dspymator/#parameters-reference","title":"Parameters Reference","text":"Parameter Type Default Description <code>program</code> <code>dspy.Module</code> required DSPy module (e.g., <code>dspy.Predict</code>, <code>dspy.ChainOfThought</code>) with a signature defining input/output fields <code>target_names</code> <code>str \\| list[str]</code> required Output field name(s) to use as predictions <code>feature_names</code> <code>list[str] \\| None</code> <code>None</code> Column names mapping input data to signature fields. If <code>None</code>, inferred from dataframe columns <code>lm</code> <code>str \\| dspy.LM</code> <code>\"openai/gpt-5-nano\"</code> Language model - either a string identifier or a pre-configured <code>dspy.LM</code> object <code>temperature</code> <code>float</code> <code>1.0</code> Sampling temperature (ignored if <code>lm</code> is a <code>dspy.LM</code> object) <code>max_tokens</code> <code>int</code> <code>16000</code> Maximum tokens in responses (ignored if <code>lm</code> is a <code>dspy.LM</code> object) <code>use_async</code> <code>bool</code> <code>True</code> Use async execution for batch predictions <code>max_concurrent</code> <code>int</code> <code>50</code> Maximum concurrent requests in async mode <code>verbose</code> <code>bool</code> <code>True</code> Show progress bars during prediction"},{"location":"user-guide/dspymator/#fit-parameters","title":"fit() Parameters","text":"Parameter Type Default Description <code>X</code> DataFrame/array required Training features <code>y</code> Series/array required Target values (can be <code>None</code> for unsupervised) <code>optimizer</code> <code>dspy.Optimizer \\| None</code> <code>None</code> DSPy optimizer instance (e.g., <code>dspy.GEPA</code>, <code>dspy.BootstrapFewShot</code>) <code>validation_data</code> <code>tuple \\| float \\| None</code> <code>None</code> Validation data as <code>(X_val, y_val)</code>, a float for train split fraction, or <code>None</code>"},{"location":"user-guide/feature-transformers/","title":"Feature Transformers","text":"<p>Feature transformers are the backbone of <code>centimators</code>, providing scikit-learn compatible transformations that work seamlessly with both Pandas and Polars DataFrames through narwhals. These transformers specialize in time-series and cross-sectional financial data transformations.</p> <p>All transformers follow the standard scikit-learn API (<code>fit</code>, <code>transform</code>, <code>fit_transform</code>) and support metadata routing for passing auxiliary data like date or ticker series through pipelines.</p>"},{"location":"user-guide/feature-transformers/#ranktransformer","title":"RankTransformer","text":"<p>Converts numeric features into their normalized rank within groups (typically by date). This is essential for creating market-neutral features that capture relative performance across assets.</p> <pre><code>from centimators.feature_transformers import RankTransformer\n\n# Rank features within each date\nranker = RankTransformer(feature_names=['close', 'volume'])\nranked_features = ranker.fit_transform(\n    X[['close', 'volume']], \n    date_series=df['date']\n)\n# Output: close_rank, volume_rank (values between 0 and 1)\n</code></pre> <p>Key Features:</p> <ul> <li>Normalizes ranks to [0, 1] range</li> <li>Handles missing values gracefully</li> <li>Groups by any categorical variable (typically dates)</li> </ul>"},{"location":"user-guide/feature-transformers/#lagtransformer","title":"LagTransformer","text":"<p>Creates lagged (shifted) versions of features within groups (typically by ticker). Essential for exposing temporal patterns to machine learning models.</p> <pre><code>from centimators.feature_transformers import LagTransformer\n\n# Create multiple lags for each feature\nlagger = LagTransformer(\n    windows=[1, 5, 10, 20],  # 1-day, 1-week, 2-week, 1-month lags\n    feature_names=['close', 'volume']\n)\nlagged_features = lagger.fit_transform(\n    X[['close', 'volume']], \n    ticker_series=df['ticker']\n)\n# Output: close_lag1, volume_lag1, close_lag5, volume_lag5, etc.\n</code></pre> <p>Key Features:</p> <ul> <li>Preserves temporal ordering within groups</li> <li>Prevents data leakage across different assets</li> <li>Configurable lag windows</li> </ul>"},{"location":"user-guide/feature-transformers/#movingaveragetransformer","title":"MovingAverageTransformer","text":"<p>Computes rolling averages over specified windows within groups. Useful for smoothing noisy signals and creating trend indicators.</p> <pre><code>from centimators.feature_transformers import MovingAverageTransformer\n\n# Create moving averages with different windows\nma_transformer = MovingAverageTransformer(\n    windows=[5, 10, 20, 50],  # Short to long-term trends\n    feature_names=['close', 'volume']\n)\nma_features = ma_transformer.fit_transform(\n    X[['close', 'volume']], \n    ticker_series=df['ticker']\n)\n# Output: close_ma5, volume_ma5, close_ma10, volume_ma10, etc.\n</code></pre> <p>Key Features:</p> <ul> <li>Rolling window calculations within groups</li> <li>Multiple window sizes in single transformation</li> </ul>"},{"location":"user-guide/feature-transformers/#logreturntransformer","title":"LogReturnTransformer","text":"<p>Computes log returns (first difference of natural logarithm) within groups. The standard way to calculate asset returns while ensuring stationarity.</p> <pre><code>from centimators.feature_transformers import LogReturnTransformer\n\n# Calculate log returns for price data\nlog_return_transformer = LogReturnTransformer(feature_names=['close', 'open'])\nreturns = log_return_transformer.fit_transform(\n    X[['close', 'open']], \n    ticker_series=df['ticker']\n)\n# Output: close_logreturn, open_logreturn\n</code></pre>"},{"location":"user-guide/feature-transformers/#groupstatstransformer","title":"GroupStatsTransformer","text":"<p>Calculates statistical measures across groups of related features horizontally (row-wise). Useful for creating aggregate features from multiple related columns.</p> <pre><code>from centimators.feature_transformers import GroupStatsTransformer\n\n# Define feature groups and calculate statistics\nfeature_groups = {\n    'price_features': ['open', 'high', 'low', 'close'],\n    'volume_features': ['volume', 'dollar_volume']\n}\n\nstats_transformer = GroupStatsTransformer(\n    feature_group_mapping=feature_groups,\n    stats=['mean', 'std', 'skew']  # Choose specific statistics\n)\n\ngroup_stats = stats_transformer.fit_transform(X)\n# Output: price_features_groupstats_mean, price_features_groupstats_std, etc.\n</code></pre> <p>Available Statistics:</p> <ul> <li><code>mean</code>: Average across the group</li> <li><code>std</code>: Standard deviation (sample, ddof=1)</li> <li><code>skew</code>: Skewness (bias-corrected)</li> <li><code>kurt</code>: Excess kurtosis (bias-corrected)</li> <li><code>range</code>: Max - Min</li> <li><code>cv</code>: Coefficient of variation (std/mean)</li> </ul>"},{"location":"user-guide/feature-transformers/#dimreducer","title":"DimReducer","text":"<p>Reduces the dimensionality of features using PCA, t-SNE, or UMAP. Useful for compressing high-dimensional feature spaces, visualization, and removing noise.</p> <pre><code>from centimators.feature_transformers import DimReducer\n\n# PCA: Fast, linear, preserves global structure\npca_reducer = DimReducer(\n    method='pca',\n    n_components=10,\n    feature_names=['feature1', 'feature2', 'feature3', 'feature4']\n)\nreduced_features = pca_reducer.fit_transform(X)\n# Output: dim_0, dim_1, ..., dim_9\n\n# t-SNE: Non-linear, preserves local structure (good for visualization)\ntsne_reducer = DimReducer(\n    method='tsne',\n    n_components=2,\n    random_state=42,\n    perplexity=30  # Additional kwargs passed to sklearn.manifold.TSNE\n)\nviz_features = tsne_reducer.fit_transform(X)\n# Output: dim_0, dim_1\n\n# UMAP: Non-linear, preserves local + global structure\n# Requires: uv add 'centimators[all]'\numap_reducer = DimReducer(\n    method='umap',\n    n_components=5,\n    random_state=42,\n    n_neighbors=15  # Additional kwargs passed to umap.UMAP\n)\nreduced_features = umap_reducer.fit_transform(X)\n# Output: dim_0, dim_1, dim_2, dim_3, dim_4\n</code></pre> <p>Available Methods:</p> <ul> <li><code>pca</code>: Principal Component Analysis (linear, fast, deterministic)</li> <li><code>tsne</code>: t-distributed Stochastic Neighbor Embedding (non-linear, stochastic, visualization)</li> <li><code>umap</code>: Uniform Manifold Approximation and Projection (non-linear, balanced, requires <code>centimators[all]</code>)</li> </ul>"},{"location":"user-guide/feature-transformers/#featureneutralizer","title":"FeatureNeutralizer","text":"<p>In competitions like Numerai, your model's predictions often correlate with specific features\u2014this is called feature exposure. High exposure to any single feature can hurt performance when that feature's predictive power shifts over time.</p> <p><code>FeatureNeutralizer</code> reduces exposure by subtracting a proportion of the linear relationship between your predictions and the features. Think of it as \"de-correlating\" your signal from known factors.</p> <p></p> <p>The chart shows predictions before (coral) and after (cyan) 50% neutralization.</p> <pre><code>from centimators.feature_transformers import FeatureNeutralizer\n\nneutralizer = FeatureNeutralizer(\n    proportion=0.5,  # How much to neutralize [0, 1]\n    pred_name='prediction',\n    feature_names=['feature1', 'feature2', 'feature3']\n)\n\nneutralized = neutralizer.fit_transform(\n    df[['prediction']],\n    features=df[['feature1', 'feature2', 'feature3']],\n    era_series=df['era']\n)\n# Output: prediction_neutralized_0.5\n</code></pre> <p>How it works:</p> <ol> <li>Gaussianizes predictions within each era</li> <li>Fits a linear model: <code>prediction ~ features</code></li> <li>Subtracts <code>proportion \u00d7 exposure</code> from predictions</li> <li>Re-normalizes to [0, 1]</li> </ol> <p>Trade-off: Higher proportion = less feature exposure, but also potentially less signal. At <code>proportion=1.0</code>, you remove all linear relationship with features.</p>"},{"location":"user-guide/feature-transformers/#featurepenalizer","title":"FeaturePenalizer","text":"<p>Requires JAX</p> <p>This transformer requires JAX. Install with: <pre><code>uv add 'centimators[keras-jax]'\n</code></pre></p> <p><code>FeaturePenalizer</code> takes a different approach: instead of subtracting a fixed proportion, it uses gradient descent to find the minimal adjustment that caps all exposures below a threshold.</p> <p></p> <p>The key difference from neutralization: penalization enforces a hard cap (dashed lines at \u00b10.1). Every cyan bar stays within bounds\u2014the optimizer finds the smallest change needed to achieve this.</p> <pre><code>from centimators.feature_transformers import FeaturePenalizer\n\npenalizer = FeaturePenalizer(\n    max_exposure=0.1,  # Cap exposure at \u00b10.1\n    pred_name='prediction',\n    feature_names=['feature1', 'feature2', 'feature3']\n)\n\npenalized = penalizer.fit_transform(\n    df[['prediction']],\n    features=df[['feature1', 'feature2', 'feature3']],\n    era_series=df['era']\n)\n# Output: prediction_penalized_0.1\n</code></pre>"},{"location":"user-guide/feature-transformers/#embeddingtransformer","title":"EmbeddingTransformer","text":"<p>Requires DSPy</p> <p>This transformer requires the <code>dspy</code> optional dependency. Install with: <pre><code>uv add 'centimators[dspy]'\n</code></pre></p> <p>Embeds text and categorical features into dense vector representations using DSPy's Embedder. Supports both hosted embedding models (OpenAI, Cohere, etc.) and custom local models.</p>"},{"location":"user-guide/feature-transformers/#basic-text-embedding","title":"Basic Text Embedding","text":"<pre><code>from centimators.feature_transformers import EmbeddingTransformer\n\n# Using a hosted model\nembedder = EmbeddingTransformer(\n    model=\"openai/text-embedding-3-small\",\n    feature_names=['news_headline', 'company_description']\n)\n\nembedded_features = embedder.fit_transform(df[['news_headline', 'company_description']])\n# Output: news_headline_embed_0, news_headline_embed_1, ..., news_headline_embed_1535\n#         company_description_embed_0, company_description_embed_1, ..., etc.\n</code></pre>"},{"location":"user-guide/feature-transformers/#local-model-sentence-transformers","title":"Local Model (Sentence Transformers)","text":"<pre><code>from sentence_transformers import SentenceTransformer\n\n# Load a local embedding model\nmodel = SentenceTransformer('all-MiniLM-L6-v2')\n\nembedder = EmbeddingTransformer(\n    model=model.encode,  # Pass the encode function\n    feature_names=['text_column'],\n    batch_size=100\n)\n\nembedded_features = embedder.fit_transform(df[['text_column']])\n# Output: text_column_embed_0, text_column_embed_1, ..., text_column_embed_383\n</code></pre>"},{"location":"user-guide/feature-transformers/#custom-embedding-function","title":"Custom Embedding Function","text":"<pre><code>import numpy as np\n\ndef custom_embedder(texts):\n    \"\"\"Custom embedding function - must return numpy array.\"\"\"\n    # Your custom logic here\n    embeddings = your_model.embed(texts)\n    return np.array(embeddings, dtype=np.float32)\n\nembedder = EmbeddingTransformer(\n    model=custom_embedder,\n    feature_names=['text_feature']\n)\n</code></pre> <p>Key Features:</p> <ul> <li>Supports both hosted (via litellm) and local embedding models</li> <li>Handles null values (fills with zero vectors)</li> <li>Automatically expands embeddings into sklearn-compatible columns</li> <li>Backend-agnostic (works with Polars, Pandas)</li> <li>Configurable batch size and caching for hosted models</li> </ul>"},{"location":"user-guide/feature-transformers/#pipeline-integration","title":"Pipeline Integration","text":"<p>All transformers work seamlessly in scikit-learn pipelines with metadata routing:</p> <pre><code>from sklearn import set_config\nfrom sklearn.pipeline import make_pipeline\n\n# Enable metadata routing\nset_config(enable_metadata_routing=True)\n\n# Create pipeline with multiple transformers\npipeline = make_pipeline(\n    LogReturnTransformer().set_transform_request(ticker_series=True),\n    RankTransformer().set_transform_request(date_series=True),\n    LagTransformer(windows=[1, 5, 10]).set_transform_request(ticker_series=True),\n    MovingAverageTransformer(windows=[5, 20]).set_transform_request(ticker_series=True)\n)\n\n# Transform data with metadata routing\ntransformed = pipeline.fit_transform(\n    df[['close', 'volume']],\n    date_series=df['date'],\n    ticker_series=df['ticker']\n)\n</code></pre> <p>Metadata Routing:</p> <ul> <li><code>date_series</code>: Used by <code>RankTransformer</code> for cross-sectional ranking</li> <li><code>ticker_series</code>: Used by temporal transformers (<code>LagTransformer</code>, <code>MovingAverageTransformer</code>, <code>LogReturnTransformer</code>) to maintain asset boundaries</li> <li><code>era_series</code>: Used by <code>FeatureNeutralizer</code> and <code>FeaturePenalizer</code> to process predictions era-by-era</li> <li><code>features</code>: Used by <code>FeatureNeutralizer</code> and <code>FeaturePenalizer</code> for exposure calculation</li> </ul>"},{"location":"user-guide/keras-cortex/","title":"KerasCortex","text":"<p>Requires keras, jax, and DSPy</p> <p>This estimator requires the <code>dspy</code> optional dependency. Install with: <pre><code>uv add centimators[all]\n</code></pre></p> <p>Warning</p> <p>This module is a work in progress. It is not yet ready for production use. This is highly experimental and likely to overfit.</p> <p><code>centimators.model_estimators.KerasCortex</code> introduces a novel approach to model development by automating aspects of architecture search. It wraps a Keras-based estimator and leverages a Large Language Model (LLM) to recursively self-reflect on its own architecture. The LLM suggests improvements to the model's source code, which are then tested. This iterative process allows <code>KerasCortex</code> to refine its internal model over several cycles, potentially discovering more optimal architectures for the given data.</p>"},{"location":"user-guide/keras-cortex/#how-it-works","title":"How It Works","text":"<p>At its core, <code>KerasCortex</code> utilizes DSPy to manage the interaction with the LLM through two key components:</p> <p>The <code>Think</code> Module: A DSPy <code>Module</code> that orchestrates the LLM's code generation process. It uses DSPy's <code>ChainOfThought</code> to enable access to the LLM's reasoning process as it improves its own architecture. The <code>Think</code> module takes the current <code>build_model</code> source code, a history of attempted code modifications and their performance, and an optimization goal (e.g., \"improve validation R2 score\"), then returns the LLM's suggested <code>build_model</code> method modification.</p> <p>The <code>think_loop</code> Method: The heart of <code>KerasCortex</code>'s self-improvement mechanism. This iterative process works as follows:</p> <ol> <li>Establish Baseline: The initial Keras estimator is cloned and trained on the training data to establish baseline performance on the validation set</li> <li>Refine Architecture: For each iteration:<ul> <li>The <code>Think</code> module suggests a new <code>build_model</code> code modification based on the current best code and performance history of all previous iterations</li> <li>The suggested code is executed to create a new <code>build_model</code> function</li> <li>A new model instance is created with the modified architecture and trained on the data</li> <li>The model is evaluated on validation data and compared to the current best</li> <li>If performance improves, the new model becomes the best candidate; each model and its performance is logged for future iterations to reflect upon</li> </ul> </li> <li>Converge?: After all iterations (or early termination due to errors), the best-performing model and complete performance log are returned</li> </ol> <p><code>KerasCortex</code> requires validation data to evaluate the performance of different architectures. It uses this information to guide its self-improvement process. The <code>lm</code> parameter specifies the language model to be used for code generation, and <code>n_iterations</code> controls how many times the model attempts to refine itself. When <code>verbose=True</code>, you can observe the LLM's reasoning process and see how it decides to modify the architecture at each step. </p> <p></p> <p>This approach allows <code>KerasCortex</code> to explore different architectural modifications and converge towards a model that performs well on the given validation data. One could even finetune the prompts or LLM weights directly to improve the quality of the suggestions in its own meta-loop. Access to tools like the keras documentation or arxiv papers could be added as well.</p>"},{"location":"user-guide/keras-cortex/#usage","title":"Usage","text":"<p>While <code>KerasCortex</code> is an advanced tool, its scikit-learn compatible API makes it surprisingly easy to integrate into existing workflows.</p> <pre><code># Define a base Keras estimator, can be anything with a `build_model` method\nbase_mlp = MLPRegressor(\n    hidden_units=(64, 32),\n    dropout_rate=0.1,\n)\n\n# Initialize KerasCortex\n# Ensure your LM (e.g., OpenAI API key) is configured in your environment\ncortex = KerasCortex(\n    base_estimator=base_mlp,\n    n_iterations=5,  # Number of self-reflection iterations\n    lm=\"openai/gpt-4o-mini\", # Or any other dspy.LM compatible model\n    verbose=True\n)\n\n# Fit KerasCortex like any scikit-learn estimator\ncortex.fit(\n    X_train,\n    y_train,\n    validation_data=(X_val, y_val), # Needed for self-reflection\n    epochs=5, # Epochs for each iteration's model training\n    batch_size=128,\n)\n\n# Make predictions with the best model found\npredictions = cortex.predict(X_val)\n</code></pre> <p>View the KerasCortex tutorial for a more detailed example.</p>"},{"location":"user-guide/model-estimators/","title":"Model Estimators","text":"<p>Centimators ships with Keras-backed model estimators that implement the familiar scikit-learn API.  This means you can train state-of-the-art neural networks while still benefitting from the rich tooling ecosystem around scikit-learn \u2013 cross-validation, pipelines, grid-search and more.</p>"},{"location":"user-guide/model-estimators/#tabular-models","title":"Tabular Models","text":"<p>These models are designed for traditional tabular data where each row represents an independent observation.</p>"},{"location":"user-guide/model-estimators/#mlpregressor","title":"MLPRegressor","text":"<p><code>centimators.model_estimators.MLPRegressor</code> is a minimal, fully-connected multilayer perceptron that works out-of-the-box for any tabular regression task.</p> <pre><code>import numpy as np\nimport polars as pl\nfrom centimators.model_estimators import MLPRegressor\n\n# Dummy data: 1 000 samples \u00d7 20 features\ngrng = np.random.default_rng(seed=42)\nX = pl.DataFrame(grng.standard_normal((1000, 20)))\ny = pl.Series(grng.standard_normal(1000))\n\nestimator = MLPRegressor(\n    hidden_units=(128, 64), \n    dropout_rate=0.1,\n    activation=\"relu\",\n    learning_rate=0.001\n)\nestimator.fit(X, y, epochs=10)\n\npredictions = estimator.predict(X)\nprint(predictions[:5])\n</code></pre> <p>Because the estimator inherits from scikit-learn's <code>BaseEstimator</code>, you can seamlessly compose it with the feature transformers provided elsewhere in the library:</p> <pre><code>from sklearn.pipeline import make_pipeline\nfrom centimators.feature_transformers import RankTransformer\n\npipeline = make_pipeline(\n    RankTransformer(feature_names=X.columns),\n    MLPRegressor(hidden_units=(128, 64), epochs=10),\n)\n\npipeline.fit(X, y)\n</code></pre>"},{"location":"user-guide/model-estimators/#bottleneckencoder","title":"BottleneckEncoder","text":"<p><code>centimators.model_estimators.BottleneckEncoder</code> implements a bottleneck autoencoder that can learn latent representations and predict targets simultaneously. This estimator:</p> <ol> <li>Encodes input features to a lower-dimensional latent space</li> <li>Decodes the latent representation back to reconstruct the input  </li> <li>Uses an additional MLP branch to predict targets from the decoded features</li> </ol> <p>The model can be used both as a regressor (via <code>predict</code>) and as a transformer (via <code>transform</code>) to get latent space representations for dimensionality reduction.</p> <pre><code>from centimators.model_estimators import BottleneckEncoder\n\n# Create bottleneck autoencoder\nencoder = BottleneckEncoder(\n    gaussian_noise=0.035,\n    encoder_units=[(1024, 0.1)],  # [(units, dropout_rate), ...]\n    latent_units=(256, 0.1),       # (units, dropout_rate)\n    ae_units=[(96, 0.4)],          # prediction branch architecture\n    activation=\"swish\",\n    reconstruction_loss_weight=1.0,\n    target_loss_weight=1.0\n)\n\n# Fit the model (learns reconstruction + target prediction)\nencoder.fit(X, y, epochs=10)\n\n# Get target predictions\npredictions = encoder.predict(X)\n\n# Get latent space representations for dimensionality reduction\nlatent_features = encoder.transform(X)\nprint(f\"Latent shape: {latent_features.shape}\")  # (1000, 256)\n</code></pre>"},{"location":"user-guide/model-estimators/#neuraldecisionforestregressor","title":"NeuralDecisionForestRegressor","text":"<p><code>centimators.model_estimators.NeuralDecisionForestRegressor</code> is a differentiable decision forest for tabular regression. It learns soft routing probabilities at each node and aggregates leaf predictions over an ensemble of trees, trained end-to-end with backprop.</p> <ul> <li>Interpretable tree-like structure with learned splits</li> <li>End-to-end differentiable training</li> <li>Ensemble averaging for stability</li> <li>Controls for routing sharpness, balance, and ensemble diversity</li> </ul>"},{"location":"user-guide/model-estimators/#quick-start","title":"Quick start","text":"<pre><code>import numpy as np\nfrom centimators.model_estimators import NeuralDecisionForestRegressor\n\nrng = np.random.default_rng(42)\nX = rng.standard_normal((2000, 20)).astype(\"float32\")\ny = (np.sin(2*X[:, 0]) + X[:, 1]**2 + X[:, 2]*X[:, 3]).astype(\"float32\").reshape(-1, 1)\n\nndf = NeuralDecisionForestRegressor(\n    num_trees=25,\n    depth=4,\n    used_features_rate=0.5,\n    # Routing sharpness (recommended): 0.5\n    temperature=0.5,\n    # Optional diversity/robustness\n    input_noise_std=0.03,   # noise before trunk\n    tree_noise_std=0.05,    # per-tree noise after trunk\n    trunk_units=[32],       # small shared MLP trunk\n    random_state=42,\n)\n\nndf.fit(X, y, epochs=50, batch_size=64, verbose=0)\npreds = ndf.predict(X)\n</code></pre>"},{"location":"user-guide/model-estimators/#key-parameters","title":"Key parameters","text":"<ul> <li><code>num_trees</code> (int): number of trees in the ensemble (e.g., 25\u2013100)</li> <li><code>depth</code> (int): tree depth; 3\u20135 are common, deeper is harder to train</li> <li><code>used_features_rate</code> (float): feature bagging rate per tree (0\u20131); never fewer than 1 feature is used (edge case handled)</li> <li><code>temperature</code> (float): routing sharpness; 0.3\u20130.5 is sharper, 1.0 is neutral</li> <li><code>l2_decision</code> / <code>l2_leaf</code> (float): separate L2 for routing vs leaves (e.g., 1e-4 and 1e-3)</li> <li><code>input_noise_std</code> (float): Gaussian noise before trunk for robustness (try 0.02\u20130.05)</li> <li><code>tree_noise_std</code> (float): per-tree Gaussian noise after trunk to decorrelate trees (try 0.03\u20130.10)</li> <li><code>tree_dropout_rate</code> (float): optionally drop tree contributions during training (0\u20130.3)</li> <li><code>trunk_units</code> (list[int] | None): optional small shared MLP before the trees (e.g., <code>[32, 32]</code>)</li> <li><code>random_state</code> (int | None): reproducible feature-bagging masks</li> </ul>"},{"location":"user-guide/model-estimators/#optional-temperature-annealing-advanced","title":"Optional temperature annealing (advanced)","text":"<p>Start with soft routing (high temperature) and anneal to sharp routing (low temperature):</p> <pre><code>from centimators.model_estimators import NeuralDecisionForestRegressor, TemperatureAnnealing\n\nndf = NeuralDecisionForestRegressor(\n    num_trees=25,\n    depth=4,\n    temperature=2.0,  # Start soft\n    random_state=42,\n)\n\nepochs = 50\nannealer = TemperatureAnnealing(ndf, start=2.0, end=0.5, epochs=epochs)\nndf.fit(X, y, epochs=epochs, callbacks=[annealer])\n</code></pre>"},{"location":"user-guide/model-estimators/#notes","title":"Notes","text":"<ul> <li>Feature-bagging edge case is handled (at least one feature per tree)</li> <li><code>random_state</code> seeds the feature mask sampling</li> <li>A small trunk often improves accuracy by splitting on learned features</li> </ul>"},{"location":"user-guide/model-estimators/#sequence-models","title":"Sequence Models","text":"<p>These models are designed for sequential/time-series data where temporal dependencies matter.</p>"},{"location":"user-guide/model-estimators/#sequenceestimator","title":"SequenceEstimator","text":"<p><code>SequenceEstimator</code> is a base class that handles the reshaping of lagged features into the 3-D tensor format required by sequence models like LSTMs and CNNs. It's not meant to be used directly, but rather inherited from by specific sequence model implementations.</p> <p>Key responsibilities: - Reshapes flattened lag matrices into (batch, timesteps, features) tensors - Manages sequence length and feature dimensionality - Provides common sequence model functionality</p>"},{"location":"user-guide/model-estimators/#lstmregressor","title":"LSTMRegressor","text":"<p><code>centimators.model_estimators.LSTMRegressor</code> provides a ready-to-use LSTM implementation for time series regression. It supports stacked LSTM layers, bidirectional processing, and various normalization strategies.</p> <pre><code>from centimators.model_estimators import LSTMRegressor\nfrom centimators.feature_transformers import LagTransformer\n\n# Create lagged features\nlag_transformer = LagTransformer(windows=[1, 2, 3, 4, 5])\nX_lagged = lag_transformer.fit_transform(X, ticker_series=tickers)\n\n# Create LSTM model\nlstm = LSTMRegressor(\n    lag_windows=[1, 2, 3, 4, 5],       # Must match lag transformer\n    n_features_per_timestep=2,          # e.g., price and volume\n    lstm_units=[\n        (128, 0.2, 0.1),                # (units, dropout, recurrent_dropout)\n        (64, 0.1, 0.1),                 # Second LSTM layer\n    ],\n    bidirectional=True,                 # Use bidirectional LSTMs\n    use_layer_norm=True,                # Layer normalization after each LSTM\n    use_batch_norm=False,               # Batch normalization (usually not both)\n    learning_rate=0.001,\n    output_units=1\n)\n\n# Fit the model\nlstm.fit(X_lagged, y, epochs=50, batch_size=32)\n\n# Make predictions\npredictions = lstm.predict(X_lagged)\n</code></pre>"},{"location":"user-guide/model-estimators/#transformerregressor","title":"TransformerRegressor","text":"<p><code>centimators.model_estimators.TransformerRegressor</code> applies transformer encoder blocks over lagged sequence inputs. It supports three attention modes \u2014 temporal (standard self-attention over timesteps), feature (iTransformer-style attention over features), and cross (dual-axis temporal + feature attention) \u2014 and two pooling strategies for collapsing the sequence dimension before the final MLP head.</p> <pre><code>from centimators.model_estimators import TransformerRegressor\nfrom centimators.feature_transformers import LagTransformer\n\n# Create lagged features\nlag_transformer = LagTransformer(windows=[1, 2, 3, 4, 5])\nX_lagged = lag_transformer.fit_transform(X, ticker_series=tickers)\n\n# Create Transformer model\ntransformer = TransformerRegressor(\n    lag_windows=[1, 2, 3, 4, 5],       # Must match lag transformer\n    n_features_per_timestep=2,          # e.g., price and volume\n    d_model=32,                         # Embedding dimension\n    num_heads=4,                        # Attention heads\n    ff_dim=128,                         # Feed-forward inner dimension\n    num_blocks=2,                       # Stacked encoder blocks\n    attention_type=\"temporal\",          # \"temporal\", \"feature\", or \"cross\"\n    pooling_type=\"attention\",           # \"attention\" (learned) or \"average\"\n    use_pre_norm=True,                  # Pre-LayerNorm (more stable training)\n    mlp_units=(64,),                    # MLP head after pooling\n    dropout_rate=0.1,\n    learning_rate=0.001,\n)\n\n# Fit and predict\ntransformer.fit(X_lagged, y, epochs=50, batch_size=32)\npredictions = transformer.predict(X_lagged)\n</code></pre>"},{"location":"user-guide/model-estimators/#key-parameters_1","title":"Key parameters","text":"<ul> <li><code>d_model</code> (int): Dimension of the internal embedding space (default: 32)</li> <li><code>num_heads</code> (int): Number of attention heads (default: 4)</li> <li><code>ff_dim</code> (int): Hidden dimension of the feed-forward network in each encoder block (default: 128)</li> <li><code>num_blocks</code> (int): Number of stacked encoder blocks (default: 1)</li> <li><code>attention_type</code> (str): <code>\"temporal\"</code> for standard self-attention over timesteps, <code>\"feature\"</code> for iTransformer-style attention over features, or <code>\"cross\"</code> for dual-axis (temporal + feature) attention</li> <li><code>pooling_type</code> (str): <code>\"attention\"</code> for learned weighted pooling or <code>\"average\"</code> for global average pooling</li> <li><code>use_pre_norm</code> (bool): Apply LayerNorm before attention/FFN rather than after (default: True)</li> <li><code>mlp_units</code> (tuple[int, ...]): Hidden layer sizes for the prediction head after pooling (default: (64,))</li> </ul>"},{"location":"user-guide/model-estimators/#loss-functions","title":"Loss Functions","text":"<p>Centimators provides custom loss functions, alongside support for standard Keras losses.</p>"},{"location":"user-guide/model-estimators/#spearmancorrelation","title":"SpearmanCorrelation","text":"<p><code>centimators.losses.SpearmanCorrelation</code> is a differentiable loss function that optimizes for rank correlation rather than absolute error. This is particularly useful for: - Ranking tasks where relative ordering matters more than exact values - Financial signals where direction and magnitude are more important than precise predictions - Robust training in the presence of outliers</p> <pre><code>from centimators.losses import SpearmanCorrelation\nfrom centimators.model_estimators import MLPRegressor\n\n# Optimize for rank correlation\nmodel = MLPRegressor(\n    hidden_units=(128, 64),\n    loss_function=SpearmanCorrelation(regularization_strength=1e-3),\n    metrics=[\"mae\", \"mse\"]\n)\n</code></pre>"},{"location":"user-guide/model-estimators/#combinedloss","title":"CombinedLoss","text":"<p><code>centimators.losses.CombinedLoss</code> blends mean squared error with Spearman correlation, allowing you to optimize for both accurate predictions and correct ranking simultaneously:</p> <pre><code>from centimators.losses import CombinedLoss\nfrom centimators.model_estimators import LSTMRegressor\n\n# Balance between MSE and rank correlation\ncombined_loss = CombinedLoss(\n    mse_weight=2.0,           # Weight for mean squared error\n    spearman_weight=1.0,      # Weight for Spearman correlation\n    spearman_regularization=1e-3\n)\n\nlstm = LSTMRegressor(\n    lag_windows=[1, 2, 3, 4, 5],\n    n_features_per_timestep=2,\n    lstm_units=[(128, 0.2, 0.1)],\n    loss_function=combined_loss\n)\n</code></pre>"},{"location":"user-guide/model-estimators/#standard-keras-losses","title":"Standard Keras Losses","text":"<p>All estimators also support standard Keras loss functions:</p> <pre><code>from keras.losses import Huber\n\n# Using Huber loss for robust regression\nmodel = MLPRegressor(\n    hidden_units=(128, 64),\n    loss_function=Huber(delta=1.0),  # Or just \"huber\"\n    metrics=[\"mae\", \"mse\"]\n)\n</code></pre>"}]}